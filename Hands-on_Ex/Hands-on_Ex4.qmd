---
title: "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method"
---

### 1. Quick Recap

In previous sections we have seen how to

-   handle geospatial and aspatial data

-   plot choropleth math

-   compute various types of spatial weights and weight matrix

-   determine hotspot and coldspot areas

-   measure global and local measures of spatial autocorrelation

-   delineate homogeneous region using spatially constrained techniques

### 2. Introduction

The objective of this study is to Calibrate Hedonic Pricing Model for Private Highrise Property in Singapore using Geographically Weighted Regression (GWR) Method

**What is Geographically Weighted Regression?**

It is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable).

![Geographically Weighted Regression](images/GWR.jpg){fig-align="center"}

### 3. Glimpse of Steps

Some of the important steps performed in this study are as follows

-   importing geospatial data using appropriate function(s) of **sf** package,

-   importing csv file using appropriate function of **readr** package,

-   converting aspatial dataframe into sf object

-   performing exploratory data analysis

-   performing simple and multiple linear regression techniques

-   building a hedonic price model using GWR method

### 4. Data

Following two data sets are used:

-   URA Master Plan subzone boundary in shapefile format (i.e. *MP14_SUBZONE_WEB_PL*)

-   condo_resale_2015 in csv format (i.e. *condo_resale_2015.csv*)

### 5.Deep Dive into Map Analysis

#### 5.1 Installing libraries and Importing files

[`p_load`](https://cran.r-project.org/web/packages/pacman/pacman.pdf) function pf [**pacman**](https://github.com/trinker/pacman) package is used to install and load sf all necessary packages into R environment.

-   **sf**, **rgdal** and **spdep -** Spatial data handling

-   **tidyverse**, especially **readr**, **ggplot2** and **dplyr -** Attribute data handling

-   **tmap -**Choropleth mapping

-   [**olsrr**](https://olsrr.rsquaredacademy.com/) **-** Ordinary Least Square(OLS) method and performing diagnostics tests

-   [**GWmodel**](https://cran.r-project.org/web/packages/GWmodel/) **-** geographical weighted family of models

-   [**corrplot**](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html) **-** multivariate data visualisation and analysis

The code chunk below installs and launches these R packages into R environment.

```{r}
#| code-fold: true
#| code-summary: "Loading packages"

pacman::p_load(olsrr, corrplot, ggpubr, 
               sf,spdep, GWmodel, tmap, 
               tidyverse, gtsummary,patchwork, ggthemes)
```

The code chunk below is used to import *MP_SUBZONE_WEB_PL* shapefile by using `st_read()` of **sf** packages.

```{r, message=FALSE}
#| code-fold: true
#| code-summary: "Importing data"

mpsz = st_read(dsn = "data/geospatial", 
               layer = "MP14_SUBZONE_WEB_PL")
condo_resale = read_csv("data/aspatial/Condo_resale_2015.csv")
```

#### 5.2 Data Wrangling

The code chunk below updates the newly imported *mpsz* with the correct ESPG code (i.e. 3414)

```{r}
#| code-fold: true
#| code-summary: "Assigning correct projection"
mpsz_svy21 <- st_transform(mpsz, 3414)

```

Currently, the *condo_resale* tibble data frame is aspatial. We will convert it to a **sf** object. The code chunk below converts condo_resale data frame into a simple feature data frame by using `st_as_sf()` of **sf** packages.

```{r}
#| code-fold: true
#| code-summary: "converting projection"
condo_resale.sf <- st_as_sf(condo_resale,
                            coords = c("LONGITUDE", "LATITUDE"),
                            crs=4326) %>%
  st_transform(crs=3414)
```

#### 5.3 Exploratory Data Analysis (EDA)

**What is the distribution of Condo selling price?**

```{r}
#| code-fold: true
#| code-summary: "Histogram-Selling Price"

h1 <- ggplot(data=condo_resale.sf, 
              aes(x=`SELLING_PRICE`,
                  y= ..density..)) +
      geom_histogram(bins=20, 
                     color="black", 
                     fill="coral")+
      geom_density(color="black",
                   alpha=0.5) +
      theme(panel.background= element_blank())
```

The figure above reveals a right skewed distribution. This means that more condominium units were transacted at relative lower prices. Tghis can be normalised by using log transformation.

The code chunk below is used to derive a new variable called *LOG_SELLING_PRICE* by using a log transformation on the variable *SELLING_PRICE*. It is performed using `mutate()` of **dplyr** package.

```{r}
#| code-fold: true
#| code-summary: "Histogram-Selling Price"

condo_resale.sf <- condo_resale.sf %>%
                   mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))

h2 <- ggplot(data=condo_resale.sf, 
             aes(x=`LOG_SELLING_PRICE`,
                 y= ..density..)) +
      geom_histogram(bins=20, 
                     color="black", 
                     fill="coral")+
      geom_density(color="black",
                   alpha=0.5)+
      theme(panel.background= element_blank())

```

Let us compare the distribution before and after performing log transformation

```{r}
#| code-fold: true
#| code-summary: "Comaprison"

h1 <- h1 + labs(title= "Raw values")
h2 <- h2 + labs(title = "Log transformation")

ggarrange(h1, h2, ncol=2)
```

Now let us view the distribution for multiple variables

The code chunk below is used to multiple histograms. Then, `ggarrange()` is used to organised these histogram into a 3 columns by 4 rows small multiple plot.

```{r, fig.height=10, fig.width=15}
#| code-fold: true
#| code-summary: "Multiple histograms"

AREA_SQM <- ggplot(data=condo_resale.sf, 
                   aes(x= `AREA_SQM`,
                   y= ..density..)) + 
            geom_histogram(bins=20, 
                           color="black", 
                           fill="coral")+
            geom_density(color="black",
                         alpha=0.5)+
            theme(panel.background= element_blank())

AGE <- ggplot(data=condo_resale.sf, 
              aes(x= `AGE`,
                  y= ..density..)) +
       geom_histogram(bins=20, 
                      color="black", 
                      fill="coral")+
            geom_density(color="black",
                         alpha=0.5)+
            theme(panel.background= element_blank())

PROX_CBD <- ggplot(data=condo_resale.sf, 
                   aes(x= `PROX_CBD`,
                       y= ..density..)) +
            geom_histogram(bins=20, 
                           color="black", 
                           fill="coral")+
            geom_density(color="black",
                         alpha=0.5)+
            theme(panel.background= element_blank())

PROX_CHILDCARE <- ggplot(data=condo_resale.sf,
                         aes(x= `PROX_CHILDCARE`,
                             y= ..density..)) + 
                  geom_histogram(bins=20,
                                 color="black", 
                                 fill="coral")+
                  geom_density(color="black",
                                alpha=0.5)+
                  theme(panel.background= element_blank())

PROX_ELDERLYCARE <- ggplot(data=condo_resale.sf, 
                           aes(x= `PROX_ELDERLYCARE`,
                               y= ..density..)) +
                    geom_histogram(bins=20, 
                                   color="black", 
                                   fill="coral")+
                    geom_density(color="black",
                                 alpha=0.5)+
                    theme(panel.background= element_blank())

PROX_URA_GROWTH_AREA <- ggplot(data=condo_resale.sf, 
                               aes(x= `PROX_URA_GROWTH_AREA`,
                                   y= ..density..)) +
                        geom_histogram(bins=20, 
                                       color="black", 
                                       fill="coral")+
                        geom_density(color="black",
                                     alpha=0.5)+
                        theme(panel.background= element_blank())

PROX_HAWKER_MARKET <- ggplot(data=condo_resale.sf, 
                             aes(x= `PROX_HAWKER_MARKET`,
                                 y= ..density..)) +
                      geom_histogram(bins=20, 
                                     color="black", 
                                     fill="coral")+
                      geom_density(color="black",
                                   alpha=0.5)+
                      theme(panel.background= element_blank())

PROX_KINDERGARTEN <- ggplot(data=condo_resale.sf, 
                            aes(x= `PROX_KINDERGARTEN`,
                                y= ..density..)) +
                     geom_histogram(bins=20, 
                                    color="black", 
                                    fill="coral")+
                     geom_density(color="black",
                                  alpha=0.5)+
            theme(panel.background= element_blank())

PROX_MRT <- ggplot(data=condo_resale.sf, 
                   aes(x= `PROX_MRT`,
                       y= ..density..)) +
            geom_histogram(bins=20, 
                           color="black", 
                           fill="coral")+
            geom_density(color="black",
                         alpha=0.5)+
            theme(panel.background= element_blank())

PROX_PARK <- ggplot(data=condo_resale.sf, 
                    aes(x= `PROX_PARK`,
                        y= ..density..)) +
             geom_histogram(bins=20, 
                            color="black", 
                            fill="coral")+
             geom_density(color="black",
                          alpha=0.5)+
             theme(panel.background= element_blank())

PROX_PRIMARY_SCH <- ggplot(data=condo_resale.sf, 
                           aes(x= `PROX_PRIMARY_SCH`,
                               y= ..density..)) +
                    geom_histogram(bins=20, 
                                   color="black", 
                                   fill="coral")+
                    geom_density(color="black",
                                 alpha=0.5)+
                    theme(panel.background= element_blank())

PROX_TOP_PRIMARY_SCH <- ggplot(data=condo_resale.sf, 
                               aes(x= `PROX_TOP_PRIMARY_SCH`,
                                   y= ..density..)) +
                        geom_histogram(bins=20, 
                                       color="black", 
                                       fill="coral")+
                        geom_density(color="black",
                                     alpha=0.5)+
                        theme(panel.background= element_blank())

ggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, 
          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,
          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  
          ncol = 3, nrow = 4)
```

**What is the geospatial distribution of condo prices in Singapore?**

The code chunks below is used to create an interactive point symbol map.

```{r}
#| code-fold: true
#| code-summary: "Geospatial distribution"

tm_shape(mpsz_svy21)+
  tm_polygons() +
tm_shape(condo_resale.sf) +  
  tm_dots(col = "SELLING_PRICE",
          alpha = 0.6,
          style="quantile") +
  tmap_options(check.and.fix = TRUE)+
  tmap_mode("view")+
  tm_view(set.zoom.limits = c(11,14))
```

#### 5.4 Hedonic Pricing Modelling in R

#### 5.4.1 Simple Linear Regression Method

First, let us build a simple linear regression model by using *SELLING_PRICE* as the dependent variable and *AREA_SQM* as the independent variable.

```{r}
#| code-fold: true
#| code-summary: "Simple Linear Regression"

condo.slr <- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)
summary(condo.slr)
```

**Interpretation**

R-squared of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.

***H~0~ (Null Hypothesis)*** - mean price is a good estimator of SELLING_PRICE

***H~1~ (Alternative Hypothesis)*** - mean price is not a good estimator of SELLING_PRICE

1.  Since p-value is much smaller than 0.0001, we will reject the null hypothesis that mean is a good estimator of SELLING_PRICE.

2.  This will allow us to infer that simple linear regression model above is a good estimator of *SELLING_PRICE*.

3.  p-values of both the estimates of the Intercept and ARA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected and so B0 and B1 are good parameter estimates.

Let us visualise the best fit curve on a scatterplot, using `lm()` as a method function in ggplot's geometry as shown in the code chunk below.

```{r}
#| code-fold: true
#| code-summary: "Goodness of fit"

ggplot(data=condo_resale.sf,  
       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +
  geom_point() +
  geom_smooth(method = lm)+
  theme(panel.background= element_blank())
```

We can see that there are a few statistical outliers with relatively high selling prices.

#### 5.4.2 Multiple Linear Regression Method

Let us check if there is a multicollinearity phenomenon by executing correlation analysis. It is important to ensure that the indepdent variables used are not highly correlated to each other.

The code chunk below is used to plot a scatterplot matrix of the relationship between the independent variables in *condo_resale* data.frame.

```{r, fig.height=10, fig.width=15}
#| code-fold: true
#| code-summary: "Correlation Analysis"

corrplot(cor(condo_resale[, 5:23]), 
         diag = FALSE, 
         order = "AOE",
         tl.pos = "td", 
         tl.cex = 0.5,
         method = "number", 
         type = "upper")
```

It is clear that ***Freehold*** is highly correlated to ***LEASE_99YEAR***. Hence, let us include either one of them i.e. ***LEASE_99YEAR*** in the subsequent model building.

#### 5.4.3 Hedonic pricing model using multiple linear regression method

The code chunk below using `lm()` to calibrate the multiple linear regression model.

```{r}
#| code-fold: true
#| code-summary: "Multiple Linear Regression"

condo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + 
                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +
                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + 
                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + 
                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + 
                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                data=condo_resale.sf)
summary(condo.mlr)
```

At 99% confidence interval, almost all the varibles are statistically significant except *PROX_HAWKER_MARKET*, *PROX_KINDERGARTEN , PROX_TOP_PRIMARY_SCH.*

#### 5.4.4 Publication Quality Table: olsrr method

It is clear that not all the independent variables are statistically significant. Let us revise the model by removing those variables which are not statistically significant.

```{r}
#| code-fold: true
#| code-summary: "Revised model"

condo.mlr1 <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + 
                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +
                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + 
                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + 
                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,
                 data=condo_resale.sf)
ols_regress(condo.mlr1)
```

Now, we have only statistically significant variables.

#### 5.4.5 Publication Quality Table: gtsummary method

In the code chunk below, [`tbl_regression()`](https://www.danieldsjoberg.com/gtsummary/reference/tbl_regression.html) is used to create a well formatted regression report using [**gtsummary**](https://www.danieldsjoberg.com/gtsummary/) package that provides an elegant and flexible way to create publication-ready summary tables in R.

```{r}
#| code-fold: true
#| code-summary: "gtsummary"

tbl_regression(condo.mlr1, intercept = TRUE)
```

With this, model statistics can also be included in the report by either appending them to the report table by using [`add_glance_table()`](https://www.danieldsjoberg.com/gtsummary/reference/add_glance.html) or adding as a table source note by using [`add_glance_source_note()`](https://www.danieldsjoberg.com/gtsummary/reference/add_glance.html) as shown in the code chunk below.

```{r}
#| code-fold: true
#| code-summary: "Model statistics"


tbl_regression(condo.mlr1, 
               intercept = TRUE) %>% 
  add_glance_source_note(
    label = list(sigma ~ "\U03C3"),
    include = c(r.squared, adj.r.squared, 
                AIC, statistic,
                p.value, sigma))
```

#### 5.4.6 Checking for Multicollinearity

Let us check if there is any sign of multicollinearity using [`ols_vif_tol()`](https://olsrr.rsquaredacademy.com/reference/ols_coll_diag.html) of olsrr package

```{r}
#| code-fold: true
#| code-summary: "Multicollinearity check"

ols_vif_tol(condo.mlr1)
```

We can conclude that there are no sign of multicollinearity among the independent variables as VIF of the independent variables are less than 10.

#### 5.4.7 Test for Non-Linearity

In the code chunk below, the [`ols_plot_resid_fit()`](https://olsrr.rsquaredacademy.com/reference/ols_plot_resid_fit.html) of **olsrr** package is used to perform linearity assumption test.

```{r}
#| code-fold: true
#| code-summary: "Non-Linearity test"

ols_plot_resid_fit(condo.mlr1)
```

We can conclude that the relationships between the dependent variable and independent variables are linear as most of the data poitns are scattered around the 0 line.

#### 5.4.8 Test for Normality Assumption

The code chunk below uses [`ols_plot_resid_hist()`](https://olsrr.rsquaredacademy.com/reference/ols_plot_resid_hist.html) of *olsrr* package to perform normality assumption test.

```{r}
#| code-fold: true
#| code-summary: "Normality Assumption Test"

ols_plot_resid_hist(condo.mlr1)
```

It is shown that the residual of the multiple linear regression model (i.e. condo.mlr1) resembles normal distribution.

#### 5.4.9 Testing for Spatial Autocorrelation

In order to perform spatial autocorrelation test, let us perform the following steps

1.  Convert *condo_resale.sf* from sf data frame into a **SpatialPointsDataFrame.**

2.  Convert *condo_resale.res.sf* from simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.

3.  Display the distribution of the residuals on an interactive map

```{r}
#| code-fold: true
#| code-summary: "Spatial autocorrelation test"

mlr.output <- as.data.frame(condo.mlr1$residuals)
condo_resale.res.sf <- cbind(condo_resale.sf, 
                        condo.mlr1$residuals) %>%
rename(`MLR_RES` = `condo.mlr1.residuals`)
condo_resale.sp <- as_Spatial(condo_resale.res.sf)

```

Let us visualise the spatial distribution using below code chunk

```{r}
#| code-fold: true
#| code-summary: "Mapping the data"

tm_shape(mpsz_svy21)+
  tmap_options(check.and.fix = TRUE) +
  tm_polygons(alpha = 0.4) +
tm_shape(condo_resale.res.sf) +  
  tm_dots(col = "MLR_RES",
          alpha = 0.6,
          style="quantile") +
  tmap_mode("view")+
  tm_view(set.zoom.limits = c(11,14))
```

The figure above reveal that there is sign of spatial autocorrelation.

Let us double check by performing Moran's I test

Following steps will be performed

1.  Compute the distance-based weight matrix by using [`dnearneigh()`](https://r-spatial.github.io/spdep/reference/dnearneigh.html) function of **spdep**.

2.  Convert the output neighbours lists (i.e. nb) into a spatial weights using nb2listw() of **spdep** packge.

3.  Conduct Moran's I test for residual spatial autocorrelation by using [`lm.morantest()`](https://r-spatial.github.io/spdep/reference/lm.morantest.html) of **spdep** package.

```{r}
#| code-fold: true
#| code-summary: "Moran's I test"

nb <- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)
nb_lw <- nb2listw(nb, style = 'W')
lm.morantest(condo.mlr1, nb_lw)

```

Since the Observed Global Moran I = 0.1424418 which is greater than 0, we can infer than the residuals resemble cluster distribution.

#### 5.5 Hedonic Pricing Models using GWmodel

Let us model hedonic pricing using both the fixed and adaptive bandwidth schemes

#### 5.5.1 Computing fixed bandwidth

In the code chunk below `bw.gwr()` of GWModel package is used to determine the optimal fixed bandwidth to use in the model.

```{r}
#| code-fold: true
#| code-summary: "Fixed bandwidth"

bw.fixed <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + 
                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + 
                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + 
                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + 
                     FAMILY_FRIENDLY + FREEHOLD, 
                   data=condo_resale.sp, 
                   approach="CV", 
                   kernel="gaussian", 
                   adaptive=FALSE, 
                   longlat=FALSE)
```

The result shows that the recommended bandwidth is 971.3398 metres

#### 5.5.2 GWRModel method - Fixed bandwidth

Let us calibrate the gwr model using fixed bandwidth and gaussian kernel.

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "GWR- Fixed bandwidth"

gwr.fixed <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + 
                         PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + 
                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + 
                         PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + 
                         FAMILY_FRIENDLY + FREEHOLD, 
                       data=condo_resale.sp, 
                       bw=bw.fixed, 
                       kernel = 'gaussian', 
                       longlat = FALSE)
```

```{r}
gwr.fixed
```

The report shows that the adjusted r-square of the gwr is 0.8430 which is significantly better than the global multiple linear regression model of 0.6472.

#### 5.5.3 GWRModel method - Adaptive bandwidth

Let us calibrate the gwr-absed hedonic pricing model by using adaptive bandwidth approach.

**Computing the adaptive bandwidth**

In the code chunk below `bw.gwr()` of GWModel package is used to determine the optimal fixed bandwidth with adaptive = TRUE to use in the model.

```{r}
#| code-fold: true
#| code-summary: "Adaptive Bandwidth"

bw.adaptive <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + 
                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + 
                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + 
                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + 
                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                      data=condo_resale.sp, 
                      approach="CV", 
                      kernel="gaussian", 
                      adaptive=TRUE, 
                      longlat=FALSE)
```

The result reveals that 30 is the recommended data points to be used.

#### Constructing the adaptive bandwidth gwr model

Let us calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and gaussian kernel as shown in the code chunk below.

```{r, warning=FALSE}
#| code-fold: true
#| code-summary: "GWR - Adaptive Bandwidth"

gwr.adaptive <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + 
                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + 
                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + 
                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + 
                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, 
                          data=condo_resale.sp, bw=bw.adaptive, 
                          kernel = 'gaussian', 
                          adaptive=TRUE, 
                          longlat = FALSE)
gwr.adaptive
```

It reveals that the adjusted r-square of the gwr is 0.8561 which is significantly better than the globel multiple linear regression model of 0.6472.

#### 5.5.4 Visualising GWR Output

The output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors. They are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its "data" slot in an object called **SDF** of the output list.

```{r}
#| code-fold: true
#| code-summary: "GWR Output"


condo_resale.sf.adaptive <- st_as_sf(gwr.adaptive$SDF) %>%
  st_transform(crs=3414)
condo_resale.sf.adaptive.svy21 <- st_transform(condo_resale.sf.adaptive, 3414)

gwr.adaptive.output <- as.data.frame(gwr.adaptive$SDF)
condo_resale.sf.adaptive <- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))

summary(gwr.adaptive$SDF$yhat)
```

#### 5.5.5 Visualising local R2

The code chunks below is used to create an interactive point symbol map to visualise local R2 values.

```{r}
#| code-fold: true
#| code-summary: "Mapping Local R2"

tmap_mode("view")
tm_shape(mpsz_svy21)+
  tm_polygons(alpha = 0.1) +
tm_shape(condo_resale.sf.adaptive) +  
  tm_dots(col = "Local_R2",
          border.col = "gray60",
          border.lwd = 1) +
  tm_view(set.zoom.limits = c(11,14))
```

We can see that the places which have darker points indicate that the local regression model fits well whereas very low values indicate the local model is performing poorly. Mapping these Local R2 values helps us to understand where GWR predicts well and where it predicts poorly which may provide clues about important variables that may be missing from the regression model.

#### 

5.5.6 Visualizing URA Planning Region

The code chunks below is used to create an static point symbol map to visualise local R2 by planning region (Here, Central region)

```{r}
#| code-fold: true
#| code-summary: "Mapping URA Planning region"

tm_shape(mpsz_svy21[mpsz_svy21$REGION_N=="CENTRAL REGION", ])+
  tm_polygons()+
tm_shape(condo_resale.sf.adaptive) + 
  tm_bubbles(col = "Local_R2",
           size = 0.15,
           border.col = "gray60",
           border.lwd = 1)+
  tmap_mode("plot")
```

In the above map, we can see that the places which have darker static point symbols indicate that the local regression model fits well whereas very low values indicate the local model is performing poorly.

### 6. Conclusion

So, in this study, we have seen in detail how to build model with geographically weighted regression and the various steps in preparing the final data such as test for normality assumption, tests for non-linearity, checking multi collinearity. Some of the new concepts mentioned in this study are building publication table using Ordinary Least Square Regression OLSR) method and gtsummary method. Stay tuned for upcoming sections....................

### 
