[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Geospatial Analytics",
    "section": "",
    "text": "Geospatial data analytics lets the eye recognize patterns like distance, proximity, contiguity and affiliation that are hidden in massive datasets. The visualization of spatial data also makes it easier to see how things are changing over time and where the change is most pronounced.Benefits of geospatial analytics include:\n\nEngaging insights — Seeing data in the context of a visual map makes it easier to understand how events are unfolding and how to react to those events.\nBetter foresight — Seeing how spatial conditions are changing in real time can help an organization better prepare for change and determine future action.\nTargeted solutions — Seeing location-based data helps organizations understand why some locations and countries, such as the United States, are more successful for business than others."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1(2).html",
    "href": "Hands-on_Ex/Hands-on_Ex1(2).html",
    "title": "Geospatial Data Wrangling",
    "section": "",
    "text": "2. Glimpse of Steps\nSome of the important steps performed in this exercise are as follows\n\ninstalling and loading sf and tidyverse packages into R environment\nimporting geospatial data by using appropriate functions of sf package\nimporting aspatial data by using appropriate function of readr package\nexploring the content of simple feature data frame by using appropriate Base R and sf functions\nassigning or transforming coordinate systems by using using appropriate sf functions\nconverting an aspatial data into a sf data frame by using appropriate function of sf package\nperforming geoprocessing tasks by using appropriate functions of sf package\nperforming data wrangling tasks by using appropriate functions of dplyr package and performing Exploratory Data Analysis (EDA) by using appropriate functions from ggplot2 package\n\n\n\n3. Data\nFollowing data sets are used:\n\nMP14_SUBZONE_WEB_PL - Master Plan 2014 Subzone Boundary (Web)\npre-schools-location-kml - Pre-Schools Location\nCyclingPath files\nlistings.csv - Latest version of Singapore Airbnb listing data\n\n\n\n4. Deep Dive into Geospatial Analytics\n\n4.1 Installing and loading the required libraries\nMajor packages used are\nsf - for importing, managing, and processing geospatial data\ntidyverse - for performing data science tasks such as importing, wrangling and visualising data.\nTidyverse consists of a family of R packages. Following packages are used\n\nreadr for importing csv data,\nreadxl for importing Excel worksheet,\ntidyr for manipulating data,\ndplyr for transforming data,\nggplot2 for visualising data\np_load function pf pacman package is used to install and load sf ,tidyverse and tmap packages into R environment.\n\n\npacman::p_load(sf, tidyverse, tmap)\n\n\n\n4.2. Importing Geospatial Data\n\n4.2.1 Importing polygon feature data in shapefile format\nThe code chunk below uses st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a polygon feature data frame.\nTwo arguments are used :\n\ndsn - destination : to define the data path\nlayer - to provide the shapefile name\n\n\nmpsz = st_read(dsn = \"data/geospatial\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `D:\\raveenaclr\\Geospatial Analytics\\Hands-on_Ex\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\n\n\n4.2.2 Importing polyline feature data in shapefile form\nThe code chunk below uses st_read() function of sf package to import CyclingPath shapefile into R as line feature data frame.\n\ncyclingpath = st_read(dsn =\"data/geospatial\", \n                         layer = \"CyclingPath\")\n\nReading layer `CyclingPath' from data source \n  `D:\\raveenaclr\\Geospatial Analytics\\Hands-on_Ex\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1625 features and 2 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 12711.19 ymin: 28711.33 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\n\nThe message above reveals that there are a total of 1625 features and 2 fields in CyclingPath linestring feature data frame and it is in svy21 projected coordinates system too.\n\n\n4.2.3 Importing GIS data in kml format\nThe code chunk below will be used to import the kml into R. Unlike previous iports, here complete path and the kml file extension are provided.\n\npreschool = st_read(\"data/geospatial/pre-schools-location-kml.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `D:\\raveenaclr\\Geospatial Analytics\\Hands-on_Ex\\data\\geospatial\\pre-schools-location-kml.kml' \n  using driver `KML'\nSimple feature collection with 1359 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\n\nThe message above reveals that preschool is a point feature data frame. There are a total of 1359 features and 2 fields. Different from the previous two simple feature data frame, preschool is in wgs84 coordinates system.\n\n\n\n4.3 Checking the Content of A Simple Feature Data Frame\nIn this sub-section, let us see what are the different ways to retrieve information related to the content of a simple feature data frame.\n\n4.3.1 Using st_geometry()\nOne of the the most common way is to use st_geometry() as shown in the code chunk below.\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((31495.56 30140.01, 31980.96 296...\n\n\nMULTIPOLYGON (((29092.28 30021.89, 29119.64 300...\n\n\nMULTIPOLYGON (((29932.33 29879.12, 29947.32 298...\n\n\nMULTIPOLYGON (((27131.28 30059.73, 27088.33 297...\n\n\nMULTIPOLYGON (((26451.03 30396.46, 26440.47 303...\n\n\nThe result only displays basic information of the feature class such as type of geometry, the geographic extent of the features and the coordinate system of the data.\n\n\n4.3.2 Using glimpse()\nLet us use glimpse() to learn more about the associated attribute information in the data frame.\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO <int> 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  <chr> \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  <chr> \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     <chr> \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N <chr> \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C <chr> \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   <chr> \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   <chr> \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    <chr> \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D <date> 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     <dbl> 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     <dbl> 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng <dbl> 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area <dbl> 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   <MULTIPOLYGON [m]> MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\n\nIt reveals the data type of each fields.\n\n\n4.3.3 Using head()\nLet us use glimpse() to reveal complete information of a feature object.\n\nhead(mpsz, n=5)  \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n\n\n\n\n\n4.4. Plotting Geospatial Data\nLet us use plot() of R Graphic to visualise the geospatial features.\n\nplot(mpsz)\n\nWarning: plotting the first 9 out of 15 attributes; use max.plot = 15 to plot\nall\n\n\n\n\n\nThe default plot of an sf object is a multi-plot of all attributes, up to a reasonable maximum as shown above.\nPlot geometry\nLet us now plot only the geometry by using the code chunk below.\n\nplot(st_geometry(mpsz))\n\n\n\n\nPlot palnning area\nSimilarly, we can also choose the plot the sf object by using a specific attribute as shown in the code chunk below.\n\nplot(mpsz[\"PLN_AREA_N\"])\n\n\n\n\n\n\n4.5 Mapping Projection\nIn this section, let us see how to project a simple feature data frame from one coordinate system to another coordinate system. This process is called projection transformation.\n\n4.5.1 Assigning EPSG code\nLet us first check the coordinate system of mpsz simple feature data frame by using st_crs() of sf package as shown in the code chunk below. In order to assign the correct EPSG code to mpsz data frame, st_set_crs() of sf package is used as shown in the code chunk below.\n\nst_crs(mpsz)\n\nCoordinate Reference System:\n  User input: SVY21 \n  wkt:\nPROJCRS[\"SVY21\",\n    BASEGEOGCRS[\"SVY21[WGS84]\",\n        DATUM[\"World Geodetic System 1984\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ID[\"EPSG\",6326]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"Degree\",0.0174532925199433]]],\n    CONVERSION[\"unnamed\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"Degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"(E)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]],\n        AXIS[\"(N)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]]\n\nmpsz3414 <- st_set_crs(mpsz, 3414)\n\nWarning: st_crs<- : replacing crs does not reproject data; use st_transform for\nthat\n\nst_crs(mpsz3414)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]\n\n\n\n\n4.5.2 Transforming the projection of preschool from wgs84 to svy21\nAs we already know that preschool shapefile is in wgs84 coordinate system, st_transform() of sf package should be used instead of st_crs(). This is because we need to reproject preschool from one coordinate system to another coordinate system mathemetically.\n\npreschool3414 <- st_transform(preschool, \n                              crs = 3414)\n\n\n\n\n4.6 Importing & Converting an Aspatial Data\nIn this section, let us see how to import an aspatial data into R environment and save it as a tibble data frame and later convert it into a simple feature data frame. read_csv() of readr package to import listings.csv as shown the code chunk below.\n\nlistings <- read_csv(\"data/aspatial/listings.csv\")\n\nRows: 4252 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (5): name, host_name, neighbourhood_group, neighbourhood, room_type\ndbl  (10): id, host_id, latitude, longitude, price, minimum_nights, number_o...\ndate  (1): last_review\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n4.6.1 Creating simple feature df from aspatial df\nThe code chunk below converts listings data frame into a simple feature data frame by using st_as_sf() of sf packages\n\nlistings_sf <- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %>%\n  st_transform(crs = 3414)\n\nLet us now examine the content of this newly created simple feature data frame using glimpse() as shown in the code chunk below\n\n\n\n4.7 Geoprocessing with sf package\nIn this section, you will learn how to perform two commonly used geoprocessing functions, namely buffering and point in polygon count.\n\n4.7.1 Buffering\nTo determine the extend of the land need to be acquired and their total area,\n\nFirstly, st_buffer() of sf package is used to compute the 5-meter buffers around cycling paths.\nThis is followed by calculating the area of the buffers as shown in the code chunk below.\nLastly, sum() is used to derive the total land involved.\n\n\nbuffer_cycling <- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\nbuffer_cycling$AREA <- st_area(buffer_cycling)\nsum(buffer_cycling$AREA)\n\n773143.9 [m^2]\n\n\n\n\n4.7.2 Point in polygon count\nTo determine the numbers of pre-schools in each Planning Subzone,\n\nFirstly, identify pre-schools located inside each Planning Subzone by using st_intersects(). Next, length() is used to calculate numbers of pre-schools that fall inside each planning subzone.\nBefore proceeding, we can check the summary statistics of the newly derived PreSch Count field by using summary().\nTo list the planning subzone with the most number of pre-school, the top_n() of dplyr package is used as shown in the code chunk below.\n\n\nmpsz3414$`PreSch Count`<- lengths(st_intersects(mpsz3414, preschool3414))\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   2.000   4.207   6.000  37.000 \n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 23449.05 ymin: 46001.23 xmax: 25594.22 ymax: 47996.47\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      290          3 WOODLANDS EAST    WDSZ03      N  WOODLANDS         WD\n      REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR\n1 NORTH REGION       NR C90769E43EE6B0F2 2014-12-05 24506.64 46991.63\n  SHAPE_Leng SHAPE_Area                       geometry PreSch Count\n1   6603.608    2553464 MULTIPOLYGON (((24786.75 46...           37\n\n\n\n\n4.7.3 Computing Density of preschool by Planning Subzone\nTo calculate the density of pre-school by planning subzone,\n\nFirstly, let us use st_area() of sf package to derive the area of each planning subzone.\nNext, mutate() of dplyr package is used to compute the density by using the code chunk below.\n\n\nmpsz3414$Area <- mpsz3414 %>%\n  st_area()\nmpsz3414 <- mpsz3414 %>%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)\n\n\n\n\n4.8 Exploratory Data Analysis\nIn this section, let us see how to use appropriate ggplot2 functions to create functional and insightful statistical graphs for EDA purposes.\n\n4.8.1 How the preschool density is distributed?\nFirstly, let us plot a histogram to reveal the distribution of PreSch Density using hist() as shown in the code chunk below.\n\nhist(mpsz3414$`PreSch Density`)\n\n\n\n\nThis function has limitation for customization despite ease of use\nIn the code chunk below, appropriate ggplot2 functions will be used which is layered set of graphics\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\n\n4.8.2 What is the relationship between Pre-school Density and Pre-school Count?\nUsing ggplot2 method, let us plot a scatterplot showing the relationship between Pre-school Density and Pre-school Count.\n\nggplot(mpsz3414, \n       aes(x=as.numeric(`PreSch Density`), \n           y=`PreSch Count`)) + \n    geom_point()+\n    coord_cartesian(xlim=c(0,40),\n                  ylim=c(0,40))+\n    labs(title=\"Are the schools densely located?\",\n         subtitle = \"Relationship between no. of schools and density of schools per sq. km\",\n         x='Pre-school density (per km sq.)',\n         y= 'Pre School count')+\n    theme(axis.title.y=element_text(angle=0),\n          axis.title.y.left = element_text(vjust = 0.5))\n\n\n\n\n\n\n\n\n5. Conclusion & Key Takeaways\nIn this exercise we have covered the initial steps of importing geospatial data, aspatial data and projection transformation using appropriate R packages. We have also learnt how to view the content of the simple feature data frame using various functions. Finally, we have seen the ways to perform Exploratory Data Analysis (EDA) using ggplot2 functions. Let us see more details in the further sections. Stay tuned…………"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex1.html",
    "href": "Hands-on_Ex/Hands-on_Ex1.html",
    "title": "Choropleth Mapping with R",
    "section": "",
    "text": "2. Introduction\nChoropleth mapping involves the symbolisation of enumeration units, such as countries, provinces, states, counties or census units, using area patterns or graduated colors.\nIn this section, let us see how to plot functional and truthful choropleth maps by using an R package called tmap package. The world map shows distribution of population density among all countries.\n\n\n\n\n\n\n\n3. Glimpse of Steps\nSome of the important steps performed in this study are as follows\n\ninstalling and loading required libraries into R environment\nperforming data wrangling using necessary transformation functions\nPlotting a choropleth map using qtm() and tmap’s elements\nDrawing a base map and leveraging its features\nPlotting choropleth maps with built-in classification methods and custom breaks\n\n\n\n4. Data\nFollowing two data sets are used:\n\nMP14_SUBZONE_WEB_PL - Master Plan 2014 Subzone Boundary in ESRI shapefile format. This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level.\nrespopagesextod2011to2020.csv - Singapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format. This is an aspatial data fie.\n\n\n5.1 Installing libraries and Importing files\np_load function pf pacman package is used to install and load sf ,tidyverse and tmap packages into R environment. Let us import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz using st_read() function. Next, we will import respopagsex2000to2018.csv file into RStudio and save the file into an R dataframe called popagsex using read_csv() function as shown in the code chunk below.\n\npacman::p_load(sf, tidyverse, tmap)\nmpsz <- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `D:\\raveenaclr\\Geospatial Analytics\\Hands-on_Ex\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\npopdata <- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\nRows: 984656 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): PA, SZ, AG, Sex, TOD\ndbl (2): Pop, Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n5.2 Data Preparation\nData has to be prepared in such a way that choropleth mapping can be performed.\nThe data table should include the variables PA, SZ, YOUNG, ECONOMY ACTIVE, AGED, TOTAL, DEPENDENCY.\n\n5.2.1 Data Wrangling\nThe code chunk first filters the year 2020 and performs the following\n\nYOUNG: age group 0 to 4 until age groyup 20 to 24,\nECONOMY ACTIVE: age group 25-29 until age group 60-64,\nAGED: age group 65 and above,\nTOTAL: all age group, and\nDEPENDENCY: the ratio between young and aged against economy active group\nSome of the transformation functions used are as follows\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 <- popdata %>%\n  filter(Time == 2020) %>%\n  group_by(PA, SZ, AG) %>%\n  summarise(`POP` = sum(`Pop`)) %>%\n  ungroup()%>%\n  pivot_wider(names_from=AG, \n              values_from=POP) %>%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %>%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%>%\nmutate(`AGED`=rowSums(.[16:21])) %>%\nmutate(`TOTAL`=rowSums(.[3:21])) %>%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %>%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n`summarise()` has grouped output by 'PA', 'SZ'. You can override using the\n`.groups` argument.\n\n\n\n\n5.2.2 Joining attribute and geo spatial data\nWe have to convert the values in PA and SZ fields to uppercase as the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase. Next, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\npopdata2020 <- popdata2020 %>%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %>%\n  filter(`ECONOMY ACTIVE` > 0)\n\nWarning: `funs()` was deprecated in dplyr 0.8.0.\nPlease use a list of either functions or lambdas: \n\n  # Simple named list: \n  list(mean = mean, median = median)\n\n  # Auto named with `tibble::lst()`: \n  tibble::lst(mean, median)\n\n  # Using lambdas\n  list(~ mean(., trim = .2), ~ median(., na.rm = TRUE))\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\n\n\n5.3 Choropleth Mapping Geospatial Data\nThere are two ways of creating thematic map. They are\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\n\n5.3.1 Using qtm()\nThe easiest and quickest to draw a choropleth map using tmap is using qtm(). It provides a good default visualisation.The code chunk below will draw a cartographic standard choropleth map as shown below.\n\ntmap_mode(\"plot\")\n\ntmap mode set to plotting\n\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\n\n5.3.2 Using tmap elements\nAlthough qtm() helps to create a thematic map easily, it lacks customisation. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n5.3.3 Drawing a basemap\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\n\n5.3.4 Using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons().\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\n\n5.3.5 Using tm_fill() and tm_border()\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\n\n\n5.4 Data Classification\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\nTo define a data classification method, the style argument of tm_fill() or tm_polygons() will be used.\n\n5.4.1 Choropleth maps with built-in classification methods - jenks\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks. Out of all, the code chunk below shows a quantile data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n5.4.2 Choropleth maps with built-in classification methods - equal\nThe code chunk below shows a equal data classification that used 5 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nIt can be observed that the distribution of quantile data classification method are more evenly distributed than equal data classification method.\n\n\n5.4.3 Using Custom break\nwe set break point at 0.60, 0.70, 0.80, and 0.90. In addition, we also need to include a minimum and maximum, which we set at 0 and 100.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\nWarning: Values have found that are higher than the highest break\n\n\n\n\n\n\n\n\n5.5. Various colour schemes\ntmap supports colour ramps either defined by the user or a set of predefined colour ramps from the RColorBrewer package.\n\n5.5.1 Using ColourBrewer palette - Blues\nTo change the colour, we assign the preferred colour to palette argument of tm_fill() as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n5.5.2 Using ColourBrewer palette - Greens\nTo reverse the colour shading, we can add a “-” prefix.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n\n5.6. Map Layouts\nMap layout refers to the combination of all map elements into a cohensive map. Map elements include among others the objects to be mapped, the title, the scale bar, the compass, margins and aspects ratios.\n\n5.6.1 Map Legend\nIn tmap, several legend options are provided to change the placement, format and appearance of the legend.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\n\n5.6.2 Map Style\ntmap allows a wide variety of layout settings to be changed. They can be called by using tmap_style().\nThe code chunk below shows the classic style is used.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\ntmap style set to \"classic\"\n\n\nother available styles are: \"white\", \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"watercolor\" \n\n\n\n\n\n\n\n\n5.7 Cartogram\ntmap also also provides arguments to draw other map furniture such as compass, scale bar and grid lines.\nIn the code chunk below, tm_compass(), tm_scale_bar() and tm_grid() are used to add compass, scale bar and grid lines onto the choropleth map.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\n\n5.8. Drawing multiple small choropleth maps\nSmall multiple maps, also referred to as facet maps, are composed of many maps arrange side-by-side, and sometimes stacked vertically.\n\n5.8.1 Assigning values to aesthetic arguments\nLet us create small multiple choropleth maps by defining ncols in tm_fill()\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\ntmap style set to \"white\"\n\n\nother available styles are: \"gray\", \"natural\", \"cobalt\", \"col_blind\", \"albatross\", \"beaver\", \"bw\", \"classic\", \"watercolor\" \n\n\n\n\n\nLet us now create small multiple choropleth maps by assigning multiple values to at least one of the aesthetic arguments.\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\n\n5.8.2 Defining a group-by variable in tm_facets()\nLet us now create multiple small choropleth maps by using tm_facets().\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\nWarning: The argument drop.shapes has been renamed to drop.units, and is\ntherefore deprecated\n\n\n\n\n\n\n\n5.8.3 By creating multiple stand-alone maps with tmap_arrange()\nLets see how to create multiple small choropleth maps by creating multiple stand-alone maps with tmap_arrange().\n\nyoungmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\n\n\n5.9 Mappping Spatial Object Meeting a Selection Criterion\nWe can also use selection function to map spatial objects meeting the selection criteria instead of creating multiple small choropleth maps.\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)+\n  tm_compass(type=\"8star\",\n                       position=c(\"right\", \"top\"))\n\nWarning in pre_process_gt(x, interactive = interactive, orig_crs =\ngm$shape.orig_crs): legend.width controls the width of the legend within a map.\nPlease use legend.outside.size to control the width of the outside legend\n\n\n\n\n\n\n\n\n6. Conclusion & Key Takeaways\nIn this exercise we have seen how to play with choropleth maps in detail starting from creating quick thematic maps using qtm() function to customising the asthetics od choropleth maps. We have also understood how to perform data classificatin and which one is suitable during the analysis. Further, lets deep dive into spatial weights in upcoming section. Stay tuned…..\n\n\n7. References\nSimple Features for R\nStandardized Support for Spatial Vector Data\nThematic Maps in R"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex2.html",
    "href": "Hands-on_Ex/Hands-on_Ex2.html",
    "title": "Global and local measures of spatial auto correlation",
    "section": "",
    "text": "2. Introduction\nIn this section, let’s see how to compute Global and Local Measure of Spatial Autocorrelation (GLSA). In spatial policy, one of the main development objective of the local govenment and planners is to ensure equal distribution of development in the province. Our task is to apply appropriate spatial statistical methods to discover if development are even distributed geographically.\n\n\n\n\n\n\n\n3. Glimpse of Steps\nSome of the important steps performed in this study are as follows\n\nimporting geospatial data using appropriate function(s) of sf package,\nimporting csv file using appropriate function of readr package,\nperforming relational join using appropriate join function of dplyr package,\ncomputing Global Spatial Autocorrelation (GSA) statistics by using appropriate functions of spdep package,\n\nplot Moran scatterplot,\ncompute and plot spatial correlogram using appropriate function of spdep package.\n\ncomputing Local Indicator of Spatial Association (LISA) statistics for detecting clusters and outliers by using appropriate functions spdep package;\ncomputing Getis-Ord’s Gi-statistics for detecting hot spot or/and cold spot area by using appropriate functions of spdep package; and\nvisualising the analysis output by using tmap package.\n\n\n\n4. Data\nFollowing two data sets are used:\n\nHunan province administrative boundary layer at county level. This is a geospatial data set in ESRI shapefile format\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012\n\n\n\n5. Deep Dive into Map Analysis\n\n5.1 Installing libraries and Importing files\np_load function pf pacman package is used to install and load sf ,tidyverse and tmap packages into R environment. First, let us import Hunan shapefile into R using st_read() of sf package. The imported shapefile will be simple features Object of sf. Next, let us import Hunan_2012.csv into R by using read_csv() of readr package. The output is R dataframe class.\n\npacman::p_load(sf, spdep, tmap, tidyverse)\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `D:\\raveenaclr\\Geospatial Analytics\\Hands-on_Ex\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n5.2 Data Wrangling\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan <- left_join(hunan,hunan2012)\n\nJoining, by = \"County\"\n\n\n\n\n5.3 Visualising Regional Development Indicator\nNow, let us prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nequal <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n   tm_compass(type=\"8star\",\n             position=c(\"right\", \"top\"))+\n  tm_layout(main.title = \"Equal classification\",\n            main.title.size = 1.5,\n            main.title.position = \"center\",\n            main.title.fontface = \"bold\")\n\nquantile <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\",\n             position=c(\"right\", \"top\"))+\n  tm_layout(main.title = \"Quantile classification\",\n            main.title.size = 1.5,\n            main.title.position = \"center\",\n            main.title.fontface = \"bold\")\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)\n\n\n\n\nAnalysis:\nIt is seen that map classification with quantile interval is better than map classification with equal interval as the former clearly spreads out rather clustering into one segment. Hence, quantile style will be used in further analysis.\n\n\n5.4 Global Spatial Autocorrelation\n\n5.4.1 Computing Contiguity Spatial Weights\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights of the study area. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area. The code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q <- poly2nb(hunan, \n                queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\n\n\n5.4.2 Row-standardised weights matrix\nNext, let us assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight (style=“W”).\n\nrswm_q <- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\n\n\n5.4.3 Global Spatial Autocorrelation: Moran’s I\nThe code chunk below performs Moran’s I statistical testing using moran.test() of spdep.\n\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\nInterpretation:\nH1 (Alternative Hypothesis) - GDPPC distribution is spatially dependent.\nH0 (Null Hypothesis) - GDPPC distribution is a random phenomenon.\nAt 95% confidence interval, p=value is less than alpha (0.05). Hence it is significant and so null hypothesis can be rejected and confirm that GDPPC is not randomly distributed and the spatial distribution of high values and/or low values are more spatially clustered and observations tend to be similar.\n\n\n\nComputing Monte Carlo Moran’s I\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nSimilarly, even after performing 1000 permutations, it is confirmed that at 95% confidence interval, p value is significant (less than 0.05) and Z score is positive. So null hypothesis can be rejected and confirm that GDPPC is not randomly distributed and the spatial distribution of high values and/or low values are more spatially clustered and observations tend to be similar.\nLet us examine the simulated Moran’s I test statistics in detail by plotting the distribution of the statistical values as a histogram by using the code chunk below.\n\nmean(bperm$res[1:999])\n\n[1] -0.01504572\n\nvar(bperm$res[1:999])\n\n[1] 0.004371574\n\nsummary(bperm$res[1:999])\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.18339 -0.06168 -0.02125 -0.01505  0.02611  0.27593 \n\n\n\nhist(bperm$res, \n     freq=TRUE, \n     breaks=20, \n     xlab=\"Simulated Moran's I\")\nabline(v=0, \n       col=\"red\") \n\n\n\n\nInterpretation:\nSimulated values are both positive and negative. The positive values indicate that the values are clustered, observations tend to be similar; whereas the negative values on the left of 0 indicate that the values are dispersed and observations tend to be dissimilar.\nChallenge: Plotting using ggplot\nNow the same visualisation is plotted using ggplot instead of base graph.\n\nres values are converted to dataframe and passed as ggplot data\ngeom_histogram() is used on top of ggplot to plot the histogram\nvertical line is plotted at value 0 using geom_vline()\n\n\np1 <- ggplot(data=as.data.frame(bperm[7]), \n       aes(x= res)) +\n      geom_histogram(bins=20, \n                     color=\"black\", \n                     fill=\"light blue\") +\n      geom_vline(aes(xintercept=0),\n                     color=\"red\", \n                     linetype=\"dashed\", \n                     size=1)+\n      labs(title = \"Moran's I test statistic distribution\",\n       x = \"Moran's I\",\n       y = \"Frequency\")\np1\n\n\n\n\nInterpretation:\nSimulated values are both positive and negative. The positive values indicate that the values are clustered, observations tend to be similar; whereas the negative values on the left of 0 indicate that the values are dispersed and observations tend to be dissimilar.\n\n5.4.5 Global Spatial Autocorrelation: Geary’s\nThe code chunk below performs Geary’s C test for spatial autocorrelation by using geary.test() of spdep.\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\n\nAnalysis:\nH0 : The attribute is randomly distributed among the features in Hunan\nH1: The attribute is not randomly distributed among the features in Hunan\nAs the p-value is significant, null hypothesis can be rejected, and we can conclude that the attribute is not randomly distributed and the spatial distribution in the dataset is more spatially clustered than the expected value.\n\n\n\nComputing Monte Carlo Geary’s C\nThe code chunk below performs permutation test for Geary’s C statistic by using geary.mc() of spdep.\n\nset.seed(1234)\nbperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\nbperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\nInterpretation:\nHere at 95% confidence interval, p value is less than 0.05 and Geary’s C value (Z score)is less than 1. So, we can reject the null hypothesis and confirm that the values are clustered, observations tend to be similar.\n\nmean(bperm$res[1:999])\n\n[1] 1.004402\n\nvar(bperm$res[1:999])\n\n[1] 0.007436493\n\nsummary(bperm$res[1:999])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.7142  0.9502  1.0052  1.0044  1.0595  1.2722 \n\n\nNext, let us plot a histogram to reveal the distribution of the simulated values by using the code chunk below.\n\nhist(bperm$res, freq=TRUE, breaks=20, xlab=\"Simulated Geary c\")\nabline(v=1, col=\"red\") \n\n\n\n\nInterpretation:\nAt c = 1, observations are arranged randomly over space. and half of the values here are less than 1 and almost half of the values are greater than 1. Hence we can confirm that the points which have larger c value (>1) are said to be dispersed, observations tend to be dissimilar and all the small values <1 indicate that they are clustered and observations tend to be similar.\n\n\n5.5 Spatial Correlogram\nSpatial correlograms are great to examine patterns of spatial autocorrelation in your data or model residuals. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.\n\n5.5.1 Compute Moran’s I correlogram\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 6-lag spatial correlogram of GDPPC. The global spatial autocorrelation used in Moran’s I and Geary’s C. The plot() of base Graph is then used to plot the output.\nThe plot() of base graph is then used to plot the output and the plots are arranged using par(mfrow()) parameter, here it is 1 row and 2 columns.\n\npar(mfrow=c(1,2))\nMI_corr <- sp.correlogram(wm_q, \n                          hunan$GDPPC,\n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr,\n     main =\"Moran's I error plot\")\n\nGC_corr <- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr,\n     main =\"Geary's C error plot\")\n\n\n\n\nInterpretation:\nThe plots reveal that the relationship between Moran’s I and Geary’C test statistic values are inverse. Moran’s I values approach -1 whereas Geary’s C test values approach 2 for subsequent lags which indicates that quite dissimilar values tend to form a cluster.\nBy plotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInterpretation:\nBoth Moran’s I and Geary’s C test results show that at 90% confidence, (alpha value = 0.01), most of the autocorrelated values are significant. And hence we can reject the null hypothesis stating the feature are random and with statistical evidences, we can confirm the following\n\nObserved spatial pattern of GDPPC is spatially dependent and not equally likely as any other spatial pattern.\nGDPPC at one location depends on values at other (neighbouring) locations.\n\n\n\n\n5.6 Cluster and Outlier Analysis\nLocal Indicators of Spatial Association or LISA are statistics that evaluate the existence of clusters in the spatial arrangement of a given variable.\n\n5.6.1 Computing local Moran’s I\nThe code chunks below are used to compute local Moran’s I of GDPPC2012 at the county level.\n\nfips <- order(hunan$County)\nlocalMI <- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\n\n\n5.6.2 Mapping the local Moran’s I\nThe code chunks below can be used to perform the task. The out SpatialPolygonDataFrame is called hunan.localMI.\n\nhunan.localMI <- cbind(hunan,localMI) %>%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\n\n\n5.6.3 Mapping local Moran’s I values\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code chinks below.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)+\n  tm_compass(type=\"8star\",\n             position=c(\"right\", \"top\"))+\n  tm_layout(main.title = \"Local Moran I spatial distribution\",\n            main.title.size = 1.5,\n            main.title.position = \"center\",\n            main.title.fontface = \"bold\")\n\nVariable(s) \"Ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\nAnalysis:\nFrom the above map, we can understand the following\n- Cluster - All positive valued region have neighboring features with similarly high or low attribute values\n- Outlier - All negative valued regions have neighboring features with dissimilar values\n\n\n5.6.4 Mapping local Moran’s I p-values\nThe code chunks below produce a choropleth map of Moran’s I p-values by using functions of tmap package.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)+\n  tm_compass(type=\"8star\",\n             position=c(\"right\", \"top\"))+\n  tm_layout(main.title = \"Local Moran p vlaues spatial distribution\",\n            main.title.size = 1.5,\n            main.title.position = \"center\",\n            main.title.fontface = \"bold\")\n\n\n\n\nAnalysis:\nFrom the above map, we can understand the following\n- For every corresponding I value, we can locate the regions which are significant or not.\n- Darker the region, higher the significance\n\n\n5.6.5 Mapping both local Moran’s I values and p-values\nLet us now plot both the local Moran’s I values map and its corresponding p-values map next to each other for effective interpretation.\n\nlocalMI.map <- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\", \n          title = \"local moran statistics\") +\n  tm_borders(alpha = 0.5)+\n   tm_compass(type=\"8star\",\n             position=c(\"right\", \"top\"))+\n  tm_layout(main.title = \"Local Moran I spatial distribution\",\n            main.title.size = 1.5,\n            main.title.position = \"center\",\n            main.title.fontface = \"bold\")\n\npvalue.map <- tm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)+\n   tm_compass(type=\"8star\",\n             position=c(\"right\", \"top\"))+\n  tm_layout(main.title = \"Local Moran p value spatial distribution\",\n            main.title.size = 1.5,\n            main.title.position = \"center\",\n            main.title.fontface = \"bold\")\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)\n\nVariable(s) \"Ii\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\nAnalysis:\nFrom the left map, we can understand the following\n- Cluster - All positive valued region have neighboring features with similarly high or low attribute values\n- Outlier - All negative valued regions have neighboring features with dissimilar values\nFrom the right map, we can understand the following\n- For every corresponding I value, we can locate the regions which are significant or not.\n- Darker the region, higher the significance.\n\n\n\n5.7 Creating a LISA Cluster Map\nThe LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.\n\n5.7.1 Plotting Moran scatterplot\nThe code chunk below plots the Moran scatterplot of GDPPC 2012 by using moran.plot() of spdep.\n\nnci <- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\n\n\n\n\n5.7.2 Plotting Moran scatterplot with standardised variable\nFirst let us use scale() to centers and scales the variable. Centering is done by subtracting the mean (omitting NAs) the corresponding columns, and scaling is done by dividing the (centered) variable by their standard deviations.\n\nhunan$Z.GDPPC <- scale(hunan$GDPPC) %>% as.vector\nnci2 <- moran.plot(hunan$Z.GDPPC, rswm_q,\n                   labels=as.character(hunan$County),\n                   xlab=\"z-GDPPC 2012\", \n                   ylab=\"Spatially Lag z-GDPPC 2012\")\n\n\n\n\n\n\n5.7.3 Preparing LISA map classes\n\nFirst, let us center the variable of interest around its mean.\nIt is followed by centering the local Moran’s around the mean.\nNext, let us set a statistical significance level for the local Moran.\nWe have defined high-high, low-low, low-high and high-low categories.\nLastly, places non-significant Moran in the category 0.\n\n\nquadrant <- vector(mode=\"numeric\",length=nrow(localMI))\nDV <- hunan$GDPPC - mean(hunan$GDPPC)     \nC_mI <- localMI[,1] - mean(localMI[,1])    \nsignif <- 0.05\nquadrant[DV >0 & C_mI>0] <- 4      \nquadrant[DV <0 & C_mI<0] <- 1      \nquadrant[DV <0 & C_mI>0] <- 2\nquadrant[DV >0 & C_mI<0] <- 3\nquadrant[localMI[,5]>signif] <- 0\n\n\n\n5.7.4 Plotting LISA map\nNow, let us build the LISA map by using the code chunks below.\n\nhunan.localMI$quadrant <- quadrant\ncolors <- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)+\n   tm_compass(type=\"8star\",\n             position=c(\"right\", \"top\"))+\n  tm_layout(main.title = \"LISA Map\",\n            main.title.size = 1.5,\n            main.title.position = \"center\",\n            main.title.fontface = \"bold\")\n\n\n\n\nAnalysis:\nWe can see that GDPPC spatial distribution is positively autocorrelated in the northern region i.e. those are associated with relatively high values of the surrounding locations.\nLet us now plot both the local Moran’s I values map and its corresponding p-values map next to each other for effective interpretation.\n\ngdppc <- qtm(hunan, \"GDPPC\")\n\nhunan.localMI$quadrant <- quadrant\ncolors <- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap <- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          popup.vars = c(\"\")) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5)+\n   tm_compass(type=\"8star\",\n             position=c(\"right\", \"top\"))+\n  tm_layout(main.title = \"LISA Map\",\n            main.title.size = 1.5,\n            main.title.position = \"center\",\n            main.title.fontface = \"bold\")\n\ntmap_arrange(gdppc, LISAmap, asp=1, ncol=2)\n\n\n\n\nAnalysis:\nIn left map with normal choropleth, GDPCC distribution is all over place. We couldn’t find any significant regions whereas in the right side map, we can see that spatial distribution of GDPPC is positively autocorrelated in the eastern region i.e. those are associated with relatively high values of the surrounding locations.\n\n\n\n5.8 Hot Spot and Cold Spot Area Analysis\nBeside detecting cluster and outliers, localised spatial statistics can be also used to detect hot spot and/or cold spot areas.\n\n5.8.1 Deriving the centroid\nTo get our longitude values we map the st_centroid() function over the geometry column of us.bound. This allows us to get only the longitude, which is the first value in each centroid. We do the same for latitude with one key difference. We access the second value per each centroid. Now that we have latitude and longitude, we use cbind to put longitude and latitude into the same object.\n\nlongitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\nlatitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\ncoords <- cbind(longitude, latitude)\n\n\n\n5.8.2 Determining the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\n#coords <- coordinates(hunan)\nk1 <- knn2nb(knearneigh(coords))\nk1dists <- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n5.8.3 Computing fixed distance weight matrix\nNow, let us compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d62 <- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\nwm62_lw <- nb2listw(wm_d62, style = 'B')\nsummary(wm62_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \nLink number distribution:\n\n 1  2  3  4  5  6 \n 6 15 14 26 20  7 \n6 least connected regions:\n6 15 30 32 56 65 with 1 link\n7 most connected regions:\n21 28 35 45 50 52 82 with 6 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1   S2\nB 88 7744 324 648 5440\n\n\n\n\n5.8.4 Computing adaptive distance weight matrix\nIt is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn <- knn2nb(knearneigh(coords, k=8))\nknn\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\nknn_lw <- nb2listw(knn, style = 'B')\nsummary(knn_lw)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 704 \nPercentage nonzero weights: 9.090909 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n 8 \n88 \n88 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n88 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 with 8 links\n\nWeights style: B \nWeights constants summary:\n   n   nn  S0   S1    S2\nB 88 7744 704 1300 23014\n\n\n\n\n\n5.9 Computing Gi statistics\n\n5.9.1 Gi statistics using fixed distance\nThe Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\n\nfips <- order(hunan$County)\ngi.fixed <- localG(hunan$GDPPC, wm62_lw)\ngi.fixed\n\n [1]  0.436075843 -0.265505650 -0.073033665  0.413017033  0.273070579\n [6] -0.377510776  2.863898821  2.794350420  5.216125401  0.228236603\n[11]  0.951035346 -0.536334231  0.176761556  1.195564020 -0.033020610\n[16]  1.378081093 -0.585756761 -0.419680565  0.258805141  0.012056111\n[21] -0.145716531 -0.027158687 -0.318615290 -0.748946051 -0.961700582\n[26] -0.796851342 -1.033949773 -0.460979158 -0.885240161 -0.266671512\n[31] -0.886168613 -0.855476971 -0.922143185 -1.162328599  0.735582222\n[36] -0.003358489 -0.967459309 -1.259299080 -1.452256513 -1.540671121\n[41] -1.395011407 -1.681505286 -1.314110709 -0.767944457 -0.192889342\n[46]  2.720804542  1.809191360 -1.218469473 -0.511984469 -0.834546363\n[51] -0.908179070 -1.541081516 -1.192199867 -1.075080164 -1.631075961\n[56] -0.743472246  0.418842387  0.832943753 -0.710289083 -0.449718820\n[61] -0.493238743 -1.083386776  0.042979051  0.008596093  0.136337469\n[66]  2.203411744  2.690329952  4.453703219 -0.340842743 -0.129318589\n[71]  0.737806634 -1.246912658  0.666667559  1.088613505 -0.985792573\n[76]  1.233609606 -0.487196415  1.626174042 -1.060416797  0.425361422\n[81] -0.837897118 -0.314565243  0.371456331  4.424392623 -0.109566928\n[86]  1.364597995 -1.029658605 -0.718000620\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm62_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\n\nhunan.gi <- cbind(hunan, as.matrix(gi.fixed)) %>%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\n\n\n5.9.2 Mapping Gi values with fixed distance weights\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc <- qtm(hunan, \"GDPPC\")\n\nGimap <-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5)+\n  tm_layout(main.title = \"Gi map (Fixed Distance)\",\n            main.title.size = 1.5,\n            main.title.position = \"center\",\n            main.title.fontface = \"bold\")+\n  tm_compass(type=\"8star\",\n             position=c(\"right\", \"top\"))\n\n\ntmap_arrange(gdppc, Gimap, asp=1, ncol=2)\n\nVariable(s) \"gstat_fixed\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\nAnalysis:\nPlotting Gi map helps us to understand that GDPPC is associated with relatively high values of the surrounding locations (clustered- hotspot areas) in the eastern region of Hunan and are associated with relatively low values in surrounding locations (scattered - coldspot) in northern and southern region of Hunan.\nThe code chunk below are used to compute the Gi values for GDPPC2012 by using an adaptive distance weight matrix (i.e knb_lw).\n\nfips <- order(hunan$County)\ngi.adaptive <- localG(hunan$GDPPC, knn_lw)\nhunan.gi <- cbind(hunan, as.matrix(gi.adaptive)) %>%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\n\n\n5.9.4 Mapping Gi values with adaptive distance weights\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc<- qtm(hunan, \"GDPPC\")\n\nGimap <- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"local Gi\") + \n  tm_borders(alpha = 0.5)+\n  tm_layout(main.title = \"Gi map (Adaptive Distance)\",\n            main.title.size = 1.5,\n            main.title.position = \"center\",\n            main.title.fontface = \"bold\")+\n  tm_compass(type=\"8star\",\n             position=c(\"right\", \"top\"))\n\ntmap_arrange(gdppc, \n             Gimap, \n             asp=1, \n             ncol=2)\n\nVariable(s) \"gstat_adaptive\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\nAnalysis:\nSimilarly, Gi Map with adaptive distance shows that GDPPC is associated with relatively high values of the surrounding locations (clustered- hotspot areas) in the eastern region of Hunan and are associated with relatively low values in surrounding locations (scattered - coldspot) in northern and southern region of Hunan.\n\n\n\n\n6. Conclusion & Key Takeaways\nIn this exercise, we have seen in greater detail that how to compute Global and Local measure of Spatial Autocorrelation (GLSA) and also how to perform cluster analysis to group the attributes and finally the ways of executing hot spot and cold spot area analysis for easier understanding. In the upcoming section, let us view how to perform geographical segmentation with spatially constrained clustering techniques."
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex3.html",
    "href": "Hands-on_Ex/Hands-on_Ex3.html",
    "title": "Geographical segmentation using clustering",
    "section": "",
    "text": "2. Introduction\nThe objective of this study is to understand how to delineate homogeneous region by using geographically referenced multivariate data. There are two major analysis, namely:\n\nhierarchical cluster analysis; and\nspatially constrained cluster analysis.\n\nWhat is geographical segmentation?\nGeographic segmentation divides a target market by location so marketers can better serve customers in a particular area. This type of market segmentation is based on the geographic units themselves (countries, states, cities, etc.), but also on various geographic factors, such as climate, cultural preferences, populations, and more.\n\n\n\nFig 1: Vietnam - Geographic Segmentation\n\n\n\n\n3. Glimpse of Steps\nIn this study, we are going to delineate Shan State, Myanmar into homogeneous regions by using multiple Information and Communication technology (ICT) measures, namely: Radio, Television, Land line phone, Mobile phone, Computer, and Internet at home.\nSome of the important steps performed in this study are as follows\n\nperforming custer analysis by using hclust() of Base R;\nperforming spatially constrained cluster analysis using skater() of Base\nvisualising the analysis output by using ggplot2 and tmap package.\n\n\n\n4. Data\nFollowing two data sets are used:\n\nMyanmar Township Boundary Data (i.e. myanmar_township_boundaries) : This is a GIS data in ESRI shapefile format. It consists of township boundary information of Myanmar. The spatial data are captured in polygon features.\nShan-ICT.csv: This is an extract of The 2014 Myanmar Population and Housing Census Myanmar at the township level.\n\n\n\n5.Deep Dive into Map Analysis\n\n5.1 Installing libraries and Importing files\np_load function pf pacman package is used to install and load sf all necessary packages into R environment.\n\nsf, rgdal and spdep - Spatial data handling\ntidyverse, especially readr, ggplot2 and dplyr - Attribute data handling\ntmap -Choropleth mapping\ncoorplot, ggpubr, and heatmaply - Multivariate data visualisation and analysis\ncluster, ClustGeo - Cluster analysis\n\nThe code chunk below installs and launches these R packages into R environment.\n\n\nLoading packages\npacman::p_load(rgdal, spdep, tmap, sf, \n               ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse, purrr)\n\n\nThe Myanmar Township Boundary GIS data is in ESRI shapefile format. It is imported into R environment by using the st_read() function of sf. The imported InfoComm variables are extracted from The 2014 Myanmar Population and Housing Census Myanmar. The attribute data set is called ict. It is saved in R’s tibble data.frame format.\n\n\nImporting data\nshan_sf <- st_read(dsn = \"data/geospatial\", \n                   layer = \"myanmar_township_boundaries\") %>%\n  filter(ST %in% c(\"Shan (East)\", \"Shan (North)\", \"Shan (South)\"))\n\n\nReading layer `myanmar_township_boundaries' from data source \n  `D:\\raveenaclr\\Geospatial Analytics\\Hands-on_Ex\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 330 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.17275 ymin: 9.671252 xmax: 101.1699 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\nImporting data\nict <- read_csv (\"data/aspatial/Shan-ICT.csv\")\n\n\n\n\n5.2 Data Wrangling\nIt is wiser to take proportion of no. of households present instead of using the numbers directly. So, let us preprocess the data accordingly by using the code chunk below:\n\n\nPreprocessing data\nict_derived <- ict %>%\n  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %>%\n  mutate(`TV_PR` = `Television`/`Total households`*1000) %>%\n  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %>%\n  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %>%\n  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %>%\n  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %>%\n  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,\n         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,\n         `TT_HOUSEHOLDS`=`Total households`,\n         `RADIO`=`Radio`, `TV`=`Television`, \n         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,\n         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) \n\n\nLet’s verify if the new columns are added bu using colnames()\n\ncolnames(ict_derived)\n\n [1] \"DT_PCODE\"      \"DT\"            \"TS_PCODE\"      \"TS\"           \n [5] \"TT_HOUSEHOLDS\" \"RADIO\"         \"TV\"            \"LLPHONE\"      \n [9] \"MPHONE\"        \"COMPUTER\"      \"INTERNET\"      \"RADIO_PR\"     \n[13] \"TV_PR\"         \"LLPHONE_PR\"    \"MPHONE_PR\"     \"COMPUTER_PR\"  \n[17] \"INTERNET_PR\"  \n\n\nWe can see that six new fields namely RADIO_PR, TV_PR, LLPHONE_PR, MPHONE_PR, COMPUTER_PR, and INTERNET_PR have been added into the data.frame.\n\n\n5.3 Exploratory Data Analysis\nHow many are using radio?\nLet us visualize using histogram\n\n\nHistogram - radio users\nggplot(data=ict_derived, \n       aes(x=`RADIO`,\n           y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")+\n  geom_density(color=\"red\",\n               alpha=0.2)\n\n\n\n\n\nLet us detect now if there are any outliers. Also the plot shows the distribution is not normal. And so let us plot the derived one.\n\n\nOutliers - Box plot\nggplot(data=ict_derived, \n       aes(x=`RADIO`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"light blue\")\n\n\n\n\n\nLet us plot the distribution of the newly derived variables (i.e. Radio penetration rate) by using the code chunk below.\n\n\nRadio users-proportion plot\nggplot(data=ict_derived, \n       aes(x=`RADIO_PR`,\n           y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")+\n  geom_density(color=\"red\",\n               alpha = 0.2)\n\n\n\n\n\n\n\nOutliers in derived proportion\nggplot(data=ict_derived, \n       aes(x=`RADIO_PR`)) +\n  geom_boxplot(color=\"black\", \n               fill=\"light blue\")\n\n\n\n\n\nObservation:\nBoth the above plot shows that the data follows normal distribution and there is only one outlier.\nSimilarly, now let’s plot for all other communication medium. First let us plot all the histograms individually and then merge them using tmap_arrange() function.\n\n\nHistogram for all ICT medium\nradio <- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.2)\n\ntv <- ggplot(data=ict_derived, \n             aes(x= `TV_PR`,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.2)\n\nllphone <- ggplot(data=ict_derived, \n             aes(x= `LLPHONE_PR`,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n    geom_density(color=\"black\",\n               alpha = 0.2)\n\nmphone <- ggplot(data=ict_derived, \n             aes(x= `MPHONE_PR`,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.2)\n\ncomputer <- ggplot(data=ict_derived, \n             aes(x= `COMPUTER_PR`,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.2)\n\ninternet <- ggplot(data=ict_derived, \n             aes(x= `INTERNET_PR`,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.)\n\nggarrange(radio, tv, llphone, mphone, computer, internet, \n          ncol = 3, \n          nrow = 2)\n\n\n\n\n\n\n\n5.4 Exploratory Spatial Data Analysis\nBefore we visualise the maps using choropleth, let us first combine both geospatial and aspatial datsets into one simple feature dataframe using the code chunk below.\n\n\nCombining geospatial and aspatial data\nshan_sf <- left_join(shan_sf, \n                     ict_derived, \n                     by=c(\"TS_PCODE\"=\"TS_PCODE\"))\n\n\nWhat is the distribution of Radio penetration rate of Shan State?\n\nqtm(shan_sf, \"RADIO_PR\")\n\n\n\n\nIn order to reveal the distribution shown in the choropleth map above are bias to the underlying total number of households at the townships, we will create two choropleth maps, one for the total number of households (i.e. TT_HOUSEHOLDS.map) and one for the total number of household with Radio (RADIO.map) by using the code chunk below.\n\n\nchoropleth maps\nTT_HOUSEHOLDS.map <- tm_shape(shan_sf) + \n  tm_fill(col = \"TT_HOUSEHOLDS\",\n          n = 5,\n          palette = \"BuPu\",\n          style = \"jenks\", \n          title = \"Total households\") + \n  tm_legend(legend.position = c(\"right\", \"bottom\"))+\n  tm_layout(main.title = \"Households\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 1, \n            legend.width = 1,\n            legend.text.size = 1,\n            legend.title.size = 1,\n            main.title.fontface = \"bold\")+\n  tm_compass(type=\"8star\",\n             position=c(\"right\", \"top\"))+\n  tm_borders(alpha = 0.5) \n\nRADIO.map <- tm_shape(shan_sf) + \n  tm_fill(col = \"RADIO\",\n          n = 5,\n          palette = \"BuPu\",\n          style = \"jenks\",\n          title = \"Number Radio \") + \n  tm_legend(legend.position = c(\"right\", \"bottom\"))+\n  tm_layout(main.title = \"Households with radio\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 1, \n            legend.width = 1,\n            legend.text.size = 1,\n            legend.title.size = 1,\n            main.title.fontface = \"bold\")+\n  tm_compass(type=\"8star\",\n             position=c(\"right\", \"top\"))+\n  tm_borders(alpha = 0.5) \n\ntmap_arrange(TT_HOUSEHOLDS.map, RADIO.map,\n             asp=NA, ncol=2)\n\n\n\n\n\nThe maps above clearly show that townships with relatively larger number of households are also showing relatively higher number of radio ownership.\nNow let us plot the choropleth maps showing the dsitribution of total number of households and Radio penetration rate by using the code chunk below.\n\n\nPlotting Map\ntm_shape(shan_sf) +\n    tm_polygons(c(\"TT_HOUSEHOLDS\", \"RADIO_PR\"),\n                style=\"jenks\") +\n    tm_fill(palette = \"BuPu\")+\n    tm_facets(sync = TRUE, ncol = 2) +\n  tm_legend(legend.position = c(\"right\", \"bottom\"))+\n  tm_layout(outer.margins=0, asp=0,\n            main.title = \"Total households and Households using Radio proportion\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 1, \n            legend.width = 1,\n            legend.text.size = 1,\n            legend.title.size = 1,\n            main.title.fontface = \"bold\")+\n   tm_compass(type=\"8star\",\n             position=c(\"right\", \"top\"))\n\n\n\n\n\nObservation:\nUnlike the previous map, here we can observe the regions where majority of households using radio out of total no. of households. For eg, we can see the central region has got quite a lot no. of households, but the no. of households using radio by proportion are more in northern region. This is the advantage of using proportion values instead of actual numbers.\n\n\n5.5 Correlation Analysis\nBefore we perform cluster analysis, it is important for us to ensure that the cluster variables are not highly correlated.\nIn this section, you will learn how to use corrplot.mixed() function of corrplot package to visualise and analyse the correlation of the input variables.\n\n\nCorrelation\ncluster_vars.cor = cor(ict_derived[,12:17])\ncorrplot.mixed(cluster_vars.cor,\n         lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\nThe correlation plot above shows that COMPUTER_PR and INTERNET_PR are highly correlated. This suggest that only one of them should be used in the cluster analysis instead of both.\n\n\n5.6 Hierarchy Cluster Analysis\nNow let us perform hierarchical cluster analysis. The analysis consists of four major steps:\n\nExtracting clustering variables\nData standardisation\nVisualising the clustering variables\nComputing Proximity Matrix\n\nExtracting clustering variables\n\n\nClustering\ncluster_vars <- shan_sf %>%\n  st_set_geometry(NULL) %>%\n  select(\"TS.x\", \"RADIO_PR\", \"TV_PR\", \"LLPHONE_PR\", \"MPHONE_PR\", \"COMPUTER_PR\")\nhead(cluster_vars,10)\n\n\n        TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\n1    Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\n2    Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\n3    Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\n4   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\n5     Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\n6      Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\n7      Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\n8   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\n9  Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\n10   Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNow let us rename the township name instead of row number and delete the old one using the code chunk below\n\n\nRenaming column\nrow.names(cluster_vars) <- cluster_vars$\"TS.x\"\nshan_ict <- select(cluster_vars, c(2:6))\nhead(shan_ict, 10)\n\n\n          RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit   286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya   417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan   484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung  231.6499 541.7189   28.54454  249.4903    13.76255\nMabein    449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw     280.7624 611.6204   42.06478  408.7951    29.63160\nPekon     318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk  387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme   210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nData Standardisation\nIn order to avoid the cluster analysis result is baised to clustering variables with large values, it is useful to standardise the input variables before performing cluster analysis.\nPerforming Min-Max standardisation\nIn the code chunk below, normalize() of heatmaply package is used to stadardisation the clustering variables by using Min-Max method. The summary() is then used to display the summary statistics of the standardised clustering variables.\n\n\nMin-Max standardisation\nshan_ict.std <- normalize(shan_ict)\nsummary(shan_ict.std)\n\n\n    RADIO_PR          TV_PR          LLPHONE_PR       MPHONE_PR     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.2544   1st Qu.:0.4600   1st Qu.:0.1123   1st Qu.:0.2199  \n Median :0.4097   Median :0.5523   Median :0.1948   Median :0.3846  \n Mean   :0.4199   Mean   :0.5416   Mean   :0.2703   Mean   :0.3972  \n 3rd Qu.:0.5330   3rd Qu.:0.6750   3rd Qu.:0.3746   3rd Qu.:0.5608  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n  COMPUTER_PR     \n Min.   :0.00000  \n 1st Qu.:0.09598  \n Median :0.17607  \n Mean   :0.23692  \n 3rd Qu.:0.29868  \n Max.   :1.00000  \n\n\nPerforming Z-score standardisation\nZ-score standardisation can be performed easily by using scale() of Base R. The code chunk below will be used to stadardisation the clustering variables by using Z-score method.\n\n\nZ standardisation\nshan_ict.z <- scale(shan_ict)\ndescribe(shan_ict.z)\n\n\n            vars  n mean sd median trimmed  mad   min  max range  skew kurtosis\nRADIO_PR       1 55    0  1  -0.04   -0.06 0.94 -1.85 2.55  4.40  0.48    -0.27\nTV_PR          2 55    0  1   0.05    0.04 0.78 -2.47 2.09  4.56 -0.38    -0.23\nLLPHONE_PR     3 55    0  1  -0.33   -0.15 0.68 -1.19 3.20  4.39  1.37     1.49\nMPHONE_PR      4 55    0  1  -0.05   -0.06 1.01 -1.58 2.40  3.98  0.48    -0.34\nCOMPUTER_PR    5 55    0  1  -0.26   -0.18 0.64 -1.03 3.31  4.34  1.80     2.96\n              se\nRADIO_PR    0.13\nTV_PR       0.13\nLLPHONE_PR  0.13\nMPHONE_PR   0.13\nCOMPUTER_PR 0.13\n\n\nWe observe that mean and standard deviation of the Z-score standardised clustering variables are 0 and 1 respectively. We should take note that Z-score standardisation method should only be used if we would assume all variables come from some normal distribution.\nVisualising the standardised clustering variables\nLet us visualise the standardised clustering variables graphically in addition to summary statistics\n\n\nVisualising clustering variables\nr <- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")+\n  ggtitle(\"Before Standardisation\")\n\nshan_ict_s_df <- as.data.frame(shan_ict.std)\ns <- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df <- as.data.frame(shan_ict.z)\nz <- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\nWe can observe that after standardisation, the variables follow normal distribution.\nComputing Proximity Matrix The code chunk below is used to compute the proximity matrix using euclidean method although the function dist() supports six distance proximity calculations, they are: euclidean, maximum, manhattan, canberra, binary and minkowski.\n\n\nProximity matrix\nproxmat <- dist(shan_ict, method = 'euclidean')\n\n\nComputing hierarchical clustering\nThe code chunk below performs hierarchical cluster analysis using ward.D method. The hierarchical clustering output is stored in an object of class hclust which describes the tree produced by the clustering process.\n\n\nHierarchical clustering\nhclust_ward <- hclust(proxmat, method = 'ward.D')\nplot(hclust_ward, cex = 0.6)\n\n\n\n\n\nSelecting the optimal clustering algorithm\nWe can identify stronger clustering structures by using use agnes() function of cluster package. It functions like hclus(), however, with the agnes() function you can also get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).\nThe code chunk below will be used to compute the agglomerative coefficients of all hierarchical clustering algorithms.\n\n\noptimal clustering\nm <- c( \"average\", \"single\", \"complete\", \"ward\")\nnames(m) <- c( \"average\", \"single\", \"complete\", \"ward\")\n\nac <- function(x) {\n  agnes(shan_ict, method = x)$ac\n}\n\nmap_dbl(m, ac)\n\n\n  average    single  complete      ward \n0.8131144 0.6628705 0.8950702 0.9427730 \n\n\nWe can see that Ward’s method provides the strongest clustering structure among the four methods assessed. Hence, in the subsequent analysis, only Ward’s method will be used.\nDetermining Optimal Clusters\nThere are three commonly used methods to determine the optimal no. of clusters, they are:\n\nElbow Method\nAverage Silhouette Method\nGap Statistic Method\n\nThe gap statistic compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.\nTo compute the gap statistic, clusGap() of cluster package will be used.\n\n\noptimal no. of clusters\nset.seed(3456)\ngap_stat <- clusGap(shan_ict, \n                    FUN = hcut, \n                    nstart = 25, \n                    K.max = 10, \n                    B = 50)\n# Print the result\nprint(gap_stat, method = \"firstmax\")\n\n\nClustering Gap statistic [\"clusGap\"] from call:\nclusGap(x = shan_ict, FUNcluster = hcut, K.max = 10, B = 50,     nstart = 25)\nB=50 simulated reference sets, k = 1..10; spaceH0=\"scaledPCA\"\n --> Number of clusters (method 'firstmax'): 1\n          logW   E.logW       gap     SE.sim\n [1,] 8.407129 8.672403 0.2652741 0.03653530\n [2,] 8.130029 8.342622 0.2125931 0.04070127\n [3,] 7.992265 8.193395 0.2011300 0.03385200\n [4,] 7.862224 8.074259 0.2120349 0.03763550\n [5,] 7.756461 7.972480 0.2160197 0.04335443\n [6,] 7.665594 7.886146 0.2205526 0.04599469\n [7,] 7.590919 7.805538 0.2146194 0.04620556\n [8,] 7.526680 7.731965 0.2052856 0.04826211\n [9,] 7.458024 7.664652 0.2066277 0.04894966\n[10,] 7.377412 7.600452 0.2230399 0.04956733\n\n\nNext, we can visualise the plot by using fviz_gap_stat() of factoextra package.\n\nfviz_gap_stat(gap_stat)\n\n\n\n\nSo, we can conclude that the recommended number of cluster to retain is 1. However, it is not logical to retain only one cluster. By examine the gap statistic graph, the 6-cluster gives the largest gap statistic and should be the next best cluster to pick.\nInterpreting the dendrograms\nLet us draw the dendrogram with a border around the selected clusters by using rect.hclust() of R stats. The argument border is used to specify the border colors for the rectangles.\n\n\nDendrograms\nplot(hclust_ward, cex = 0.6)\nrect.hclust(hclust_ward, \n            k = 6, \n            border = 2:5)\n\n\n\n\n\nHierarchical clustering analysis using heatmaply\nIn this section, we will learn how to perform visually-driven hiearchical clustering analysis by using heatmaply package. With heatmaply, we are able to build both highly interactive cluster heatmap or static cluster heatmap.\n\nFirst transform shan_ict data frame into a data matrix\nheatmaply() of heatmaply package is used to build an interactive cluster heatmap\n\n\n\nHierarchical clustering\nshan_ict_mat <- data.matrix(shan_ict)\nheatmaply(normalize(shan_ict_mat),\n          Colv=NA,\n          dist_method = \"euclidean\",\n          hclust_method = \"ward.D\",\n          seriate = \"OLO\",\n          colors = Blues,\n          k_row = 6,\n          margins = c(NA,200,60,NA),\n          fontsize_row = 4,\n          fontsize_col = 5,\n          main=\"Geographic Segmentation of Shan State by ICT indicators\",\n          xlab = \"ICT Indicators\",\n          ylab = \"Townships of Shan State\"\n          )\n\n\n\n\n\n\nMapping the clusters formed\nBased on the above dendragram, let us decide to retain six clusters. cutree() of R Base will be used in the code chunk below to derive a 6-cluster model.\n\ngroups <- as.factor(cutree(hclust_ward, k=6))\n\nThe code chunk below appends the groups object onto shan_sf simple feature object in three steps:\n\nthe groups list object will be converted into a matrix\ncbind() is used to append groups matrix onto shan_sf to produce an output simple feature object called shan_sf_cluster\nrename of dplyr package is used to rename as.matrix.groups field as CLUSTER\n\n\n\nRenaming column\nshan_sf_cluster <- cbind(shan_sf, as.matrix(groups)) %>%\n  rename(`CLUSTER`=`as.matrix.groups.`)\n\n\nNext, qtm() of tmap package is used to plot the choropleth map showing the cluster formed\n\nqtm(shan_sf_cluster, \"CLUSTER\")\n\n\n\n\nThe choropleth map above reveals the clusters are very fragmented. The is one of the major limitation when non-spatial clustering algorithm such as hierarchical cluster analysis method is used.\n\n\n5.8 Spatially Constrained Clustering - SKATER\nLet us see how to derive spatially constrained cluster by using skater() method of spdep package.\nConverting into SpatialPolygonsDataFrame\nLet us convert shan_sf into SpatialPolygonsDataFrame. as SKATER function only support sp objects such as SpatialPolygonDataFrame.\nThe code chunk below uses as_Spatial() of sf package to convert shan_sf into a SpatialPolygonDataFrame called shan_sp.\n\nshan_sp <- as_Spatial(shan_sf)\n\nComputing Neighbour List\npoly2nd() of spdep package will be used to compute the neighbours list from polygon list\n\n\nNeighbour list\nshan.nb <- poly2nb(shan_sp)\nsummary(shan.nb)\n\n\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\n\n\n\nNeighbouring list network\nplot(shan_sp, \n     border=grey(.5))\nplot(shan.nb, \n     coordinates(shan_sp), \n     col=\"red\", \n     add=TRUE)\n\n\n\n\n\nComputing minimum spanning tree\nnbcosts() of spdep package is used to compute the cost of each edge. It is the distance between it nodes. This function compute this distance using a data.frame with observations vector in each node.\nThe code chunk below is used to compute the cost of each edge.\n\nlcosts <- nbcosts(shan.nb, shan_ict)\n\nNext, let us incorporate these costs into a weights object in the same way as we did in the calculation of inverse of distance weights. In other words, we convert the neighbour list to a list weights object by specifying the just computed lcosts as the weights.\n\n\ncosts into weights\nshan.w <- nb2listw(shan.nb, \n                   lcosts, \n                   style=\"B\")\nsummary(shan.w)\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 55 \nNumber of nonzero links: 264 \nPercentage nonzero weights: 8.727273 \nAverage number of links: 4.8 \nLink number distribution:\n\n 2  3  4  5  6  7  8  9 \n 5  9  7 21  4  3  5  1 \n5 least connected regions:\n3 5 7 9 47 with 2 links\n1 most connected region:\n8 with 9 links\n\nWeights style: B \nWeights constants summary:\n   n   nn       S0       S1        S2\nB 55 3025 76267.65 58260785 522016004\n\n\nComputing minimum spanning tree\nThe minimum spanning tree is computed by mean of the mstree() of spdep package as shown in the code chunk below\n\nshan.mst <- mstree(shan.w)\nclass(shan.mst)\n\n[1] \"mst\"    \"matrix\"\n\n\nThe plot method for the MST include a way to show the observation numbers of the nodes in addition to the edge. As before, we plot this together with the township boundaries. We can see how the initial neighbour list is simplified to just one edge connecting each of the nodes, while passing through all the nodes.\n\n\nPlotting Minimum spanning tree\nplot(shan_sp, border=gray(.5))\nplot.mst(shan.mst, \n         coordinates(shan_sp), \n         col=\"blue\", \n         cex.lab=0.7, \n         cex.circles=0.005, \n         add=TRUE)\n\n\n\n\n\nComputing spatially constrained clusters using SKATER method\nThe code chunk below compute the spatially constrained cluster using skater() of spdep package\n\n\nSpatially constrained cluster\nclust6 <- skater(edges = shan.mst[,1:2], \n                 data = shan_ict, \n                 method = \"euclidean\", \n                 ncuts = 5)\n\n\nLastly, let us plot the pruned tree that shows the five clusters on top of the townshop area.\n\n\nPruned tree\nplot(shan_sp, border=gray(.5))\nplot(clust6, \n     coordinates(shan_sp), \n     cex.lab=.7,\n     groups.colors=c(\"red\",\"green\",\"blue\", \"brown\", \"pink\"),\n     cex.circles=0.005, \n     add=TRUE)\n\n\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1],\ncoords[id2, : \"add\" is not a graphical parameter\n\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1],\ncoords[id2, : \"add\" is not a graphical parameter\n\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1],\ncoords[id2, : \"add\" is not a graphical parameter\n\nWarning in segments(coords[id1, 1], coords[id1, 2], coords[id2, 1],\ncoords[id2, : \"add\" is not a graphical parameter\n\n\n\n\n\nVisualising the clusters in choropleth map\nThe code chunk below is used to plot the newly derived clusters by using SKATER method\n\n\nChoropleth clustering\ngroups_mat <- as.matrix(clust6$groups)\nshan_sf_spatialcluster <- cbind(shan_sf_cluster, as.factor(groups_mat)) %>%\n  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)\nqtm(shan_sf_spatialcluster, \"SP_CLUSTER\")\n\n\n\n\n\nFor effective interpretation, let us plot both the hierarchical clustering and spatially constrained hierarchical clustering maps next to each other.\n\n\nClustering comparison\nhclust.map <- qtm(shan_sf_cluster,\n                  \"CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\nshclust.map <- qtm(shan_sf_spatialcluster,\n                   \"SP_CLUSTER\") + \n  tm_borders(alpha = 0.5) \n\ntmap_arrange(hclust.map, shclust.map,\n             asp=NA, ncol=2)\n\n\n\n\n\nThe map on the right shows the clusters of the region vividly which helps us to delineate homogeneous region easily.\n\n\n\n6. Conclusion\nIn this study, we have achieved delineating homogeneous region of Shan State, Myanmar by using multiple Information and Communication technology (ICT) measures, namely: Radio, Television, Land line phone, Mobile phone, Computer, and Internet at home. In the upcoming section, In the upcoming section let us see how to calibrate Hedonic Pricing Model for Private Highrise Property with GWR Method. Stay tuned………………"
  },
  {
    "objectID": "Hands-on_Ex/Hands-on_Ex4.html",
    "href": "Hands-on_Ex/Hands-on_Ex4.html",
    "title": "Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "",
    "text": "2. Introduction\nThe objective of this study is to Calibrate Hedonic Pricing Model for Private Highrise Property in Singapore using Geographically Weighted Regression (GWR) Method\nWhat is Geographically Weighted Regression?\nIt is a spatial statistical technique that takes non-stationary variables into consideration (e.g., climate; demographic factors; physical environment characteristics) and models the local relationships between these independent variables and an outcome of interest (also known as dependent variable).\n\n\n\nGeographically Weighted Regression\n\n\n\n\n3. Glimpse of Steps\nSome of the important steps performed in this study are as follows\n\nimporting geospatial data using appropriate function(s) of sf package,\nimporting csv file using appropriate function of readr package,\nconverting aspatial dataframe into sf object\nperforming exploratory data analysis\nperforming simple and multiple linear regression techniques\nbuilding a hedonic price model using GWR method\n\n\n\n4. Data\nFollowing two data sets are used:\n\nURA Master Plan subzone boundary in shapefile format (i.e. MP14_SUBZONE_WEB_PL)\ncondo_resale_2015 in csv format (i.e. condo_resale_2015.csv)\n\n\n\n5.Deep Dive into Map Analysis\n\n5.1 Installing libraries and Importing files\np_load function pf pacman package is used to install and load sf all necessary packages into R environment.\n\nsf, rgdal and spdep - Spatial data handling\ntidyverse, especially readr, ggplot2 and dplyr - Attribute data handling\ntmap -Choropleth mapping\nolsrr - Ordinary Least Square(OLS) method and performing diagnostics tests\nGWmodel - geographical weighted family of models\ncorrplot - multivariate data visualisation and analysis\n\nThe code chunk below installs and launches these R packages into R environment.\n\n\nLoading packages\npacman::p_load(olsrr, corrplot, ggpubr, \n               sf,spdep, GWmodel, tmap, \n               tidyverse, gtsummary,patchwork, ggthemes)\n\n\nThe code chunk below is used to import MP_SUBZONE_WEB_PL shapefile by using st_read() of sf packages.\n\n\nImporting data\nmpsz = st_read(dsn = \"data/geospatial\", \n               layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `D:\\raveenaclr\\Geospatial Analytics\\Hands-on_Ex\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nImporting data\ncondo_resale = read_csv(\"data/aspatial/Condo_resale_2015.csv\")\n\n\n\n\n5.2 Data Wrangling\nThe code chunk below updates the newly imported mpsz with the correct ESPG code (i.e. 3414)\n\n\nAssigning correct projection\nmpsz_svy21 <- st_transform(mpsz, 3414)\n\n\nCurrently, the condo_resale tibble data frame is aspatial. We will convert it to a sf object. The code chunk below converts condo_resale data frame into a simple feature data frame by using st_as_sf() of sf packages.\n\n\nconverting projection\ncondo_resale.sf <- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %>%\n  st_transform(crs=3414)\n\n\n\n\n5.3 Exploratory Data Analysis (EDA)\nWhat is the distribution of Condo selling price?\n\n\nHistogram-Selling Price\nh1 <- ggplot(data=condo_resale.sf, \n              aes(x=`SELLING_PRICE`,\n                  y= ..density..)) +\n      geom_histogram(bins=20, \n                     color=\"black\", \n                     fill=\"coral\")+\n      geom_density(color=\"black\",\n                   alpha=0.5) +\n      theme(panel.background= element_blank())\n\n\nThe figure above reveals a right skewed distribution. This means that more condominium units were transacted at relative lower prices. Tghis can be normalised by using log transformation.\nThe code chunk below is used to derive a new variable called LOG_SELLING_PRICE by using a log transformation on the variable SELLING_PRICE. It is performed using mutate() of dplyr package.\n\n\nHistogram-Selling Price\ncondo_resale.sf <- condo_resale.sf %>%\n                   mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\nh2 <- ggplot(data=condo_resale.sf, \n             aes(x=`LOG_SELLING_PRICE`,\n                 y= ..density..)) +\n      geom_histogram(bins=20, \n                     color=\"black\", \n                     fill=\"coral\")+\n      geom_density(color=\"black\",\n                   alpha=0.5)+\n      theme(panel.background= element_blank())\n\n\nLet us compare the distribution before and after performing log transformation\n\n\nComaprison\nh1 <- h1 + labs(title= \"Raw values\")\nh2 <- h2 + labs(title = \"Log transformation\")\n\nggarrange(h1, h2, ncol=2)\n\n\n\n\n\nNow let us view the distribution for multiple variables\nThe code chunk below is used to multiple histograms. Then, ggarrange() is used to organised these histogram into a 3 columns by 4 rows small multiple plot.\n\n\nMultiple histograms\nAREA_SQM <- ggplot(data=condo_resale.sf, \n                   aes(x= `AREA_SQM`,\n                   y= ..density..)) + \n            geom_histogram(bins=20, \n                           color=\"black\", \n                           fill=\"coral\")+\n            geom_density(color=\"black\",\n                         alpha=0.5)+\n            theme(panel.background= element_blank())\n\nAGE <- ggplot(data=condo_resale.sf, \n              aes(x= `AGE`,\n                  y= ..density..)) +\n       geom_histogram(bins=20, \n                      color=\"black\", \n                      fill=\"coral\")+\n            geom_density(color=\"black\",\n                         alpha=0.5)+\n            theme(panel.background= element_blank())\n\nPROX_CBD <- ggplot(data=condo_resale.sf, \n                   aes(x= `PROX_CBD`,\n                       y= ..density..)) +\n            geom_histogram(bins=20, \n                           color=\"black\", \n                           fill=\"coral\")+\n            geom_density(color=\"black\",\n                         alpha=0.5)+\n            theme(panel.background= element_blank())\n\nPROX_CHILDCARE <- ggplot(data=condo_resale.sf,\n                         aes(x= `PROX_CHILDCARE`,\n                             y= ..density..)) + \n                  geom_histogram(bins=20,\n                                 color=\"black\", \n                                 fill=\"coral\")+\n                  geom_density(color=\"black\",\n                                alpha=0.5)+\n                  theme(panel.background= element_blank())\n\nPROX_ELDERLYCARE <- ggplot(data=condo_resale.sf, \n                           aes(x= `PROX_ELDERLYCARE`,\n                               y= ..density..)) +\n                    geom_histogram(bins=20, \n                                   color=\"black\", \n                                   fill=\"coral\")+\n                    geom_density(color=\"black\",\n                                 alpha=0.5)+\n                    theme(panel.background= element_blank())\n\nPROX_URA_GROWTH_AREA <- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_URA_GROWTH_AREA`,\n                                   y= ..density..)) +\n                        geom_histogram(bins=20, \n                                       color=\"black\", \n                                       fill=\"coral\")+\n                        geom_density(color=\"black\",\n                                     alpha=0.5)+\n                        theme(panel.background= element_blank())\n\nPROX_HAWKER_MARKET <- ggplot(data=condo_resale.sf, \n                             aes(x= `PROX_HAWKER_MARKET`,\n                                 y= ..density..)) +\n                      geom_histogram(bins=20, \n                                     color=\"black\", \n                                     fill=\"coral\")+\n                      geom_density(color=\"black\",\n                                   alpha=0.5)+\n                      theme(panel.background= element_blank())\n\nPROX_KINDERGARTEN <- ggplot(data=condo_resale.sf, \n                            aes(x= `PROX_KINDERGARTEN`,\n                                y= ..density..)) +\n                     geom_histogram(bins=20, \n                                    color=\"black\", \n                                    fill=\"coral\")+\n                     geom_density(color=\"black\",\n                                  alpha=0.5)+\n            theme(panel.background= element_blank())\n\nPROX_MRT <- ggplot(data=condo_resale.sf, \n                   aes(x= `PROX_MRT`,\n                       y= ..density..)) +\n            geom_histogram(bins=20, \n                           color=\"black\", \n                           fill=\"coral\")+\n            geom_density(color=\"black\",\n                         alpha=0.5)+\n            theme(panel.background= element_blank())\n\nPROX_PARK <- ggplot(data=condo_resale.sf, \n                    aes(x= `PROX_PARK`,\n                        y= ..density..)) +\n             geom_histogram(bins=20, \n                            color=\"black\", \n                            fill=\"coral\")+\n             geom_density(color=\"black\",\n                          alpha=0.5)+\n             theme(panel.background= element_blank())\n\nPROX_PRIMARY_SCH <- ggplot(data=condo_resale.sf, \n                           aes(x= `PROX_PRIMARY_SCH`,\n                               y= ..density..)) +\n                    geom_histogram(bins=20, \n                                   color=\"black\", \n                                   fill=\"coral\")+\n                    geom_density(color=\"black\",\n                                 alpha=0.5)+\n                    theme(panel.background= element_blank())\n\nPROX_TOP_PRIMARY_SCH <- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_TOP_PRIMARY_SCH`,\n                                   y= ..density..)) +\n                        geom_histogram(bins=20, \n                                       color=\"black\", \n                                       fill=\"coral\")+\n                        geom_density(color=\"black\",\n                                     alpha=0.5)+\n                        theme(panel.background= element_blank())\n\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, \n          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)\n\n\n\n\n\nWhat is the geospatial distribution of condo prices in Singapore?\nThe code chunks below is used to create an interactive point symbol map.\n\n\nGeospatial distribution\ntm_shape(mpsz_svy21)+\n  tm_polygons() +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"SELLING_PRICE\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tmap_options(check.and.fix = TRUE)+\n  tmap_mode(\"view\")+\n  tm_view(set.zoom.limits = c(11,14))\n\n\ntmap mode set to interactive viewing\n\n\nWarning: The shape mpsz_svy21 is invalid (after reprojection). See\nsf::st_is_valid\n\n\n\n\n\n\n\n\n\n5.4 Hedonic Pricing Modelling in R\n\n\n5.4.1 Simple Linear Regression Method\nFirst, let us build a simple linear regression model by using SELLING_PRICE as the dependent variable and AREA_SQM as the independent variable.\n\n\nSimple Linear Regression\ncondo.slr <- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\nsummary(condo.slr)\n\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3695815  -391764   -87517   258900 13503875 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -258121.1    63517.2  -4.064 5.09e-05 ***\nAREA_SQM      14719.0      428.1  34.381  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 942700 on 1434 degrees of freedom\nMultiple R-squared:  0.4518,    Adjusted R-squared:  0.4515 \nF-statistic:  1182 on 1 and 1434 DF,  p-value: < 2.2e-16\n\n\nInterpretation\nR-squared of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.\nH0 (Null Hypothesis) - mean price is a good estimator of SELLING_PRICE\nH1 (Alternative Hypothesis) - mean price is not a good estimator of SELLING_PRICE\n\nSince p-value is much smaller than 0.0001, we will reject the null hypothesis that mean is a good estimator of SELLING_PRICE.\nThis will allow us to infer that simple linear regression model above is a good estimator of SELLING_PRICE.\np-values of both the estimates of the Intercept and ARA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected and so B0 and B1 are good parameter estimates.\n\nLet us visualise the best fit curve on a scatterplot, using lm() as a method function in ggplot’s geometry as shown in the code chunk below.\n\n\nGoodness of fit\nggplot(data=condo_resale.sf,  \n       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +\n  geom_point() +\n  geom_smooth(method = lm)+\n  theme(panel.background= element_blank())\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nWe can see that there are a few statistical outliers with relatively high selling prices.\n\n\n5.4.2 Multiple Linear Regression Method\nLet us check if there is a multicollinearity phenomenon by executing correlation analysis. It is important to ensure that the indepdent variables used are not highly correlated to each other.\nThe code chunk below is used to plot a scatterplot matrix of the relationship between the independent variables in condo_resale data.frame.\n\n\nCorrelation Analysis\ncorrplot(cor(condo_resale[, 5:23]), \n         diag = FALSE, \n         order = \"AOE\",\n         tl.pos = \"td\", \n         tl.cex = 0.5,\n         method = \"number\", \n         type = \"upper\")\n\n\n\n\n\nIt is clear that Freehold is highly correlated to LEASE_99YEAR. Hence, let us include either one of them i.e. LEASE_99YEAR in the subsequent model building.\n\n\n5.4.3 Hedonic pricing model using multiple linear regression method\nThe code chunk below using lm() to calibrate the multiple linear regression model.\n\n\nMultiple Linear Regression\ncondo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                data=condo_resale.sf)\nsummary(condo.mlr)\n\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + \n    PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + \n    PROX_KINDERGARTEN + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + \n    PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3475964  -293923   -23069   241043 12260381 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           481728.40  121441.01   3.967 7.65e-05 ***\nAREA_SQM               12708.32     369.59  34.385  < 2e-16 ***\nAGE                   -24440.82    2763.16  -8.845  < 2e-16 ***\nPROX_CBD              -78669.78    6768.97 -11.622  < 2e-16 ***\nPROX_CHILDCARE       -351617.91  109467.25  -3.212  0.00135 ** \nPROX_ELDERLYCARE      171029.42   42110.51   4.061 5.14e-05 ***\nPROX_URA_GROWTH_AREA   38474.53   12523.57   3.072  0.00217 ** \nPROX_HAWKER_MARKET     23746.10   29299.76   0.810  0.41782    \nPROX_KINDERGARTEN     147468.99   82668.87   1.784  0.07466 .  \nPROX_MRT             -314599.68   57947.44  -5.429 6.66e-08 ***\nPROX_PARK             563280.50   66551.68   8.464  < 2e-16 ***\nPROX_PRIMARY_SCH      180186.08   65237.95   2.762  0.00582 ** \nPROX_TOP_PRIMARY_SCH    2280.04   20410.43   0.112  0.91107    \nPROX_SHOPPING_MALL   -206604.06   42840.60  -4.823 1.57e-06 ***\nPROX_SUPERMARKET      -44991.80   77082.64  -0.584  0.55953    \nPROX_BUS_STOP         683121.35  138353.28   4.938 8.85e-07 ***\nNO_Of_UNITS             -231.18      89.03  -2.597  0.00951 ** \nFAMILY_FRIENDLY       140340.77   47020.55   2.985  0.00289 ** \nFREEHOLD              359913.01   49220.22   7.312 4.38e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 755800 on 1417 degrees of freedom\nMultiple R-squared:  0.6518,    Adjusted R-squared:  0.6474 \nF-statistic: 147.4 on 18 and 1417 DF,  p-value: < 2.2e-16\n\n\nAt 99% confidence interval, almost all the varibles are statistically significant except PROX_HAWKER_MARKET, PROX_KINDERGARTEN , PROX_TOP_PRIMARY_SCH.\n\n\n5.4.4 Publication Quality Table: olsrr method\nIt is clear that not all the independent variables are statistically significant. Let us revise the model by removing those variables which are not statistically significant.\n\n\nRevised model\ncondo.mlr1 <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + \n                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + \n                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                 data=condo_resale.sf)\nols_regress(condo.mlr1)\n\n\n                             Model Summary                               \n------------------------------------------------------------------------\nR                       0.807       RMSE                     755957.289 \nR-Squared               0.651       Coef. Var                    43.168 \nAdj. R-Squared          0.647       MSE                571471422208.591 \nPred R-Squared          0.638       MAE                      414819.628 \n------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                                     ANOVA                                       \n--------------------------------------------------------------------------------\n                    Sum of                                                      \n                   Squares          DF         Mean Square       F         Sig. \n--------------------------------------------------------------------------------\nRegression    1.512586e+15          14        1.080418e+14    189.059    0.0000 \nResidual      8.120609e+14        1421    571471422208.591                      \nTotal         2.324647e+15        1435                                          \n--------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n               model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n-----------------------------------------------------------------------------------------------------------------\n         (Intercept)     527633.222    108183.223                   4.877    0.000     315417.244     739849.200 \n            AREA_SQM      12777.523       367.479        0.584     34.771    0.000      12056.663      13498.382 \n                 AGE     -24687.739      2754.845       -0.167     -8.962    0.000     -30091.739     -19283.740 \n            PROX_CBD     -77131.323      5763.125       -0.263    -13.384    0.000     -88436.469     -65826.176 \n      PROX_CHILDCARE    -318472.751    107959.512       -0.084     -2.950    0.003    -530249.889    -106695.613 \n    PROX_ELDERLYCARE     185575.623     39901.864        0.090      4.651    0.000     107302.737     263848.510 \nPROX_URA_GROWTH_AREA      39163.254     11754.829        0.060      3.332    0.001      16104.571      62221.936 \n            PROX_MRT    -294745.107     56916.367       -0.112     -5.179    0.000    -406394.234    -183095.980 \n           PROX_PARK     570504.807     65507.029        0.150      8.709    0.000     442003.938     699005.677 \n    PROX_PRIMARY_SCH     159856.136     60234.599        0.062      2.654    0.008      41697.849     278014.424 \n  PROX_SHOPPING_MALL    -220947.251     36561.832       -0.115     -6.043    0.000    -292668.213    -149226.288 \n       PROX_BUS_STOP     682482.221    134513.243        0.134      5.074    0.000     418616.359     946348.082 \n         NO_Of_UNITS       -245.480        87.947       -0.053     -2.791    0.005       -418.000        -72.961 \n     FAMILY_FRIENDLY     146307.576     46893.021        0.057      3.120    0.002      54320.593     238294.560 \n            FREEHOLD     350599.812     48506.485        0.136      7.228    0.000     255447.802     445751.821 \n-----------------------------------------------------------------------------------------------------------------\n\n\nNow, we have only statistically significant variables.\n\n\n5.4.5 Publication Quality Table: gtsummary method\nIn the code chunk below, tbl_regression() is used to create a well formatted regression report using gtsummary package that provides an elegant and flexible way to create publication-ready summary tables in R.\n\n\ngtsummary\ntbl_regression(condo.mlr1, intercept = TRUE)\n\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n527,633\n315,417, 739,849\n<0.001\n    AREA_SQM\n12,778\n12,057, 13,498\n<0.001\n    AGE\n-24,688\n-30,092, -19,284\n<0.001\n    PROX_CBD\n-77,131\n-88,436, -65,826\n<0.001\n    PROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n    PROX_ELDERLYCARE\n185,576\n107,303, 263,849\n<0.001\n    PROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n<0.001\n    PROX_MRT\n-294,745\n-406,394, -183,096\n<0.001\n    PROX_PARK\n570,505\n442,004, 699,006\n<0.001\n    PROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n    PROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n<0.001\n    PROX_BUS_STOP\n682,482\n418,616, 946,348\n<0.001\n    NO_Of_UNITS\n-245\n-418, -73\n0.005\n    FAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n    FREEHOLD\n350,600\n255,448, 445,752\n<0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nWith this, model statistics can also be included in the report by either appending them to the report table by using add_glance_table() or adding as a table source note by using add_glance_source_note() as shown in the code chunk below.\n\n\nModel statistics\ntbl_regression(condo.mlr1, \n               intercept = TRUE) %>% \n  add_glance_source_note(\n    label = list(sigma ~ \"\\U03C3\"),\n    include = c(r.squared, adj.r.squared, \n                AIC, statistic,\n                p.value, sigma))\n\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n527,633\n315,417, 739,849\n<0.001\n    AREA_SQM\n12,778\n12,057, 13,498\n<0.001\n    AGE\n-24,688\n-30,092, -19,284\n<0.001\n    PROX_CBD\n-77,131\n-88,436, -65,826\n<0.001\n    PROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n    PROX_ELDERLYCARE\n185,576\n107,303, 263,849\n<0.001\n    PROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n<0.001\n    PROX_MRT\n-294,745\n-406,394, -183,096\n<0.001\n    PROX_PARK\n570,505\n442,004, 699,006\n<0.001\n    PROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n    PROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n<0.001\n    PROX_BUS_STOP\n682,482\n418,616, 946,348\n<0.001\n    NO_Of_UNITS\n-245\n-418, -73\n0.005\n    FAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n    FREEHOLD\n350,600\n255,448, 445,752\n<0.001\n  \n  \n    \n      R² = 0.651; Adjusted R² = 0.647; AIC = 42,967; Statistic = 189; p-value = <0.001; σ = 755,957\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\n\n5.4.6 Checking for Multicollinearity\nLet us check if there is any sign of multicollinearity using ols_vif_tol() of olsrr package\n\n\nMulticollinearity check\nols_vif_tol(condo.mlr1)\n\n\n              Variables Tolerance      VIF\n1              AREA_SQM 0.8728554 1.145665\n2                   AGE 0.7071275 1.414172\n3              PROX_CBD 0.6356147 1.573280\n4        PROX_CHILDCARE 0.3066019 3.261559\n5      PROX_ELDERLYCARE 0.6598479 1.515501\n6  PROX_URA_GROWTH_AREA 0.7510311 1.331503\n7              PROX_MRT 0.5236090 1.909822\n8             PROX_PARK 0.8279261 1.207837\n9      PROX_PRIMARY_SCH 0.4524628 2.210126\n10   PROX_SHOPPING_MALL 0.6738795 1.483945\n11        PROX_BUS_STOP 0.3514118 2.845664\n12          NO_Of_UNITS 0.6901036 1.449058\n13      FAMILY_FRIENDLY 0.7244157 1.380423\n14             FREEHOLD 0.6931163 1.442759\n\n\nWe can conclude that there are no sign of multicollinearity among the independent variables as VIF of the independent variables are less than 10.\n\n\n5.4.7 Test for Non-Linearity\nIn the code chunk below, the ols_plot_resid_fit() of olsrr package is used to perform linearity assumption test.\n\n\nNon-Linearity test\nols_plot_resid_fit(condo.mlr1)\n\n\n\n\n\nWe can conclude that the relationships between the dependent variable and independent variables are linear as most of the data poitns are scattered around the 0 line.\n\n\n5.4.8 Test for Normality Assumption\nThe code chunk below uses ols_plot_resid_hist() of olsrr package to perform normality assumption test.\n\n\nNormality Assumption Test\nols_plot_resid_hist(condo.mlr1)\n\n\n\n\n\nIt is shown that the residual of the multiple linear regression model (i.e. condo.mlr1) resembles normal distribution.\n\n\n5.4.9 Testing for Spatial Autocorrelation\nIn order to perform spatial autocorrelation test, let us perform the following steps\n\nConvert condo_resale.sf from sf data frame into a SpatialPointsDataFrame.\nConvert condo_resale.res.sf from simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.\nDisplay the distribution of the residuals on an interactive map\n\n\n\nSpatial autocorrelation test\nmlr.output <- as.data.frame(condo.mlr1$residuals)\ncondo_resale.res.sf <- cbind(condo_resale.sf, \n                        condo.mlr1$residuals) %>%\nrename(`MLR_RES` = `condo.mlr1.residuals`)\ncondo_resale.sp <- as_Spatial(condo_resale.res.sf)\n\n\nLet us visualise the spatial distribution using below code chunk\n\n\nMapping the data\ntm_shape(mpsz_svy21)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.res.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tmap_mode(\"view\")+\n  tm_view(set.zoom.limits = c(11,14))\n\n\ntmap mode set to interactive viewing\n\n\nWarning: The shape mpsz_svy21 is invalid (after reprojection). See\nsf::st_is_valid\n\n\nVariable(s) \"MLR_RES\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\nThe figure above reveal that there is sign of spatial autocorrelation.\nLet us double check by performing Moran’s I test\nFollowing steps will be performed\n\nCompute the distance-based weight matrix by using dnearneigh() function of spdep.\nConvert the output neighbours lists (i.e. nb) into a spatial weights using nb2listw() of spdep packge.\nConduct Moran’s I test for residual spatial autocorrelation by using lm.morantest() of spdep package.\n\n\n\nMoran’s I test\nnb <- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)\nnb_lw <- nb2listw(nb, style = 'W')\nlm.morantest(condo.mlr1, nb_lw)\n\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD +\nPROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT +\nPROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\nNO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data = condo_resale.sf)\nweights: nb_lw\n\nMoran I statistic standard deviate = 24.366, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nObserved Moran I      Expectation         Variance \n    1.438876e-01    -5.487594e-03     3.758259e-05 \n\n\nSince the Observed Global Moran I = 0.1424418 which is greater than 0, we can infer than the residuals resemble cluster distribution.\n\n\n5.5 Hedonic Pricing Models using GWmodel\nLet us model hedonic pricing using both the fixed and adaptive bandwidth schemes\n\n\n5.5.1 Computing fixed bandwidth\nIn the code chunk below bw.gwr() of GWModel package is used to determine the optimal fixed bandwidth to use in the model.\n\n\nFixed bandwidth\nbw.fixed <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=FALSE, \n                   longlat=FALSE)\n\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.378294e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3408 CV score: 4.721292e+14 \nFixed bandwidth: 971.3403 CV score: 4.721292e+14 \nFixed bandwidth: 971.3406 CV score: 4.721292e+14 \nFixed bandwidth: 971.3404 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \n\n\nThe result shows that the recommended bandwidth is 971.3398 metres\n\n\n5.5.2 GWRModel method - Fixed bandwidth\nLet us calibrate the gwr model using fixed bandwidth and gaussian kernel.\n\n\nGWR- Fixed bandwidth\ngwr.fixed <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                         PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                         PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                         FAMILY_FRIENDLY + FREEHOLD, \n                       data=condo_resale.sp, \n                       bw=bw.fixed, \n                       kernel = 'gaussian', \n                       longlat = FALSE)\n\n\n\ngwr.fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2022-12-11 16:54:43 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.fixed, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  < 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  < 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  < 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  < 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: < 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.3405 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3600e+04  3.4646e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7425e+04  2.9007e+05\n   PROX_ELDERLYCARE     -3.5000e+06 -1.5970e+05  3.1971e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5207e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8073e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112793548\n   AREA_SQM                 21575\n   AGE                     434201\n   PROX_CBD               2704596\n   PROX_CHILDCARE         1654087\n   PROX_ELDERLYCARE      38867814\n   PROX_URA_GROWTH_AREA  78515730\n   PROX_MRT               3124316\n   PROX_PARK             18122425\n   PROX_PRIMARY_SCH       4637503\n   PROX_SHOPPING_MALL     1529952\n   PROX_BUS_STOP         11342182\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720744\n   FREEHOLD               6073636\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3804 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6196 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.53407e+14 \n   R-square value:  0.8909912 \n   Adjusted R-square value:  0.8430417 \n\n   ***********************************************************************\n   Program stops at: 2022-12-11 16:54:44 \n\n\nThe report shows that the adjusted r-square of the gwr is 0.8430 which is significantly better than the global multiple linear regression model of 0.6472.\n\n\n5.5.3 GWRModel method - Adaptive bandwidth\nLet us calibrate the gwr-absed hedonic pricing model by using adaptive bandwidth approach.\nComputing the adaptive bandwidth\nIn the code chunk below bw.gwr() of GWModel package is used to determine the optimal fixed bandwidth with adaptive = TRUE to use in the model.\n\n\nAdaptive Bandwidth\nbw.adaptive <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + \n                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + \n                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + \n                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                      data=condo_resale.sp, \n                      approach=\"CV\", \n                      kernel=\"gaussian\", \n                      adaptive=TRUE, \n                      longlat=FALSE)\n\n\nAdaptive bandwidth: 895 CV score: 7.952401e+14 \nAdaptive bandwidth: 561 CV score: 7.667364e+14 \nAdaptive bandwidth: 354 CV score: 6.953454e+14 \nAdaptive bandwidth: 226 CV score: 6.15223e+14 \nAdaptive bandwidth: 147 CV score: 5.674373e+14 \nAdaptive bandwidth: 98 CV score: 5.426745e+14 \nAdaptive bandwidth: 68 CV score: 5.168117e+14 \nAdaptive bandwidth: 49 CV score: 4.859631e+14 \nAdaptive bandwidth: 37 CV score: 4.646518e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \nAdaptive bandwidth: 25 CV score: 4.430816e+14 \nAdaptive bandwidth: 32 CV score: 4.505602e+14 \nAdaptive bandwidth: 27 CV score: 4.462172e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \n\n\nThe result reveals that 30 is the recommended data points to be used.\n\n\nConstructing the adaptive bandwidth gwr model\nLet us calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and gaussian kernel as shown in the code chunk below.\n\n\nGWR - Adaptive Bandwidth\ngwr.adaptive <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + \n                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                          data=condo_resale.sp, bw=bw.adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE, \n                          longlat = FALSE)\ngwr.adaptive\n\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2022-12-11 16:54:55 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  < 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  < 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  < 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  < 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: < 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 30 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -1.3487e+08 -2.4669e+05  7.7928e+05  1.6194e+06\n   AREA_SQM              3.3188e+03  5.6285e+03  7.7825e+03  1.2738e+04\n   AGE                  -9.6746e+04 -2.9288e+04 -1.4043e+04 -5.6119e+03\n   PROX_CBD             -2.5330e+06 -1.6256e+05 -7.7242e+04  2.6624e+03\n   PROX_CHILDCARE       -1.2790e+06 -2.0175e+05  8.7158e+03  3.7778e+05\n   PROX_ELDERLYCARE     -1.6212e+06 -9.2050e+04  6.1029e+04  2.8184e+05\n   PROX_URA_GROWTH_AREA -7.2686e+06 -3.0350e+04  4.5869e+04  2.4613e+05\n   PROX_MRT             -4.3781e+07 -6.7282e+05 -2.2115e+05 -7.4593e+04\n   PROX_PARK            -2.9020e+06 -1.6782e+05  1.1601e+05  4.6572e+05\n   PROX_PRIMARY_SCH     -8.6418e+05 -1.6627e+05 -7.7853e+03  4.3222e+05\n   PROX_SHOPPING_MALL   -1.8272e+06 -1.3175e+05 -1.4049e+04  1.3799e+05\n   PROX_BUS_STOP        -2.0579e+06 -7.1461e+04  4.1104e+05  1.2071e+06\n   NO_Of_UNITS          -2.1993e+03 -2.3685e+02 -3.4699e+01  1.1657e+02\n   FAMILY_FRIENDLY      -5.9879e+05 -5.0927e+04  2.6173e+04  2.2481e+05\n   FREEHOLD             -1.6340e+05  4.0765e+04  1.9023e+05  3.7960e+05\n                            Max.\n   Intercept            18758355\n   AREA_SQM                23064\n   AGE                     13303\n   PROX_CBD             11346650\n   PROX_CHILDCARE        2892127\n   PROX_ELDERLYCARE      2465671\n   PROX_URA_GROWTH_AREA  7384059\n   PROX_MRT              1186242\n   PROX_PARK             2588497\n   PROX_PRIMARY_SCH      3381462\n   PROX_SHOPPING_MALL   38038564\n   PROX_BUS_STOP        12081592\n   NO_Of_UNITS              1010\n   FAMILY_FRIENDLY       2072414\n   FREEHOLD              1813995\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 350.3088 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1085.691 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 41982.22 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41546.74 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 41914.08 \n   Residual sum of squares: 2.528227e+14 \n   R-square value:  0.8912425 \n   Adjusted R-square value:  0.8561185 \n\n   ***********************************************************************\n   Program stops at: 2022-12-11 16:54:57 \n\n\nIt reveals that the adjusted r-square of the gwr is 0.8561 which is significantly better than the globel multiple linear regression model of 0.6472.\n\n\n5.5.4 Visualising GWR Output\nThe output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors. They are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called SDF of the output list.\n\n\nGWR Output\ncondo_resale.sf.adaptive <- st_as_sf(gwr.adaptive$SDF) %>%\n  st_transform(crs=3414)\ncondo_resale.sf.adaptive.svy21 <- st_transform(condo_resale.sf.adaptive, 3414)\n\ngwr.adaptive.output <- as.data.frame(gwr.adaptive$SDF)\ncondo_resale.sf.adaptive <- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))\n\nsummary(gwr.adaptive$SDF$yhat)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  171347  1102001  1385528  1751842  1982307 13887901 \n\n\n\n\n5.5.5 Visualising local R2\nThe code chunks below is used to create an interactive point symbol map to visualise local R2 values.\n\n\nMapping Local R2\ntmap_mode(\"view\")\n\n\ntmap mode set to interactive viewing\n\n\nMapping Local R2\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\n\nWarning: The shape mpsz_svy21 is invalid (after reprojection). See\nsf::st_is_valid\n\n\n\n\n\n\n\nWe can see that the places which have darker points indicate that the local regression model fits well whereas very low values indicate the local model is performing poorly. Mapping these Local R2 values helps us to understand where GWR predicts well and where it predicts poorly which may provide clues about important variables that may be missing from the regression model.\n\n\n\n5.5.6 Visualizing URA Planning Region\nThe code chunks below is used to create an static point symbol map to visualise local R2 by planning region (Here, Central region)\n\n\nMapping URA Planning region\ntm_shape(mpsz_svy21[mpsz_svy21$REGION_N==\"CENTRAL REGION\", ])+\n  tm_polygons()+\ntm_shape(condo_resale.sf.adaptive) + \n  tm_bubbles(col = \"Local_R2\",\n           size = 0.15,\n           border.col = \"gray60\",\n           border.lwd = 1)+\n  tmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nWarning: The shape mpsz_svy21[mpsz_svy21$REGION_N == \"CENTRAL REGION\", ] is\ninvalid. See sf::st_is_valid\n\n\n\n\n\nIn the above map, we can see that the places which have darker static point symbols indicate that the local regression model fits well whereas very low values indicate the local model is performing poorly.\n\n\n\n6. Conclusion\nSo, in this study, we have seen in detail how to build model with geographically weighted regression and the various steps in preparing the final data such as test for normality assumption, tests for non-linearity, checking multi collinearity. Some of the new concepts mentioned in this study are building publication table using Ordinary Least Square Regression OLSR) method and gtsummary method. Stay tuned for upcoming sections……………….."
  },
  {
    "objectID": "Hands-on_Ex/rgeoda.html",
    "href": "Hands-on_Ex/rgeoda.html",
    "title": "REDCAP and AZP Spatial clustering",
    "section": "",
    "text": "Spatial clustering aims to group of a large number of geographic areas or points into a smaller number of regions based on similiarities in one or more variables. Spatially constrained clustering is needed when clusters are required to be spatially contiguous.\nThere are three different approaches explicitly incorporate the contiguity constraint in the optimization process:\n\nSKATER\nRedcap\nAZP"
  },
  {
    "objectID": "Hands-on_Ex/rgeoda.html#rgeoda-package",
    "href": "Hands-on_Ex/rgeoda.html#rgeoda-package",
    "title": "REDCAP and AZP Spatial clustering",
    "section": "rgeoda Package",
    "text": "rgeoda Package\nrgeoda provides spatial data analysis functionalities including Exploratory Spatial Data Analysis, Spatial Cluster Detection and Clustering Analysis, Regionalization, etc.\n\nInstall Packages\n\n\nLoading Packages\npacman::p_load(sf, tidyverse, tmap, rgeoda,\n               plotly, GGally, spdep, hrbrthemes, viridis)\n\n\n\n\nReading Data files\n\nnga_derived <- read_rds(\"data/nga_derived.rds\")\nnga_wp_clus <- read_rds(\"data/nga_wp_clus.rds\")\n\n\n\n1. REDCAP\nREDCAP (Regionalization with dynamically constrained agglomerative clustering and partitioning) is developed by D. Guo (2008). Like SKATER, REDCAP starts from building a spanning tree in 3 different ways (single-linkage, average-linkage, and the complete-linkage). The single-linkage way leads to build a minimum spanning tree. Then, REDCAP provides 2 different ways (first-order and full-order constraining) to prune the tree to find clusters. The first-order approach with a minimum spanning tree is the same as SKATER. In GeoDa and rgeoda, the following methods are provided:\n\nFirst-order and Single-linkage\nFull-order and Complete-linkage\nFull-order and Average-linkage\nFull-order and Single-linkage\nFull-order and Wards-linkage\n\nFor example, to find 4 clusters using the same dataset and weights as above using REDCAP with Full-order and Complete-linkage method\nWe have to remove the region which has no neighbours while creating a Queen contiguity weight\n\nnga_wp_clusf = nga_wp_clus %>%\n                       filter(shapeName != \"Bakassi\")\n\nqueen_w <- queen_weights(nga_wp_clusf)\n\nThe below code chunk is to compute REDCAP clusters with fullorder complete linkage method using redcap() of rgeoda package.\n\nredcap_clusters <- redcap(k = 4, \n                          w = queen_w, \n                          df = nga_derived, \n                          method = \"fullorder-completelinkage\")\n\nThe code chunk below form the join in three steps:\n\nthe redcap_clusters list object will be converted into a matrix;\ncbind() is used to append groups matrix onto nga_wp_clusf to produce an output simple feature object called redcapClusterand\nrename of dplyr package is used to rename as.matrix.groups field as Redcap_Cluster\n\n\nredcap_cluster <- as.matrix(redcap_clusters$Clusters)\nredcapCluster <- cbind(nga_wp_clusf, as.factor(redcap_cluster)) %>%\n  rename(`Redcap_Cluster`=`as.factor.redcap_cluster.`)\n\nLet us map the clusters and view its spatial distribution using tmap options\n\ncmap1 <- tm_shape (redcapCluster) +\n          tm_polygons(\"Redcap_Cluster\",\n          title = \"Redcap Cluster\") +\n          tm_layout(main.title = \"Spatially constrained - Redcap\",\n                    main.title.position = \"center\",\n                    main.title.size = 1.5,\n                    legend.height = 1.5, \n                    legend.width = 1.5,\n                    legend.text.size = 1.5,\n                    legend.title.size = 1.5,\n                    main.title.fontface = \"bold\",\n                    frame = TRUE) +\n        tmap_mode(\"plot\")+\n        tm_borders(alpha = 0.5) +\n        tm_compass(type=\"8star\",\n                  position=c(\"right\", \"top\"))\n\ntmap mode set to plotting\n\ncmap1\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\nVisual interpretation of clusters - Parallel Plot\nTo reveal the clustering variables by cluster very effectively, let us create parallel coordinate plot using the code chunk below, ggparcoord() of GGally package\n\n\nREDCAP - Parallel plot\np <- ggparcoord(data = redcapCluster,\n           columns = c(7,8,16,17,18,20:23),\n           scale = \"std\",\n           alphaLines = 0.2,\n           boxplot = TRUE,\n           groupColumn = \"Redcap_Cluster\",\n           title = \"Multiple Parallel Coordinates Plots of REDCAP Cluster\") +\n  theme_minimal()+\n  scale_color_manual(values=c( \"#69b3a2\", \"red\", \"blue\", \"green\") )+\n  facet_grid(~ `Redcap_Cluster`) +\n  theme(axis.text.x = element_text(angle = 30,size = 5))+\n  xlab(\"\")\n\nggplotly(p)\n\n\n\n\n\n\nInterpretation:\nIt is alarming that in cluster 2 no. of non-functional points is greater than no. of functional points. Also most of the waterpoints here are in urban regions. The cruciality of waterpoints is the same for all the clusters whereas the waterpoints with higher score of staleness (the fact of not being fresh and tasting or smelling unpleasant) are more in cluster 1 and cluster 2 which are to be taken care of.\n\n\n2. SKATER\nThe Spatial C(K)luster Analysis by Tree Edge Removal(SKATER) algorithm introduced by Assuncao et al. (2006) is based on the optimal pruning of a minimum spanning tree that reflects the contiguity structure among the observations. It provides an optimized algorithm to prune to tree into several clusters that their values of selected variables are as similar as possible.\nThe code chunk below computes SKATER clusters using skater() function of rgeoda package\n\nSKATER_clusters <- rgeoda::skater(k = 4, \n                          w = queen_w, \n                          df = nga_derived )\n\naaa0after gda_skater\n\n\nThe code chunk below form the join in three steps:\n\nthe SKATER_clusters list object will be converted into a matrix;\ncbind() is used to append groups matrix onto nga_wp_clusf to produce an output simple feature object called skaterCluster\nrename of dplyr package is used to rename as.matrix.groups field as SKATER_Cluster\n\n\nSKATER_cluster <- as.matrix(SKATER_clusters$Clusters)\nskaterCluster <- cbind(nga_wp_clusf, as.factor(SKATER_cluster)) %>%\n  rename(`SKATER_Cluster`=`as.factor.SKATER_cluster.`)\n\nLet us map the clusters and view its spatial distribution using tmap options\n\ncmap2 <- tm_shape (skaterCluster) +\n          tm_polygons(\"SKATER_Cluster\",\n          title = \"SKATER Cluster\") +\n          tm_layout(main.title = \"Spatially constrained - SKATER\",\n                    main.title.position = \"center\",\n                    main.title.size = 1.5,\n                    legend.height = 1.5, \n                    legend.width = 1.5,\n                    legend.text.size = 1.5,\n                    legend.title.size = 1.5,\n                    main.title.fontface = \"bold\",\n                    frame = TRUE) +\n        tmap_mode(\"plot\")+\n        tm_borders(alpha = 0.5) +\n        tm_compass(type=\"8star\",\n                  position=c(\"right\", \"top\"))\n\ntmap mode set to plotting\n\ncmap2\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\nVisual interpretation of clusters - Parallel Plot\nTo reveal the clustering variables by cluster very effectively, let us create parallel coordinate plot using the code chunk below, ggparcoord() of GGally package\n\n\nSKATER - Parallel plot\np <- ggparcoord(data = skaterCluster,\n           columns = c(7,8,16,17,18,20:23),\n           scale = \"std\",\n           alphaLines = 0.2,\n           boxplot = TRUE,\n           groupColumn = \"SKATER_Cluster\",\n           title = \"Multiple Parallel Coordinates Plots of SKATER Cluster\") +\n  theme_minimal()+\n  scale_color_manual(values=c( \"#69b3a2\", \"red\", \"blue\", \"green\") )+\n  facet_grid(~ `SKATER_Cluster`) +\n  theme(axis.text.x = element_text(angle = 30, size = 5))+\n  xlab(\"\")\n\nggplotly(p)\n\n\n\n\n\n\nInterpretation:\nProportion of functional waterpoints is greater than non functional waterpoints in both clusters 1 and 2 which is very good sign and also the percentage of waterpoints with high staleness is lesser in numbers in cluster 2 which is again positive. But the same staleness percentage is quite high in cluster 1 which is a matter of concern.\n\n\n3. AZP\nThe automatic zoning procedure (AZP) was initially outlined in Openshaw (1977) as a way to address some of the consequences of the modifiable areal unit problem (MAUP). In essence, it consists of a heuristic to find the best set of combinations of contiguous spatial units into p regions, minimizing the within-sum of squares as a criterion of homogeneity. The number of regions needs to be specified beforehand, as in most other clustering methods considered so far.\nrgeoda provides three different heuristic algorithms to find an optimal solution for AZP:\n\ngreedy\nTabu Search\nSimulated Annealing\n\n\na) AZP Greedy\nThe original AZP heuristic is a local optimization procedure that cycles through a series of possible swaps between spatial units at the boundary of a set of regions. The process starts with an initial feasible solution, i.e., a grouping of n spatial units into p contiguous regions. This initial solution can be constructed in several different ways. The initial solution must satisfy the contiguity constraints. For example, this can be accomplished by growing a set of contiguous regions from p randomly selected seed units by adding neighboring locations until the contiguity constraint can no longer be met.\n\nazp_greedyclusters <- azp_greedy(p = 4, \n                           w = queen_w, \n                           df = nga_derived)\n\n\n\nb) AZP Tabu Search\nTo use tabu search algorithm in maxp function, we can specify the parameters of tabu_length and conv_tabu:\n\nazp_tabuclusters <- azp_tabu(p = 4, \n                         w = queen_w, \n                         df = nga_derived, \n                         tabu_length = 10, \n                         conv_tabu = 10)\n\n\n\nc) AZP Simulated Annealing\nTo apply simulated annealing algorithm in maxp function with the parameter of cooling rate:\n\nazp_clusters <- azp_sa( p = 4, \n                        w = queen_w, \n                        df = nga_derived, \n                        cooling_rate = 0.85)\nazp_clusters\n\n$Clusters\n  [1] 1 1 1 3 2 1 2 2 1 1 1 2 2 2 2 1 2 1 1 3 1 2 1 3 1 1 1 2 2 1 1 2 1 2 2 1 2\n [38] 2 1 1 2 2 1 1 1 1 1 3 2 1 1 1 1 1 1 1 1 1 3 1 1 2 1 3 3 1 1 2 1 1 2 2 1 3\n [75] 2 2 1 1 1 1 1 1 1 3 1 3 3 2 1 1 2 1 2 1 3 3 2 1 2 3 1 2 1 1 2 1 1 3 2 1 3\n[112] 3 3 2 3 2 2 1 1 1 1 1 2 2 1 1 3 3 2 3 2 2 1 1 2 2 2 3 2 3 3 2 1 1 1 3 3 3\n[149] 3 3 1 1 3 2 1 1 1 1 1 2 1 1 2 1 2 3 3 2 1 1 2 1 1 1 1 2 2 2 1 1 1 1 1 1 1\n[186] 1 1 1 1 2 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 2 1 2 2 2 1 1 1 3 3 3 2 1\n[223] 2 1 3 2 3 2 1 1 1 2 1 3 1 1 1 3 2 1 3 1 1 3 1 1 2 3 2 3 2 1 1 1 3 2 1 3 1\n[260] 1 3 2 3 3 2 4 2 1 1 1 2 2 1 1 2 2 2 2 2 1 2 2 1 2 1 1 2 2 2 2 1 2 2 1 2 1\n[297] 2 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 2 2 2 2 1 1 1 1 3 1 1 2 2 2 1 1 1 2 1\n[334] 1 1 1 1 1 1 1 1 1 2 3 2 1 2 1 3 2 1 2 1 2 1 2 1 3 2 2 2 2 1 1 1 1 1 1 2 1\n[371] 1 1 1 2 1 1 2 2 1 1 1 1 1 1 3 2 2 3 2 1 1 1 4 2 3 3 2 3 1 2 1 3 2 2 1 3 2\n[408] 1 3 2 1 2 2 3 2 1 1 3 3 2 1 1 3 2 1 2 3 2 1 1 2 1 3 2 3 3 2 1 1 1 1 2 2 3\n[445] 3 1 2 1 2 3 1 1 2 1 1 3 2 1 1 2 2 2 1 1 3 2 1 2 1 2 1 1 3 2 3 2 3 2 1 1 3\n[482] 3 2 2 3 2 1 3 3 2 1 3 1 3 3 2 2 2 1 1 1 3 1 1 2 1 2 1 2 2 1 1 3 2 2 1 1 2\n[519] 1 1 1 1 1 1 2 3 2 2 2 1 1 1 1 1 2 2 1 1 1 2 1 1 2 2 1 1 1 1 1 1 1 2 1 1 2\n[556] 2 1 2 2 1 2 1 2 1 1 2 2 2 1 1 1 2 1 1 1 1 1 1 1 2 2 1 1 2 1 1 1 1 1 1 2 1\n[593] 1 2 2 1 1 2 1 1 1 1 2 1 1 1 2 1 2 1 1 2 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1\n[630] 1 2 1 2 1 1 2 2 2 2 3 2 1 2 1 1 2 3 3 2 1 3 3 3 2 1 3 1 2 3 3 3 2 3 2 1 3\n[667] 2 1 1 1 3 3 2 1 1 1 3 2 2 1 2 3 2 3 3 2 1 1 1 1 3 2 2 1 1 1 1 1 3 3 3 2 2\n[704] 2 1 3 1 2 1 3 1 2 3 2 1 1 2 2 1 1 1 1 1 1 2 1 1 2 1 3 1 2 1 1 1 1 2 2 1 3\n[741] 2 2 2 1 1 1 3 2 1 1 3 3 3 2 1 1 1 1 1 3 2 1 1 1 1 1 1 1 2 3 1 3 3\n\n$`Total sum of squares`\n[1] 6957\n\n$`Within-cluster sum of squares`\n[1] 747.3657 700.0637 635.9083 519.1652 666.1842 596.0765 526.5721 582.3839\n[9] 762.0337\n\n$`Total within-cluster sum of squares`\n[1] 1221.247\n\n$`The ratio of between to total sum of squares`\n[1] 0.1755421\n\n\nLet us perform the join as we did in previous methods. The code chunk below form the join in three steps:\n\nthe azp_clusters list object will be converted into a matrix;\ncbind() is used to append groups matrix onto nga_wp_clusf to produce an output simple feature object called AZPCluster\nrename of dplyr package is used to rename as.factor.azp_cluster..groups field as AZP_Cluster\n\n\nazp_cluster <- as.matrix(azp_clusters$Clusters)\nAZPCluster <- cbind(nga_wp_clusf, as.factor(azp_cluster)) %>%\n  rename(`AZP_Cluster`=`as.factor.azp_cluster.`)\n\nLet us map the clusters and view its spatial distribution using tmap options\n\ncmap3 <- tm_shape (AZPCluster) +\n          tm_polygons(\"AZP_Cluster\",\n          title = \"AZP Cluster\") +\n          tm_layout(main.title = \"AZP Clustering Distribution\",\n                    main.title.position = \"center\",\n                    main.title.size = 1.5,\n                    legend.height = 1.5, \n                    legend.width = 1.5,\n                    legend.text.size = 1.5,\n                    legend.title.size = 1.5,\n                    main.title.fontface = \"bold\",\n                    frame = TRUE) +\n        tmap_mode(\"plot\")+\n        tm_borders(alpha = 0.5) +\n        tm_compass(type=\"8star\",\n                  position=c(\"right\", \"top\"))\n\ntmap mode set to plotting\n\ncmap3\n\n\n\n\nVisual interpretation of clusters - Parallel Plot\n\n\nAZP - Parallel plot\np <- ggparcoord(data = AZPCluster,\n           columns = c(7,8,16,17,18,20:23),\n           scale = \"std\",\n           alphaLines = 0.2,\n           boxplot = TRUE,\n           groupColumn = \"AZP_Cluster\",\n           title = \"Multiple Parallel Coordinates Plots of AZP Cluster\") +\n  theme_minimal()+\n  scale_color_manual(values=c( \"#69b3a2\", \"red\", \"blue\", \"green\") )+\n  facet_grid(~ `AZP_Cluster`) +\n  theme(axis.text.x = element_text(angle = 30, size=5))+\n  xlab(\"\")\n\nggplotly(p)\n\n\n\n\n\n\nInterpretation:\nWe can infer from the chart that we are substantiating the results of REDCAP where in cluster 2 no. of non-functional points is greater than no. of functional points. Also most of the waterpoints here are in urban regions. The cruciality of waterpoints is the same for all the clusters whereas the waterpoints with higher score of staleness (the fact of not being fresh and tasting or smelling unpleasant) are more in cluster 1 and cluster 2 which are to be taken care of.\n\ntmap_arrange(cmap1, cmap2, cmap3,\n             ncol = 3)"
  },
  {
    "objectID": "InClass_Ex/InClass_Ex1.html",
    "href": "InClass_Ex/InClass_Ex1.html",
    "title": "Spatial Weights and Applications",
    "section": "",
    "text": "2. Introduction\nSpatial statistics integrate space and spatial relationships directly into their mathematics (area, distance, length, or proximity, for example). Typically, these spatial relationships are defined formally through values called spatial weights. Spatial weights are structured into a spatial weights matrix and stored as a spatial weights matrix file. Spatial weights are a way to define spatial neighbourhood. One practical use case can be - Are these two planning zones neighbours? They can be determined by either contiguity neighbours - rook / hexagon / queen or adjacency based neighbours - weight matrix\n\n\n\n\n\n\n\n3. Glimpse of Steps\nSome of the important steps performed in this study are as follows\n\nimporting geospatial data using appropriate function(s) of sf package,\nimporting csv file using appropriate function of readr package,\nperforming relational join using appropriate join function of dplyr package,\ncomputing spatial weights using appropriate functions of spdep package, and\ncalculating spatially lagged variables using appropriate functions of spdep package.\n\n\n\n4. Data\nFollowing two data sets are used:\n\nHunan country boundary layer. This is a geospatial data set in ESRI shapefile format.\nHunan_2012.csv: This csv file contains selected Hunan’s local development indicators in 2012.\n\n\n\n5. Deep Dive into Map Analysis\n\n5.1 Installing libraries and Importing files\np_load function pf pacman package is used to install and load sf ,tidyverse and tmap packages into R environment. First, let us import Hunan shapefile into R using st_read() of sf package. The imported shapefile will be simple features Object of sf. Next, let us import Hunan_2012.csv into R by using read_csv() of readr package. The output is R dataframe class.\n\npacman::p_load(sf, spdep, tmap, tidyverse)\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nReading layer `Hunan' from data source \n  `D:\\raveenaclr\\Geospatial Analytics\\InClass_Ex\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\nRows: 88 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): County, City\ndbl (27): avg_wage, deposite, FAI, Gov_Rev, Gov_Exp, GDP, GDPPC, GIO, Loan, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n5.2 Data Wrangling\nThe code chunk below will be used to update the attribute table of hunan’s SpatialPolygonsDataFrame with the attribute fields of hunan2012 dataframe. This is performed by using left_join() of dplyr package.\n\nhunan <- left_join(hunan,hunan2012)\n\nJoining, by = \"County\"\n\n\n\n\n5.3 Visualising Regional Development Indicator\nNow, we are going to prepare a basemap and a choropleth map showing the distribution of GDPPC 2012 by using qtm() of tmap package.\n\nbasemap <- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.5)\n\ngdppc <- qtm(hunan, \"GDPPC\")\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n5.4 Computing Contiguity Spatial Weights\nIn this sub section, let u see how to use poly2nb() of spdep package to compute contiguity weight matrices for the study area. This function builds a neighbours list based on regions with contiguous boundaries.\n\n5.4.1 Computing (QUEEN) contiguity based neighbours\nThe code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q <- poly2nb(hunan, queen=TRUE)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one heighbours.\n\nwm_q[[1]]\n\n[1]  2  3  4 57 85\n\nhunan$County[1]\n\n[1] \"Anxiang\"\n\nhunan$NAME_3[c(2,3,4,57,85)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\nnb1 <- wm_q[[1]]\nnb1 <- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\n\n\n\n5.4.2 Creating (ROOK) contiguity based neighbours\nThe code chunk below is used to compute Rook contiguity weight matrix.\n\nwm_r <- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two area units with only one heighbours.\n\n\n5.4.3 Visualising contiguity weights\nA connectivity graph takes a point and displays a line to each neighboring point. One of the most common method to get points in order to make connectivity graphs is polygon centroids. Let us calculate these in the sf package before moving onto the graphs.\n\nlongitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[1]])\nlatitude <- map_dbl(hunan$geometry, ~st_centroid(.x)[[2]])\ncoords <- cbind(longitude, latitude)\n\n\n\n5.4.4 Plotting Queen contiguity based neighbours map\nThe code chunk below is used to plot Queen contiguity based neighbours map\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\")\n\n\n\n\n\n\n5.4.5 Plotting Rook contiguity based neighbours map\nThe code chunk below is used to plot Rook contiguity based neighbours map\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\n\n\n5.4.6 Plotting both Queen and Rook contiguity based neighbours maps\nThe code chunk below is used to plot both Queen and Rook contiguity based neighbours maps\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_q, coords, pch = 19, cex = 0.6, add = TRUE, col= \"red\", main=\"Queen Contiguity\")\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_r, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\", main=\"Rook Contiguity\")\n\n\n\n\n\n\n\n5.5 Computing distance based neighbours\nIn this section, let us learn how to derive distance-based weight matrices by using dnearneigh() of spdep package.\n\n5.5.1 Determine the cut-off distance\nFirstly, let us determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\n#coords <- coordinates(hunan)\nk1 <- knn2nb(knearneigh(coords))\nk1dists <- unlist(nbdists(k1, coords, longlat = TRUE))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.79   32.57   38.01   39.07   44.52   61.79 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 61.79 km, so let us consider it as the upper threshold gives certainty that all units will have at least one neighbour.\n\n\n5.5.2 Computing fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d62 <- dnearneigh(coords, 0, 62, longlat = TRUE)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 324 \nPercentage nonzero weights: 4.183884 \nAverage number of links: 3.681818 \n\n\n\n\n5.5.3 Plotting fixed distance weight matrix\nLet us plot the distance weight matrix by using the code chunk below.\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE)\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\n\n\n\n\nAlternative Approach\nWe can also plot both of them next to each other by using the code chunk below.\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\")\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08, main=\"1st nearest neighbours\")\nplot(hunan$geometry, border=\"lightgrey\")\nplot(wm_d62, coords, add=TRUE, pch = 19, cex = 0.6, main=\"Distance link\")\n\n\n\n\n\n\n5.5.4 Computing adaptive distance weight matrix\nMore densely settled areas (usually the urban areas) tend to have more neighbours and the less densely settled areas (usually the rural counties) tend to have lesser neighbours. Having many neighbours smoothes the neighbour relationship across more neighbours. It is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below.\n\nknn6 <- knn2nb(knearneigh(coords, k=6))\nknn6\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\n\n\n5.5.5 Plotting distance based neighbours\nLet us plot the weight matrix using the code chunk below.\n\nplot(hunan$geometry, border=\"lightgrey\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\n\n\n\n5.6 Weights based on IDW\nIn this section, let us see how to derive a spatial weight matrix based on Inversed Distance method.\nFirst, we will compute the distances between areas by using nbdists() of spdep.\n\ndist <- nbdists(wm_q, coords, longlat = TRUE)\nids <- lapply(dist, function(x) 1/(x))\nids\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\n[[2]]\n[1] 0.01535405 0.01764308 0.01925924 0.02323898 0.01719350\n\n[[3]]\n[1] 0.03916350 0.02822040 0.03695795 0.01395765\n\n[[4]]\n[1] 0.01820896 0.02822040 0.03414741 0.01539065\n\n[[5]]\n[1] 0.03695795 0.03414741 0.01524598 0.01618354\n\n[[6]]\n[1] 0.015390649 0.015245977 0.021748129 0.011883901 0.009810297\n\n[[7]]\n[1] 0.01708612 0.01473997 0.01150924 0.01872915\n\n[[8]]\n[1] 0.02022144 0.03453056 0.02529256 0.01036340 0.02284457 0.01500600 0.01515314\n\n[[9]]\n[1] 0.02022144 0.01574888 0.02109502 0.01508028 0.02902705 0.01502980\n\n[[10]]\n[1] 0.02281552 0.01387777 0.01538326 0.01346650 0.02100510 0.02631658 0.01874863\n[8] 0.01500046\n\n[[11]]\n[1] 0.01882869 0.02243492 0.02247473\n\n[[12]]\n[1] 0.02779227 0.02419652 0.02333385 0.02986130 0.02335429\n\n[[13]]\n[1] 0.02779227 0.02650020 0.02670323 0.01714243\n\n[[14]]\n[1] 0.01882869 0.01233868 0.02098555\n\n[[15]]\n[1] 0.02650020 0.01233868 0.01096284 0.01562226\n\n[[16]]\n[1] 0.02281552 0.02466962 0.02765018 0.01476814 0.01671430\n\n[[17]]\n[1] 0.01387777 0.02243492 0.02098555 0.01096284 0.02466962 0.01593341 0.01437996\n\n[[18]]\n[1] 0.02039779 0.02032767 0.01481665 0.01473691 0.01459380\n\n[[19]]\n[1] 0.01538326 0.01926323 0.02668415 0.02140253 0.01613589 0.01412874\n\n[[20]]\n[1] 0.01346650 0.02039779 0.01926323 0.01723025 0.02153130 0.01469240 0.02327034\n\n[[21]]\n[1] 0.02668415 0.01723025 0.01766299 0.02644986 0.02163800\n\n[[22]]\n[1] 0.02100510 0.02765018 0.02032767 0.02153130 0.01489296\n\n[[23]]\n[1] 0.01481665 0.01469240 0.01401432 0.02246233 0.01880425 0.01530458 0.01849605\n\n[[24]]\n[1] 0.02354598 0.01837201 0.02607264 0.01220154 0.02514180\n\n[[25]]\n[1] 0.02354598 0.02188032 0.01577283 0.01949232 0.02947957\n\n[[26]]\n[1] 0.02155798 0.01745522 0.02212108 0.02220532\n\n[[27]]\n[1] 0.02155798 0.02490625 0.01562326\n\n[[28]]\n[1] 0.01837201 0.02188032 0.02229549 0.03076171 0.02039506\n\n[[29]]\n[1] 0.02490625 0.01686587 0.01395022\n\n[[30]]\n[1] 0.02090587\n\n[[31]]\n[1] 0.02607264 0.01577283 0.01219005 0.01724850 0.01229012 0.01609781 0.01139438\n[8] 0.01150130\n\n[[32]]\n[1] 0.01220154 0.01219005 0.01712515 0.01340413 0.01280928 0.01198216 0.01053374\n[8] 0.01065655\n\n[[33]]\n[1] 0.01949232 0.01745522 0.02229549 0.02090587 0.01979045\n\n[[34]]\n[1] 0.03113041 0.03589551 0.02882915\n\n[[35]]\n[1] 0.01766299 0.02185795 0.02616766 0.02111721 0.02108253 0.01509020\n\n[[36]]\n[1] 0.01724850 0.03113041 0.01571707 0.01860991 0.02073549 0.01680129\n\n[[37]]\n[1] 0.01686587 0.02234793 0.01510990 0.01550676\n\n[[38]]\n[1] 0.01401432 0.02407426 0.02276151 0.01719415\n\n[[39]]\n[1] 0.01229012 0.02172543 0.01711924 0.02629732 0.01896385\n\n[[40]]\n[1] 0.01609781 0.01571707 0.02172543 0.01506473 0.01987922 0.01894207\n\n[[41]]\n[1] 0.02246233 0.02185795 0.02205991 0.01912542 0.01601083 0.01742892\n\n[[42]]\n[1] 0.02212108 0.01562326 0.01395022 0.02234793 0.01711924 0.01836831 0.01683518\n\n[[43]]\n[1] 0.01510990 0.02629732 0.01506473 0.01836831 0.03112027 0.01530782\n\n[[44]]\n[1] 0.01550676 0.02407426 0.03112027 0.01486508\n\n[[45]]\n[1] 0.03589551 0.01860991 0.01987922 0.02205991 0.02107101 0.01982700\n\n[[46]]\n[1] 0.03453056 0.04033752 0.02689769\n\n[[47]]\n[1] 0.02529256 0.02616766 0.04033752 0.01949145 0.02181458\n\n[[48]]\n[1] 0.02313819 0.03370576 0.02289485 0.01630057 0.01818085\n\n[[49]]\n[1] 0.03076171 0.02138091 0.02394529 0.01990000\n\n[[50]]\n[1] 0.01712515 0.02313819 0.02551427 0.02051530 0.02187179\n\n[[51]]\n[1] 0.03370576 0.02138091 0.02873854\n\n[[52]]\n[1] 0.02289485 0.02394529 0.02551427 0.02873854 0.03516672\n\n[[53]]\n[1] 0.01630057 0.01979945 0.01253977\n\n[[54]]\n[1] 0.02514180 0.02039506 0.01340413 0.01990000 0.02051530 0.03516672\n\n[[55]]\n[1] 0.01280928 0.01818085 0.02187179 0.01979945 0.01882298\n\n[[56]]\n[1] 0.01036340 0.01139438 0.01198216 0.02073549 0.01214479 0.01362855 0.01341697\n\n[[57]]\n[1] 0.028079221 0.017643082 0.031423501 0.029114131 0.013520292 0.009903702\n\n[[58]]\n[1] 0.01925924 0.03142350 0.02722997 0.01434859 0.01567192\n\n[[59]]\n[1] 0.01696711 0.01265572 0.01667105 0.01785036\n\n[[60]]\n[1] 0.02419652 0.02670323 0.01696711 0.02343040\n\n[[61]]\n[1] 0.02333385 0.01265572 0.02343040 0.02514093 0.02790764 0.01219751 0.02362452\n\n[[62]]\n[1] 0.02514093 0.02002219 0.02110260\n\n[[63]]\n[1] 0.02986130 0.02790764 0.01407043 0.01805987\n\n[[64]]\n[1] 0.02911413 0.01689892\n\n[[65]]\n[1] 0.02471705\n\n[[66]]\n[1] 0.01574888 0.01726461 0.03068853 0.01954805 0.01810569\n\n[[67]]\n[1] 0.01708612 0.01726461 0.01349843 0.01361172\n\n[[68]]\n[1] 0.02109502 0.02722997 0.03068853 0.01406357 0.01546511\n\n[[69]]\n[1] 0.02174813 0.01645838 0.01419926\n\n[[70]]\n[1] 0.02631658 0.01963168 0.02278487\n\n[[71]]\n[1] 0.01473997 0.01838483 0.03197403\n\n[[72]]\n[1] 0.01874863 0.02247473 0.01476814 0.01593341 0.01963168\n\n[[73]]\n[1] 0.01500046 0.02140253 0.02278487 0.01838483 0.01652709\n\n[[74]]\n[1] 0.01150924 0.01613589 0.03197403 0.01652709 0.01342099 0.02864567\n\n[[75]]\n[1] 0.011883901 0.010533736 0.012539774 0.018822977 0.016458383 0.008217581\n\n[[76]]\n[1] 0.01352029 0.01434859 0.01689892 0.02471705 0.01954805 0.01349843 0.01406357\n\n[[77]]\n[1] 0.014736909 0.018804247 0.022761507 0.012197506 0.020022195 0.014070428\n[7] 0.008440896\n\n[[78]]\n[1] 0.02323898 0.02284457 0.01508028 0.01214479 0.01567192 0.01546511 0.01140779\n\n[[79]]\n[1] 0.01530458 0.01719415 0.01894207 0.01912542 0.01530782 0.01486508 0.02107101\n\n[[80]]\n[1] 0.01500600 0.02882915 0.02111721 0.01680129 0.01601083 0.01982700 0.01949145\n[8] 0.01362855\n\n[[81]]\n[1] 0.02947957 0.02220532 0.01150130 0.01979045 0.01896385 0.01683518\n\n[[82]]\n[1] 0.02327034 0.02644986 0.01849605 0.02108253 0.01742892\n\n[[83]]\n[1] 0.023354289 0.017142433 0.015622258 0.016714303 0.014379961 0.014593799\n[7] 0.014892965 0.018059871 0.008440896\n\n[[84]]\n[1] 0.01872915 0.02902705 0.01810569 0.01361172 0.01342099 0.01297994\n\n[[85]]\n [1] 0.011451133 0.017193502 0.013957649 0.016183544 0.009810297 0.010656545\n [7] 0.013416965 0.009903702 0.014199260 0.008217581 0.011407794\n\n[[86]]\n[1] 0.01515314 0.01502980 0.01412874 0.02163800 0.01509020 0.02689769 0.02181458\n[8] 0.02864567 0.01297994\n\n[[87]]\n[1] 0.01667105 0.02362452 0.02110260 0.02058034\n\n[[88]]\n[1] 0.01785036 0.02058034\n\n\n\n5.6.1 Row-standardised weights matrix\nNext, we need to assign weights to each neighboring polygon\n\nrswm_q <- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\nrswm_q$weights[10]\n\n[[1]]\n[1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125\n\nrswm_ids <- nb2listw(wm_q, glist=ids, style=\"B\", zero.policy=TRUE)\nrswm_ids\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn       S0        S1     S2\nB 88 7744 8.786867 0.3776535 3.8137\n\nrswm_ids$weights[1]\n\n[[1]]\n[1] 0.01535405 0.03916350 0.01820896 0.02807922 0.01145113\n\nsummary(unlist(rswm_ids$weights))\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n0.008218 0.015088 0.018739 0.019614 0.022823 0.040338 \n\n\n\n\n\n5.7 Application of Spatial Weight Matrix\nIn this section, let us see how to create four different spatial lagged variables,\n\nspatial lag with row-standardized weights,\nspatial lag as a sum of neighbouring values,\nspatial window average, and spatial window sum.\n\n\n5.7.1 Spatial lag with row-standardized weights\nFinally, we’ll compute the average neighbor GDPPC value for each polygon. These values are often referred to as spatially lagged values. We can append the spatially lag GDPPC values onto hunan sf data frame by using the code chunk below.\n\nGDPPC.lag <- lag.listw(rswm_q, hunan$GDPPC)\nGDPPC.lag\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\nnb1 <- wm_q[[1]]\nnb1 <- hunan$GDPPC[nb1]\nnb1\n\n[1] 20981 34592 24473 21311 22879\n\nlag.list <- list(hunan$NAME_3, lag.listw(rswm_q, hunan$GDPPC))\nlag.res <- as.data.frame(lag.list)\ncolnames(lag.res) <- c(\"NAME_3\", \"lag GDPPC\")\nhunan <- left_join(hunan,lag.res)\n\nJoining, by = \"NAME_3\"\n\n\n\n\n5.7.2 Comparing GDPPC and spatial lag GDPPC maps\nNow, let us plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below.\n\ngdppc <- qtm(hunan, \"GDPPC\")\nlag_gdppc <- qtm(hunan, \"lag GDPPC\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n5.7.3 Spatial lag as a sum of neighboring values\nWe can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist = in the nb2listw function to explicitly assign these weights.\n\nb_weights <- lapply(wm_q, function(x) 0*x + 1)\nb_weights2 <- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\nlag_sum <- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nlag.res <- as.data.frame(lag_sum)\ncolnames(lag.res) <- c(\"NAME_3\", \"lag_sum GDPPC\")\n\n\n\n5.7.4 Comparing GDPPC and Spatial Lag Sum GDPPC maps\nWe can plot both the GDPPC and Spatial Lag Sum GDPPC for comparison using the code chunk below.\n\nhunan <- left_join(hunan, lag.res)\n\nJoining, by = \"NAME_3\"\n\ngdppc <- qtm(hunan, \"GDPPC\")\nlag_sum_gdppc <- qtm(hunan, \"lag_sum GDPPC\")\ntmap_arrange(gdppc, lag_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n5.7.5 Spatial window average\nThe spatial window average uses row-standardized weights and includes the diagonal element.\n\nFirstly, let us assign k6 to a new variable because we will directly alter its structure to add the diagonal elements.\nTo add the diagonal element to the neighbour list, let us use include.self() from spdep.\nAs a next step, we have obtained weights with nb2listw()\nLastly, we just need to create the lag variable from our weight structure and GDPPC variable.\n\n\nwm_q1 <- wm_q\ninclude.self(wm_q1)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nwm_q1 <- nb2listw(wm_q1)\nwm_q1\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\nlag_w_avg_gpdpc <- lag.listw(wm_q1, \n                             hunan$GDPPC)\nlag_w_avg_gpdpc\n\n [1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80 43747.00 33582.71\n [9] 45651.17 32027.62 32671.00 20810.00 25711.50 30672.33 33457.75 31689.20\n[17] 20269.00 23901.60 25126.17 21903.43 22718.60 25918.80 20307.00 20023.80\n[25] 16576.80 18667.00 14394.67 19848.80 15516.33 20518.00 17572.00 15200.12\n[33] 18413.80 14419.33 24094.50 22019.83 12923.50 14756.00 13869.80 12296.67\n[41] 15775.17 14382.86 11566.33 13199.50 23412.00 39541.00 36186.60 16559.60\n[49] 20772.50 19471.20 19827.33 15466.80 12925.67 18577.17 14943.00 24913.00\n[57] 25093.00 24428.80 17003.00 21143.75 20435.00 17131.33 24569.75 23835.50\n[65] 26360.00 47383.40 55157.75 37058.00 21546.67 23348.67 42323.67 28938.60\n[73] 25880.80 47345.67 18711.33 29087.29 20748.29 35933.71 15439.71 29787.50\n[81] 18145.00 21617.00 29203.89 41363.67 22259.09 44939.56 16902.00 16930.00\n\nlag.list.wm_q1 <- list(hunan$NAME_3, lag.listw(wm_q1, hunan$GDPPC))\nlag_wm_q1.res <- as.data.frame(lag.list.wm_q1)\ncolnames(lag_wm_q1.res) <- c(\"NAME_3\", \"lag_window_avg GDPPC\")\nhunan <- left_join(hunan, lag_wm_q1.res)\n\nJoining, by = \"NAME_3\"\n\n\n\n\n5.7.6 Comparing GDPPC and lag_window_avg GDPPC maps\nWe can plot both the GDPPC and Lag window average GDPPC for comparison using the code chunk below.\n\ngdppc <- qtm(hunan, \"GDPPC\")\nw_avg_gdppc <- qtm(hunan, \"lag_window_avg GDPPC\")\ntmap_arrange(gdppc, w_avg_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n5.7.7 Spatial window sum\nThe spatial window sum is the counter part of the window average, but without using row-standardized weights.\nFirstly, let us assign binary weights to the neighbor structure that includes the diagonal element. To add the diagonal element to the neighbour list, let us just use include.self() from spdep. Next, let us assign binary weights to the neighbour structure that includes the diagonal element Now, let us use nb2listw() and glist() to explicitly assign weight values.\n\nwm_q1 <- wm_q\ninclude.self(wm_q1)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 536 \nPercentage nonzero weights: 6.921488 \nAverage number of links: 6.090909 \n\nwm_q1\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nb_weights <- lapply(wm_q1, function(x) 0*x + 1)\nb_weights[1]\n\n[[1]]\n[1] 1 1 1 1 1\n\nb_weights2 <- nb2listw(wm_q1, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\nw_sum_gdppc <- list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC))\nw_sum_gdppc\n\n[[1]]\n [1] \"Anxiang\"       \"Hanshou\"       \"Jinshi\"        \"Li\"           \n [5] \"Linli\"         \"Shimen\"        \"Liuyang\"       \"Ningxiang\"    \n [9] \"Wangcheng\"     \"Anren\"         \"Guidong\"       \"Jiahe\"        \n[13] \"Linwu\"         \"Rucheng\"       \"Yizhang\"       \"Yongxing\"     \n[17] \"Zixing\"        \"Changning\"     \"Hengdong\"      \"Hengnan\"      \n[21] \"Hengshan\"      \"Leiyang\"       \"Qidong\"        \"Chenxi\"       \n[25] \"Zhongfang\"     \"Huitong\"       \"Jingzhou\"      \"Mayang\"       \n[29] \"Tongdao\"       \"Xinhuang\"      \"Xupu\"          \"Yuanling\"     \n[33] \"Zhijiang\"      \"Lengshuijiang\" \"Shuangfeng\"    \"Xinhua\"       \n[37] \"Chengbu\"       \"Dongan\"        \"Dongkou\"       \"Longhui\"      \n[41] \"Shaodong\"      \"Suining\"       \"Wugang\"        \"Xinning\"      \n[45] \"Xinshao\"       \"Shaoshan\"      \"Xiangxiang\"    \"Baojing\"      \n[49] \"Fenghuang\"     \"Guzhang\"       \"Huayuan\"       \"Jishou\"       \n[53] \"Longshan\"      \"Luxi\"          \"Yongshun\"      \"Anhua\"        \n[57] \"Nan\"           \"Yuanjiang\"     \"Jianghua\"      \"Lanshan\"      \n[61] \"Ningyuan\"      \"Shuangpai\"     \"Xintian\"       \"Huarong\"      \n[65] \"Linxiang\"      \"Miluo\"         \"Pingjiang\"     \"Xiangyin\"     \n[69] \"Cili\"          \"Chaling\"       \"Liling\"        \"Yanling\"      \n[73] \"You\"           \"Zhuzhou\"       \"Sangzhi\"       \"Yueyang\"      \n[77] \"Qiyang\"        \"Taojiang\"      \"Shaoyang\"      \"Lianyuan\"     \n[81] \"Hongjiang\"     \"Hengyang\"      \"Guiyang\"       \"Changsha\"     \n[85] \"Taoyuan\"       \"Xiangtan\"      \"Dao\"           \"Jiangyong\"    \n\n[[2]]\n [1] 124236 113624  96573 110950 109081 106244 174988 235079 273907 256221\n[11]  98013 104050 102846  92017 133831 158446 141883 119508 150757 153324\n[21] 113593 129594 142149 100119  82884  74668  43184  99244  46549  20518\n[31] 140576 121601  92069  43258 144567 132119  51694  59024  69349  73780\n[41]  94651 100680  69398  52798 140472 118623 180933  82798  83090  97356\n[51]  59482  77334  38777 111463  74715 174391 150558 122144  68012  84575\n[61] 143045  51394  98279  47671  26360 236917 220631 185290  64640  70046\n[71] 126971 144693 129404 284074 112268 203611 145238 251536 108078 238300\n[81] 108870 108085 262835 248182 244850 404456  67608  33860\n\nw_sum_gdppc.res <- as.data.frame(w_sum_gdppc)\ncolnames(w_sum_gdppc.res) <- c(\"NAME_3\", \"w_sum GDPPC\")\nhunan <- left_join(hunan, w_sum_gdppc.res)\n\nJoining, by = \"NAME_3\"\n\n\n\n\n5.7.8 Comparing GDPPC and lag_sum GDPPC maps\nWe can plot both the GDPPC and Lag sum GDPPC for comparison using the code chunk below.\n\ngdppc <- qtm(hunan, \"GDPPC\")\nw_sum_gdppc <- qtm(hunan, \"w_sum GDPPC\")\ntmap_arrange(gdppc, w_sum_gdppc, asp=1, ncol=2)\n\n\n\n\n\n\n\n\n6. Conclusion & Key Takeaway\nIn this section, we have seen what is spatial weights and how to compute spatial weights. We have also witnessed types of contiguity based neighbours and what are the applications of it. Finally we have learnt how to compute various types of spatially lagged variables and compare it with general ones. Now that we have understood about spatial weights, let us see how spatial weights are used for auto correlation in upcoming session. Stay tuned………………"
  },
  {
    "objectID": "InClass_Ex/InClass_Ex2.html",
    "href": "InClass_Ex/InClass_Ex2.html",
    "title": "Spatial Patterns of Non-functional points - Nigeria",
    "section": "",
    "text": "Table1: Datasets Used\n\n\nData Type\nDescription\nSource\n\n\n\n\nGeospatial\nNigeria Level-2 Administrative Boundary\nGeoboundaries\n\n\nGeospatial\nWater point related data on WPdx standard\nWaterpoint access data\n\n\n\n\nDeep Dive into Geospatial Analysis\nLet us try to understand the dynamics of spatial patterns of non-functional water points in Nigeria and its diffusion over spatial boundaries using appropriate global and local measures of spatial association techniques.\n\nLoading packages\nLet us first load required packages into R environment. p_load function pf pacman package is used to install and load sf and tidyverse pacagkes into R environment.\n\n\nPackages\npacman::p_load(sf, tidyverse, tmap, spdep, patchwork, ggthemes)\n\n\n\n\nImporting Geospatial Data\nNow let us import both the geospatial data. The code chunk below uses st_read() function of sf package to import geoBoundaries-NGA-ADM2_simplified shapefile and geo_export into R environment.\nTwo arguments are used :\n\ndsn - destination : to define the data path\nlayer - to provide the shapefile name\n\n\n\nImporting Data\nwaterpts <- st_read(dsn = \"data2/aspatial\",\n              layer = \"geo_export\",\n              crs = 4326) %>%\n  filter(clean_coun == \"Nigeria\")\nnigeria <- st_read(dsn = \"data2/geospatial\",\n               layer = \"geoBoundaries-NGA-ADM2\",\n               crs = 4326)\n\n\n\nst_read() of sf package is used to import geo_export and geoBoundaries-NGA-ADM2_simplified shapefile into R environment and save the imported geospatial data into simple feature data table.\nfilter() of dplyr package is used to extract water point records of Nigeria.\n\nIn order to reduce the file size let us save the data in .rds format.\n\n\nSaving the file\nwrite_rds(waterpts, \"data2/wp_nga.rds\")\n\n\nwrite_rds() of readr package is used to save the extracted sf data table into an output file in rds data format.\n\n\nData Wrangling\nLet us now preprocess the data before performing any analysis\n\n\nReplacing NA values into Unknown\nHere, we are recoding the NA values into Unknown. In the code chunk below, replace_na() is used to recode all the NA values in status_cle field into Unknown.\n\n\nReplacing NA\nwp_nga <- read_rds(\"data2/wp_nga.rds\") %>%\n  mutate(status_cle = replace_na(status_cle, \"Unknown\"))\n\n\n\n\nExtracting Funtional, Non-Functional and Unknown water points\nAs our objective is to focus on waterpoints, let us extract the three types and save it as a dataframe for further analysis\n\n\nExtracting waterpoints\nwpt_functional <- wp_nga %>%\n  filter(status_cle %in%\n           c(\"Functional\", \n             \"Functional but not in use\",\n             \"Functional but needs repair\"))\n\nwpt_nonfunctional <- wp_nga %>%\n  filter(status_cle %in%\n           c(\"Abandoned/Decommissioned\", \n             \"Abandoned\",\n             \"Non-Functional\",\n             \"Non functional due to dry season\",\n             \"Non-Functional due to dry season\"))\n\nwpt_unknown <- wp_nga %>%\n  filter(status_cle == \"Unknown\")\n\n\nIn the code chunk above, filter() of dplyr is used to select the specific water points.\n\n\nComputing Number of Waterpoints in each Second-level Administrative Division\nWe have to perform 2 steps to calculate the total number of functional, non-functional and Unknown waterpoints in each division.\n\nLet us identify no. of waterpoints located inside each division by using st_intersects().\nNext, let us calculate numbers of pre-schools that fall inside each planning subzone by using length() function.\n\n\n\nComputing numbers\nnga_wp <- nigeria %>% \n  mutate(`total wpt` = lengths(\n    st_intersects(nigeria, wp_nga))) %>%\n  mutate(`wpt functional` = lengths(\n    st_intersects(nigeria, wpt_functional))) %>%\n  mutate(`wpt non-functional` = lengths(\n    st_intersects(nigeria, wpt_nonfunctional))) %>%\n  mutate(`wpt unknown` = lengths(\n    st_intersects(nigeria, wpt_unknown)))\n\n\n\n\nComputing Proportion of Functional and Non-Functional water points\nNow, let us calculate what is the overall proportion of functional and non-functional waterpoints by dividing the no. of functional waterpoints by the total no. of waterpoints. Similarly, for non-functional waterpoint proportion, numerator is replaced by non-functional waterpoint.\n\n\nComputing proportion\nnga_wp <- nga_wp %>%\n  mutate(pct_functional = `wpt functional`/`total wpt`) %>%\n  mutate(`pct_non-functional` = `wpt non-functional`/`total wpt`)\n\n\n\n\nSaving the final rds file\nIn order to manage the storage data efficiently, we are saving the final data frame in rds format.\n\n\nSaving rds file\nwrite_rds(nga_wp, \"data2/nga_wp.rds\")\nwrite_rds(wpt_functional, \"data2/wpt_functional.rds\")\nwrite_rds(wpt_nonfunctional, \"data2/wpt_nonfunctional.rds\")\n\n\n\n\nExploratory Data Analysis\nBefore performing spatial analysis, let us first do some preliminary data analysis to understand the data better in terms of water points.\n\n\nWhat is the proportion of functional and non-functional water points?\nBefore visualising, its important for us to prepare the data. Based on WPDx Data Standard, the variable ‘#status_id’ refers to Presence of Water when Assessed. Binary response, i.e. Yes/No are recoded into Functional / Not-Functional.\n\n\nPreparing the data\nnga_sf <- read_rds(\"data2/nga_sf.rds\")\nngawater_sf <- read_rds(\"data2/ngawater_sf.rds\")\nngawater_sf<-ngawater_sf %>%\n  mutate(`#status_id`=\n                case_when(`#status_id`==\"Yes\"~\"Functional\",\n                          `#status_id`==\"No\"~\"Non-Functional\",\n                          `#status_id`== \"Unknown\"~\"unknown\"))\n\n\n\n\nProportion graph\nggplot(data= ngawater_sf, \n       aes(x= `#status_id`)) +\n       geom_bar(fill= '#CD5C5C') +\n       #ylim(0, 150) +\n       geom_text(stat = 'count',\n           aes(label= paste0(stat(count), ', ', \n                             round(stat(count)/sum(stat(count))*100, \n                             1), '%')), vjust= -0.5, size= 2.5) +\n       labs(y= 'No. of \\nwater points',\n            x= 'Water Points',\n            title = \"Distribution of water points\") +\n       theme(axis.title.y= element_text(angle=0), \n             axis.ticks.x= element_blank(),\n             panel.background= element_blank(),\n             axis.line= element_line(color= 'grey'),\n             axis.title.y.left = element_text(vjust = 0.5),\n             plot.title = element_text(hjust=0.5))\n\n\n\n\n\nInsights:\nNigeria consists of almost 55% of functional, 34% of non-functional and 11% of unknown waterpoints.\n\n\nWhat is the district wise proportion of water points?\nFirst let us prepare the data\n\nFilter all the NA values in waterpoint_status\nGroup by shape name and waterpoint status\nCompute the count and proposition by dividing the count by total number of waterpoints in that division\nSelecting top 10 rows with more no. of waterpoints\n\n\n\nPreparing data\nnga_sf <- nga_sf %>% filter(!is.na(waterpoint_status)) \ndf <- nga_sf %>% \n  group_by(shapeName,waterpoint_status) %>% \n  tally() %>%\n  group_by(shapeName) %>%\n  mutate(total=sum(n),\n         prop=round(n*100/total)) %>%\n  arrange(desc(total))\ntop_10 <- head(df,10)\n\n\n\n\nDistrictwise Proportion\np3 <- ggplot(data=top_10,\n             aes(x=shapeName,\n                 y=prop,\n                 fill=waterpoint_status))+\n  geom_col()+\n  geom_text(aes(label=paste0(prop,\"%\")),\n            position = position_stack(vjust=0.5),size=3)+\n  theme(axis.text.x=element_text(angle=0))+\n    xlab(\"Division\")+\n    ylab(\"% of \\n Waterpoints\")+\n    ggtitle(\"Proportion of waterpoints by District\")+\n    theme_bw()+\n    guides(fill=guide_legend(title=\"Waterpoint\"),\n           shape=guide_legend(override.aes = list(size=0.5)))+\n    theme(plot.title = element_text(hjust=0.5, size=13),\n          legend.title = element_text(size=9),\n          legend.text = element_text(size=7),\n          axis.text = element_text(face=\"bold\"),\n          axis.ticks.x=element_blank(),\n          axis.title.y=element_text(angle=0),\n          axis.title.y.left = element_text(vjust = 0.5))\np3\n\n\n\n\n\nInsights:\nAmong the divisions which have most no. of waterpoints, in Ifelodun division, almost 50% of waterpoints are non-functional. It is a matter of concern.\n\n\nWhich district has most no. of non-functional waterpoints?\nTo find out the solution for this question, first let’s prepare the data accordingly:\n\nFilter non-functional waterpoints\nArrange it in descending order by the count values\nSelect top 10 divisions\n\n\n\nPreparing data\nnonfunc_top10 <- df %>%\n  filter(waterpoint_status == \"Non-Functional\") %>%\n  arrange(desc(n)) \nnonfunc_top10 <- head(nonfunc_top10, 10)\n\n\n\n\nTop 10 divisions\nggplot(data = nonfunc_top10,\n       aes(y = reorder(shapeName, n),\n           x=n)) + \n  geom_bar(stat = \"identity\",\n           fill = \"coral\")+\n  labs(y= 'Division',\n       x='No. of Non-Functional water points',\n       title=\"Top 10 divisions by Non-Functional waterpoints\",) +\n  geom_text(stat='identity', aes(label=paste0(n)),hjust=-0.5)+\n  theme(axis.title.y=element_text(angle=0), \n        axis.ticks.x=element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_line(color='grey'),\n        plot.title = element_text(hjust = 0.5),\n        axis.title.y.left = element_text(vjust = 0.5), \n        axis.text = element_text(face=\"bold\") )\n\n\n\n\n\nInsights:\nAmong all 774 administrative level 2 divisions, Ifelodun has most no. of non-functional water points followed by Igabi, Irepodun, Oyun and Sabon-Gari.\n\n\nWhat are the causes of waterpoints to be non-functional?\nAs our objective is to focus more on non-functional waterpoints, let us try to understand what are the major causes for a water point to become non-functional and which contributes the most?\nTo find out the solution for this question, first let’s prepare the data accordingly:\n\nImport the data which is already filtered by non-functional waterpoints.\nSimilar causes are recoded to avoid redundancy\n\n\n\nPreparing data\nwpt_nonfunctional <- read_rds(\"data2/wpt_nonfunctional.rds\")\nwpt_nonfunctional <- wpt_nonfunctional %>%\n  mutate(status_cle=recode(status_cle, \n                     'Non-Functional due to dry season'='Dry Season',\n                     'Non functional due to dry season'='Dry Season',\n                     'Abandoned/Decommissioned' = 'Abandoned / Decommissioned',\n                     'Abandoned' = 'Abandoned / Decommissioned'))\nnonfun_order <- factor(wpt_nonfunctional$status_cle, level = c('Non-Functional', 'Dry Season','Abandoned / Decommissioned'))\n\n\n\n\nNon-functional types\nggplot(data= wpt_nonfunctional, \n       aes(x= nonfun_order)) +\n       geom_bar(fill= 'plum') +\n       #ylim(0, 150) +\n       geom_text(stat = 'count',\n           aes(label= paste0(stat(count), ', ', \n                             round(stat(count)/sum(stat(count))*100, \n                             1), '%')), vjust= -0.5, size= 2.5) +\n       labs(x= 'Reasons',\n            y= 'No. of \\nwater points',\n            title = \"What are the causes of non-functional water points?\") +\n       theme(axis.title.y= element_text(angle=0), \n             axis.ticks.x= element_blank(),\n             panel.background= element_blank(),\n             axis.line= element_line(color= 'grey'),\n             axis.title.y.left = element_text(vjust = 0.5),\n             plot.title = element_text(hjust=0.5))\n\n\n\n\n\nInsights:\nAlmost 90% of non-functional waterpoints are not reason specific. Some other reasons include dry season , waterpoints are decommissioned or some are left without being taken care of."
  },
  {
    "objectID": "InClass_Ex/InClass_Ex3.html",
    "href": "InClass_Ex/InClass_Ex3.html",
    "title": "Spatially constrained hierarchical clustering",
    "section": "",
    "text": "2.Data\nFollowing two data sets are used:\n\nMyanmar Township Boundary Data (i.e. myanmar_township_boundaries) : This is a GIS data in ESRI shapefile format. It consists of township boundary information of Myanmar. The spatial data are captured in polygon features.\nShan-ICT.csv: This is an extract of The 2014 Myanmar Population and Housing Census Myanmar at the township level.\n\np_load function pf pacman package is used to install and load sf all necessary packages into R environment.\n\nsf, rgdal and spdep - Spatial data handling\ntidyverse, especially readr, ggplot2 and dplyr - Attribute data handling\ntmap -Choropleth mapping\ncoorplot, ggpubr, and heatmaply - Multivariate data visualisation and analysis\ncluster, ClustGeo - Cluster analysis\n\nThe code chunk below installs and launches these R packages into R environment.\n\n\nLoading packages\npacman::p_load(rgdal, spdep, tmap, sf, \n               ggpubr, cluster, factoextra, NbClust,\n               heatmaply, corrplot, psych, tidyverse, purrr,ClustGeo)\n\n\n\n\n3. Geospatial Analysis\n\n3.1 Importing data\nThe Myanmar Township Boundary GIS data is in ESRI shapefile format. It is imported into R environment by using the st_read() function of sf. The imported InfoComm variables are extracted from The 2014 Myanmar Population and Housing Census Myanmar. The attribute data set is called ict. It is saved in R’s tibble data.frame format.\n\n\nImporting data\nshan_sf <- st_read(dsn = \"data3/geospatial\", \n                   layer = \"myanmar_township_boundaries\") %>%\n  filter(ST %in% c(\"Shan (East)\", \"Shan (North)\", \"Shan (South)\"))\n\n\nReading layer `myanmar_township_boundaries' from data source \n  `D:\\raveenaclr\\Geospatial Analytics\\InClass_Ex\\data3\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 330 features and 14 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 92.17275 ymin: 9.671252 xmax: 101.1699 ymax: 28.54554\nGeodetic CRS:  WGS 84\n\n\nImporting data\nict <- read_csv (\"data3/aspatial/Shan-ICT.csv\")\n\n\n\n\n5.2 Data Wrangling\nIt is wiser to take proportion of no. of households present instead of using the numbers directly. So, let us preprocess the data accordingly by using the code chunk below:\n\n\nPreprocessing data\nict_derived <- ict %>%\n  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %>%\n  mutate(`TV_PR` = `Television`/`Total households`*1000) %>%\n  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %>%\n  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %>%\n  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %>%\n  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %>%\n  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,\n         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,\n         `TT_HOUSEHOLDS`=`Total households`,\n         `RADIO`=`Radio`, `TV`=`Television`, \n         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,\n         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) \n\n\n\n\n5.3 Visualising the distribution\n\n\nHistogram for all ICT medium\nradio <- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.2)\n\ntv <- ggplot(data=ict_derived, \n             aes(x= `TV_PR`,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"red\",\n               alpha = 0.2)\n\nllphone <- ggplot(data=ict_derived, \n             aes(x= `LLPHONE_PR`,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n    geom_density(color=\"black\",\n               alpha = 0.2)\n\nmphone <- ggplot(data=ict_derived, \n             aes(x= `MPHONE_PR`,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.2)\n\ncomputer <- ggplot(data=ict_derived, \n             aes(x= `COMPUTER_PR`,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.2)\n\ninternet <- ggplot(data=ict_derived, \n             aes(x= `INTERNET_PR`,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.)\n\nggarrange(radio, tv, llphone, mphone, computer, internet, \n          ncol = 3, \n          nrow = 2)\n\n\n\n\n\n\n\n5.4 Correlation Analysis\nBefore we perform cluster analysis, it is important for us to ensure that the cluster variables are not highly correlated.\nIn this section, you will learn how to use corrplot.mixed() function of corrplot package to visualise and analyse the correlation of the input variables.\n\n\nCorrelation\ncluster_vars.cor = cor(ict_derived[,12:17])\ncorrplot.mixed(cluster_vars.cor,\n         lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\nBefore we visualise the maps using choropleth, let us first combine both geospatial and aspatial datsets into one simple feature dataframe using the code chunk below.\n\n\nCombining geospatial and aspatial data\nshan_sf <- left_join(shan_sf, \n                     ict_derived, \n                     by=c(\"TS_PCODE\"=\"TS_PCODE\"))\n\n\n\n\n5.5 Hierarchy Cluster Analysis\nNow let us perform hierarchical cluster analysis. The analysis consists of four major steps:\n\nExtracting clustering variables\nData standardisation\nVisualising the clustering variables\nComputing Proximity Matrix\n\nExtracting clustering variables\n\n\nClustering\ncluster_vars <- shan_sf %>%\n  st_set_geometry(NULL) %>%\n  select(\"TS.x\", \"RADIO_PR\", \"TV_PR\", \"LLPHONE_PR\", \"MPHONE_PR\", \"COMPUTER_PR\")\nhead(cluster_vars,10)\n\n\n        TS.x RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\n1    Mongmit 286.1852 554.1313   35.30618  260.6944    12.15939\n2    Pindaya 417.4647 505.1300   19.83584  162.3917    12.88190\n3    Ywangan 484.5215 260.5734   11.93591  120.2856     4.41465\n4   Pinlaung 231.6499 541.7189   28.54454  249.4903    13.76255\n5     Mabein 449.4903 708.6423   72.75255  392.6089    16.45042\n6      Kalaw 280.7624 611.6204   42.06478  408.7951    29.63160\n7      Pekon 318.6118 535.8494   39.83270  214.8476    18.97032\n8   Lawksawk 387.1017 630.0035   31.51366  320.5686    21.76677\n9  Nawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\n10   Kyaukme 210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nNow let us rename the township name instead of row number and delete the old one using the code chunk below\n\n\nRenaming column\nrow.names(cluster_vars) <- cluster_vars$\"TS.x\"\nshan_ict <- select(cluster_vars, c(2:6))\nhead(shan_ict, 10)\n\n\n          RADIO_PR    TV_PR LLPHONE_PR MPHONE_PR COMPUTER_PR\nMongmit   286.1852 554.1313   35.30618  260.6944    12.15939\nPindaya   417.4647 505.1300   19.83584  162.3917    12.88190\nYwangan   484.5215 260.5734   11.93591  120.2856     4.41465\nPinlaung  231.6499 541.7189   28.54454  249.4903    13.76255\nMabein    449.4903 708.6423   72.75255  392.6089    16.45042\nKalaw     280.7624 611.6204   42.06478  408.7951    29.63160\nPekon     318.6118 535.8494   39.83270  214.8476    18.97032\nLawksawk  387.1017 630.0035   31.51366  320.5686    21.76677\nNawnghkio 349.3359 547.9456   38.44960  323.0201    15.76465\nKyaukme   210.9548 601.1773   39.58267  372.4930    30.94709\n\n\nData Standardisation\nIn order to avoid the cluster analysis result is baised to clustering variables with large values, it is useful to standardise the input variables before performing cluster analysis.\nPerforming Min-Max standardisation\nIn the code chunk below, normalize() of heatmaply package is used to stadardisation the clustering variables by using Min-Max method. The summary() is then used to display the summary statistics of the standardised clustering variables.\n\n\nMin-Max standardisation\nshan_ict.std <- normalize(shan_ict)\nsummary(shan_ict.std)\n\n\n    RADIO_PR          TV_PR          LLPHONE_PR       MPHONE_PR     \n Min.   :0.0000   Min.   :0.0000   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:0.2544   1st Qu.:0.4600   1st Qu.:0.1123   1st Qu.:0.2199  \n Median :0.4097   Median :0.5523   Median :0.1948   Median :0.3846  \n Mean   :0.4199   Mean   :0.5416   Mean   :0.2703   Mean   :0.3972  \n 3rd Qu.:0.5330   3rd Qu.:0.6750   3rd Qu.:0.3746   3rd Qu.:0.5608  \n Max.   :1.0000   Max.   :1.0000   Max.   :1.0000   Max.   :1.0000  \n  COMPUTER_PR     \n Min.   :0.00000  \n 1st Qu.:0.09598  \n Median :0.17607  \n Mean   :0.23692  \n 3rd Qu.:0.29868  \n Max.   :1.00000  \n\n\nPerforming Z-score standardisation\nZ-score standardisation can be performed easily by using scale() of Base R. The code chunk below will be used to stadardisation the clustering variables by using Z-score method.\n\n\nZ standardisation\nshan_ict.z <- scale(shan_ict)\ndescribe(shan_ict.z)\n\n\n            vars  n mean sd median trimmed  mad   min  max range  skew kurtosis\nRADIO_PR       1 55    0  1  -0.04   -0.06 0.94 -1.85 2.55  4.40  0.48    -0.27\nTV_PR          2 55    0  1   0.05    0.04 0.78 -2.47 2.09  4.56 -0.38    -0.23\nLLPHONE_PR     3 55    0  1  -0.33   -0.15 0.68 -1.19 3.20  4.39  1.37     1.49\nMPHONE_PR      4 55    0  1  -0.05   -0.06 1.01 -1.58 2.40  3.98  0.48    -0.34\nCOMPUTER_PR    5 55    0  1  -0.26   -0.18 0.64 -1.03 3.31  4.34  1.80     2.96\n              se\nRADIO_PR    0.13\nTV_PR       0.13\nLLPHONE_PR  0.13\nMPHONE_PR   0.13\nCOMPUTER_PR 0.13\n\n\nWe observe that mean and standard deviation of the Z-score standardised clustering variables are 0 and 1 respectively. We should take note that Z-score standardisation method should only be used if we would assume all variables come from some normal distribution.\nVisualising the standardised clustering variables\nLet us visualise the standardised clustering variables graphically in addition to summary statistics\n\n\nVisualising clustering variables\nr <- ggplot(data=ict_derived, \n             aes(x= `RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\")+\n  ggtitle(\"Before Standardisation\")\n\nshan_ict_s_df <- as.data.frame(shan_ict.std)\ns <- ggplot(data=shan_ict_s_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Min-Max Standardisation\")\n\nshan_ict_z_df <- as.data.frame(shan_ict.z)\nz <- ggplot(data=shan_ict_z_df, \n       aes(x=`RADIO_PR`)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  ggtitle(\"Z-score Standardisation\")\n\nggarrange(r, s, z,\n          ncol = 3,\n          nrow = 1)\n\n\n\n\n\nWe can observe that after standardisation, the variables follow normal distribution.\nComputing Proximity Matrix The code chunk below is used to compute the proximity matrix using euclidean method although the function dist() supports six distance proximity calculations, they are: euclidean, maximum, manhattan, canberra, binary and minkowski.\n\n\nProximity matrix\nproxmat <- dist(shan_ict, method = 'euclidean')\n\n\n\n\n5.6 Spatially Constrained Clustering: ClustGeo Method\n\n\n5.6.1 Ward-like hierarchical clustering: ClustGeo\nTo perform non-spatially constrained hierarchical clustering, we only need to provide the function a dissimilarity matrix as shown in the code chunk below.\n\nnongeo_cluster <- hclustgeo(proxmat)\nplot(nongeo_cluster, cex = 0.5)\nrect.hclust(nongeo_cluster, \n            k = 6, \n            border = 2:5)\n\n\n\n\n\n\n5.6.2 Mapping the clusters formed\nSimilarly, we can plot the clusters on a categorical area shaded map by using the steps we learned in 5.7.12 Mapping the clusters formed.\n\ngroups <- as.factor(cutree(nongeo_cluster, k=6))\nshan_sf_ngeo_cluster <- cbind(shan_sf, as.matrix(groups)) %>%\n  rename(`CLUSTER` = `as.matrix.groups.`)\nqtm(shan_sf_ngeo_cluster, \"CLUSTER\")\n\n\n\n\n\n\n5.6.3 Spatially Constrained Hierarchical Clustering\nBefore we can performed spatially constrained hierarchical clustering, a spatial distance matrix will be derived by using st_distance() of sf package.\n\ndist <- st_distance(shan_sf, shan_sf)\ndistmat <- as.dist(dist)\ncr <- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)\n\n\n\n\n\n\n\nWith reference to the graphs above, alpha = 0.3 will be used as shown in the code chunk below\n\nclustG <- hclustgeo(proxmat, distmat, alpha = 0.3)\ngroups <- as.factor(cutree(clustG, k=6))\nshan_sf_Gcluster <- cbind(shan_sf, as.matrix(groups)) %>%\n  rename(`CLUSTER` = `as.matrix.groups.`)\nqtm(shan_sf_Gcluster, \"CLUSTER\")"
  },
  {
    "objectID": "InClass_Ex/InClass_Ex4.html",
    "href": "InClass_Ex/InClass_Ex4.html",
    "title": "Inclass Ex - Calibrating Hedonic Pricing Model for Private Highrise Property with GWR Method",
    "section": "",
    "text": "2. Glimpse of Steps\nSome of the important steps performed in this study are as follows\n\nimporting geospatial data using appropriate function(s) of sf package,\nimporting csv file using appropriate function of readr package,\nconverting aspatial dataframe into sf object\nperforming exploratory data analysis\nperforming simple and multiple linear regression techniques\nbuilding a hedonic price model using GWR method\n\n\n\n3. Data\nFollowing two data sets are used:\n\nURA Master Plan subzone boundary in shapefile format (i.e. MP14_SUBZONE_WEB_PL)\ncondo_resale_2015 in csv format (i.e. condo_resale_2015.csv)\n\n\n\n4.Deep Dive into Map Analysis\n\n4.1 Installing libraries and Importing files\np_load function pf pacman package is used to install and load sf all necessary packages into R environment.\n\nsf, rgdal and spdep - Spatial data handling\ntidyverse, especially readr, ggplot2 and dplyr - Attribute data handling\ntmap -Choropleth mapping\nolsrr - Ordinary Least Square(OLS) method and performing diagnostics tests\nGWmodel - geographical weighted family of models\ncorrplot - multivariate data visualisation and analysis\npatchwork- It has a very simple syntax where we can create layouts super easily. The general syntax that combines:\nTwo-Column Layout using the Plus Sign +\nParenthesis () to create a subplot group\nTwo-Row Layout using the Division Sign \\\nggthemes- package to use various themes which control all non-display data\n\nThe code chunk below installs and launches these R packages into R environment.\n\n\nLoading packages\npacman::p_load(olsrr, corrplot, ggpubr, \n               sf,spdep, GWmodel, tmap, \n               tidyverse, gtsummary,patchwork, ggthemes)\n\n\nThe code chunk below is used to import MP_SUBZONE_WEB_PL shapefile by using st_read() of sf packages and uses read_csv() function of readr package to import condo_resale_2015 into R as a tibble data frame called condo_resale.\n\n\nImporting data\nmpsz = st_read(dsn = \"data4/geospatial\", \n               layer = \"MP14_SUBZONE_WEB_PL\")\n\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `D:\\raveenaclr\\Geospatial Analytics\\InClass_Ex\\data4\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\n\nImporting data\ncondo_resale = read_csv(\"data4/aspatial/Condo_resale_2015.csv\")\n\n\n\n\n4.2 Data Wrangling\nThe code chunk below updates the newly imported mpsz with the correct ESPG code (i.e. 3414)\n\n\nAssigning correct projection\nmpsz_svy21 <- st_transform(mpsz, 3414)\n\n\nCurrently, the condo_resale tibble data frame is aspatial. We will convert it to a sf object. The code chunk below converts condo_resale data frame into a simple feature data frame by using st_as_sf() of sf packages.\n\n\nconverting projection\ncondo_resale.sf <- st_as_sf(condo_resale,\n                            coords = c(\"LONGITUDE\", \"LATITUDE\"),\n                            crs=4326) %>%\n  st_transform(crs=3414)\n\n\n\n\n4.3 Exploratory Data Analysis (EDA)\nWhat is the distribution of Condo selling price?\n\n\nHistogram-Selling Price\nh1 <- ggplot(data=condo_resale.sf, \n              aes(x=`SELLING_PRICE`,\n                  y= ..density..)) +\n      geom_histogram(bins=20, \n                     color=\"black\", \n                     fill=\"coral\")+\n      geom_density(color=\"black\",\n                   alpha=0.5) +\n      theme(panel.background= element_blank())\n\n\nThe figure above reveals a right skewed distribution. This means that more condominium units were transacted at relative lower prices. Tghis can be normalised by using log transformation.\nThe code chunk below is used to derive a new variable called LOG_SELLING_PRICE by using a log transformation on the variable SELLING_PRICE. It is performed using mutate() of dplyr package.\n\n\nHistogram-Selling Price\ncondo_resale.sf <- condo_resale.sf %>%\n                   mutate(`LOG_SELLING_PRICE` = log(SELLING_PRICE))\n\nh2 <- ggplot(data=condo_resale.sf, \n             aes(x=`LOG_SELLING_PRICE`,\n                 y= ..density..)) +\n      geom_histogram(bins=20, \n                     color=\"black\", \n                     fill=\"coral\")+\n      geom_density(color=\"black\",\n                   alpha=0.5)+\n      theme(panel.background= element_blank())\n\n\nLet us compare the distribution before and after performing log transformation\n\n\nComaprison\nh1 <- h1 + labs(title= \"Raw values\")\nh2 <- h2 + labs(title = \"Log transformation\")\n\nggarrange(h1, h2, ncol=2)\n\n\n\n\n\nNow let us view the distribution for multiple variables\nThe code chunk below is used to multiple histograms. Then, ggarrange() is used to organised these histogram into a 3 columns by 4 rows small multiple plot.\n\n\nMultiple histograms\nAREA_SQM <- ggplot(data=condo_resale.sf, \n                   aes(x= `AREA_SQM`,\n                   y= ..density..)) + \n            geom_histogram(bins=20, \n                           color=\"black\", \n                           fill=\"coral\")+\n            geom_density(color=\"black\",\n                         alpha=0.5)+\n            theme(panel.background= element_blank())\n\nAGE <- ggplot(data=condo_resale.sf, \n              aes(x= `AGE`,\n                  y= ..density..)) +\n       geom_histogram(bins=20, \n                      color=\"black\", \n                      fill=\"coral\")+\n            geom_density(color=\"black\",\n                         alpha=0.5)+\n            theme(panel.background= element_blank())\n\nPROX_CBD <- ggplot(data=condo_resale.sf, \n                   aes(x= `PROX_CBD`,\n                       y= ..density..)) +\n            geom_histogram(bins=20, \n                           color=\"black\", \n                           fill=\"coral\")+\n            geom_density(color=\"black\",\n                         alpha=0.5)+\n            theme(panel.background= element_blank())\n\nPROX_CHILDCARE <- ggplot(data=condo_resale.sf,\n                         aes(x= `PROX_CHILDCARE`,\n                             y= ..density..)) + \n                  geom_histogram(bins=20,\n                                 color=\"black\", \n                                 fill=\"coral\")+\n                  geom_density(color=\"black\",\n                                alpha=0.5)+\n                  theme(panel.background= element_blank())\n\nPROX_ELDERLYCARE <- ggplot(data=condo_resale.sf, \n                           aes(x= `PROX_ELDERLYCARE`,\n                               y= ..density..)) +\n                    geom_histogram(bins=20, \n                                   color=\"black\", \n                                   fill=\"coral\")+\n                    geom_density(color=\"black\",\n                                 alpha=0.5)+\n                    theme(panel.background= element_blank())\n\nPROX_URA_GROWTH_AREA <- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_URA_GROWTH_AREA`,\n                                   y= ..density..)) +\n                        geom_histogram(bins=20, \n                                       color=\"black\", \n                                       fill=\"coral\")+\n                        geom_density(color=\"black\",\n                                     alpha=0.5)+\n                        theme(panel.background= element_blank())\n\nPROX_HAWKER_MARKET <- ggplot(data=condo_resale.sf, \n                             aes(x= `PROX_HAWKER_MARKET`,\n                                 y= ..density..)) +\n                      geom_histogram(bins=20, \n                                     color=\"black\", \n                                     fill=\"coral\")+\n                      geom_density(color=\"black\",\n                                   alpha=0.5)+\n                      theme(panel.background= element_blank())\n\nPROX_KINDERGARTEN <- ggplot(data=condo_resale.sf, \n                            aes(x= `PROX_KINDERGARTEN`,\n                                y= ..density..)) +\n                     geom_histogram(bins=20, \n                                    color=\"black\", \n                                    fill=\"coral\")+\n                     geom_density(color=\"black\",\n                                  alpha=0.5)+\n            theme(panel.background= element_blank())\n\nPROX_MRT <- ggplot(data=condo_resale.sf, \n                   aes(x= `PROX_MRT`,\n                       y= ..density..)) +\n            geom_histogram(bins=20, \n                           color=\"black\", \n                           fill=\"coral\")+\n            geom_density(color=\"black\",\n                         alpha=0.5)+\n            theme(panel.background= element_blank())\n\nPROX_PARK <- ggplot(data=condo_resale.sf, \n                    aes(x= `PROX_PARK`,\n                        y= ..density..)) +\n             geom_histogram(bins=20, \n                            color=\"black\", \n                            fill=\"coral\")+\n             geom_density(color=\"black\",\n                          alpha=0.5)+\n             theme(panel.background= element_blank())\n\nPROX_PRIMARY_SCH <- ggplot(data=condo_resale.sf, \n                           aes(x= `PROX_PRIMARY_SCH`,\n                               y= ..density..)) +\n                    geom_histogram(bins=20, \n                                   color=\"black\", \n                                   fill=\"coral\")+\n                    geom_density(color=\"black\",\n                                 alpha=0.5)+\n                    theme(panel.background= element_blank())\n\nPROX_TOP_PRIMARY_SCH <- ggplot(data=condo_resale.sf, \n                               aes(x= `PROX_TOP_PRIMARY_SCH`,\n                                   y= ..density..)) +\n                        geom_histogram(bins=20, \n                                       color=\"black\", \n                                       fill=\"coral\")+\n                        geom_density(color=\"black\",\n                                     alpha=0.5)+\n                        theme(panel.background= element_blank())\n\nggarrange(AREA_SQM, AGE, PROX_CBD, PROX_CHILDCARE, PROX_ELDERLYCARE, \n          PROX_URA_GROWTH_AREA, PROX_HAWKER_MARKET, PROX_KINDERGARTEN, PROX_MRT,\n          PROX_PARK, PROX_PRIMARY_SCH, PROX_TOP_PRIMARY_SCH,  \n          ncol = 3, nrow = 4)\n\n\n\n\n\nWhat is the geospatial distribution of condo prices in Singapore?\nThe code chunks below is used to create an interactive point symbol map.\n\n\nGeospatial distribution\ntm_shape(mpsz_svy21)+\n  tm_polygons() +\ntm_shape(condo_resale.sf) +  \n  tm_dots(col = \"SELLING_PRICE\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tmap_options(check.and.fix = TRUE)+\n  tmap_mode(\"view\")+\n  tm_view(set.zoom.limits = c(11,14))\n\n\ntmap mode set to interactive viewing\n\n\nWarning: The shape mpsz_svy21 is invalid (after reprojection). See\nsf::st_is_valid\n\n\n\n\n\n\n\n\n\n4.4 Hedonic Pricing Modelling in R\n\n\n4.4.1 Simple Linear Regression Method\nFirst, let us build a simple linear regression model by using SELLING_PRICE as the dependent variable and AREA_SQM as the independent variable.\n\n\nSimple Linear Regression\ncondo.slr <- lm(formula=SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\nsummary(condo.slr)\n\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM, data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3695815  -391764   -87517   258900 13503875 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -258121.1    63517.2  -4.064 5.09e-05 ***\nAREA_SQM      14719.0      428.1  34.381  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 942700 on 1434 degrees of freedom\nMultiple R-squared:  0.4518,    Adjusted R-squared:  0.4515 \nF-statistic:  1182 on 1 and 1434 DF,  p-value: < 2.2e-16\n\n\nInterpretation\nR-squared of 0.4518 reveals that the simple regression model built is able to explain about 45% of the resale prices.\nH0 (Null Hypothesis) - mean price is a good estimator of SELLING_PRICE\nH1 (Alternative Hypothesis) - mean price is not a good estimator of SELLING_PRICE\n\nSince p-value is much smaller than 0.0001, we will reject the null hypothesis that mean is a good estimator of SELLING_PRICE.\nThis will allow us to infer that simple linear regression model above is a good estimator of SELLING_PRICE.\np-values of both the estimates of the Intercept and ARA_SQM are smaller than 0.001. In view of this, the null hypothesis of the B0 and B1 are equal to 0 will be rejected and so B0 and B1 are good parameter estimates.\n\nLet us visualise the best fit curve on a scatterplot, using lm() as a method function in ggplot’s geometry as shown in the code chunk below.\n\n\nGoodness of fit\nggplot(data=condo_resale.sf,  \n       aes(x=`AREA_SQM`, y=`SELLING_PRICE`)) +\n  geom_point() +\n  geom_smooth(method = lm)+\n  theme(panel.background= element_blank())\n\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nWe can see that there are a few statistical outliers with relatively high selling prices.\n\n\n4.4.2 Multiple Linear Regression Method\nLet us check if there is a multicollinearity phenomenon by executing correlation analysis. It is important to ensure that the indepdent variables used are not highly correlated to each other.\nThe code chunk below is used to plot a scatterplot matrix of the relationship between the independent variables in condo_resale data.frame.\n\n\nCorrelation Analysis\ncorrplot(cor(condo_resale[, 5:23]), \n         diag = FALSE, \n         order = \"AOE\",\n         tl.pos = \"td\", \n         tl.cex = 0.5,\n         method = \"number\", \n         type = \"upper\")\n\n\n\n\n\nIt is clear that Freehold is highly correlated to LEASE_99YEAR. Hence, let us include either one of them i.e. LEASE_99YEAR in the subsequent model building.\n\n\n4.4.3 Hedonic pricing model using multiple linear regression method\nThe code chunk below using lm() to calibrate the multiple linear regression model.\n\n\nMultiple Linear Regression\ncondo.mlr <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE    + \n                  PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                  PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + PROX_KINDERGARTEN + \n                  PROX_MRT  + PROX_PARK + PROX_PRIMARY_SCH + \n                  PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n                  PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                data=condo_resale.sf)\nsummary(condo.mlr)\n\n\n\nCall:\nlm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + PROX_CHILDCARE + \n    PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_HAWKER_MARKET + \n    PROX_KINDERGARTEN + PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + \n    PROX_TOP_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_SUPERMARKET + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3475964  -293923   -23069   241043 12260381 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)           481728.40  121441.01   3.967 7.65e-05 ***\nAREA_SQM               12708.32     369.59  34.385  < 2e-16 ***\nAGE                   -24440.82    2763.16  -8.845  < 2e-16 ***\nPROX_CBD              -78669.78    6768.97 -11.622  < 2e-16 ***\nPROX_CHILDCARE       -351617.91  109467.25  -3.212  0.00135 ** \nPROX_ELDERLYCARE      171029.42   42110.51   4.061 5.14e-05 ***\nPROX_URA_GROWTH_AREA   38474.53   12523.57   3.072  0.00217 ** \nPROX_HAWKER_MARKET     23746.10   29299.76   0.810  0.41782    \nPROX_KINDERGARTEN     147468.99   82668.87   1.784  0.07466 .  \nPROX_MRT             -314599.68   57947.44  -5.429 6.66e-08 ***\nPROX_PARK             563280.50   66551.68   8.464  < 2e-16 ***\nPROX_PRIMARY_SCH      180186.08   65237.95   2.762  0.00582 ** \nPROX_TOP_PRIMARY_SCH    2280.04   20410.43   0.112  0.91107    \nPROX_SHOPPING_MALL   -206604.06   42840.60  -4.823 1.57e-06 ***\nPROX_SUPERMARKET      -44991.80   77082.64  -0.584  0.55953    \nPROX_BUS_STOP         683121.35  138353.28   4.938 8.85e-07 ***\nNO_Of_UNITS             -231.18      89.03  -2.597  0.00951 ** \nFAMILY_FRIENDLY       140340.77   47020.55   2.985  0.00289 ** \nFREEHOLD              359913.01   49220.22   7.312 4.38e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 755800 on 1417 degrees of freedom\nMultiple R-squared:  0.6518,    Adjusted R-squared:  0.6474 \nF-statistic: 147.4 on 18 and 1417 DF,  p-value: < 2.2e-16\n\n\nAt 99% confidence interval, almost all the varibles are statistically significant except PROX_HAWKER_MARKET, PROX_KINDERGARTEN , PROX_TOP_PRIMARY_SCH.\n\n\n4.4.4 Publication Quality Table: olsrr method\nIt is clear that not all the independent variables are statistically significant. Let us revise the model by removing those variables which are not statistically significant.\n\n\nRevised model\ncondo.mlr1 <- lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                   PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE +\n                   PROX_URA_GROWTH_AREA + PROX_MRT  + PROX_PARK + \n                   PROX_PRIMARY_SCH + PROX_SHOPPING_MALL    + PROX_BUS_STOP + \n                   NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD,\n                 data=condo_resale.sf)\nols_regress(condo.mlr1)\n\n\n                             Model Summary                               \n------------------------------------------------------------------------\nR                       0.807       RMSE                     755957.289 \nR-Squared               0.651       Coef. Var                    43.168 \nAdj. R-Squared          0.647       MSE                571471422208.591 \nPred R-Squared          0.638       MAE                      414819.628 \n------------------------------------------------------------------------\n RMSE: Root Mean Square Error \n MSE: Mean Square Error \n MAE: Mean Absolute Error \n\n                                     ANOVA                                       \n--------------------------------------------------------------------------------\n                    Sum of                                                      \n                   Squares          DF         Mean Square       F         Sig. \n--------------------------------------------------------------------------------\nRegression    1.512586e+15          14        1.080418e+14    189.059    0.0000 \nResidual      8.120609e+14        1421    571471422208.591                      \nTotal         2.324647e+15        1435                                          \n--------------------------------------------------------------------------------\n\n                                               Parameter Estimates                                                \n-----------------------------------------------------------------------------------------------------------------\n               model           Beta    Std. Error    Std. Beta       t        Sig           lower          upper \n-----------------------------------------------------------------------------------------------------------------\n         (Intercept)     527633.222    108183.223                   4.877    0.000     315417.244     739849.200 \n            AREA_SQM      12777.523       367.479        0.584     34.771    0.000      12056.663      13498.382 \n                 AGE     -24687.739      2754.845       -0.167     -8.962    0.000     -30091.739     -19283.740 \n            PROX_CBD     -77131.323      5763.125       -0.263    -13.384    0.000     -88436.469     -65826.176 \n      PROX_CHILDCARE    -318472.751    107959.512       -0.084     -2.950    0.003    -530249.889    -106695.613 \n    PROX_ELDERLYCARE     185575.623     39901.864        0.090      4.651    0.000     107302.737     263848.510 \nPROX_URA_GROWTH_AREA      39163.254     11754.829        0.060      3.332    0.001      16104.571      62221.936 \n            PROX_MRT    -294745.107     56916.367       -0.112     -5.179    0.000    -406394.234    -183095.980 \n           PROX_PARK     570504.807     65507.029        0.150      8.709    0.000     442003.938     699005.677 \n    PROX_PRIMARY_SCH     159856.136     60234.599        0.062      2.654    0.008      41697.849     278014.424 \n  PROX_SHOPPING_MALL    -220947.251     36561.832       -0.115     -6.043    0.000    -292668.213    -149226.288 \n       PROX_BUS_STOP     682482.221    134513.243        0.134      5.074    0.000     418616.359     946348.082 \n         NO_Of_UNITS       -245.480        87.947       -0.053     -2.791    0.005       -418.000        -72.961 \n     FAMILY_FRIENDLY     146307.576     46893.021        0.057      3.120    0.002      54320.593     238294.560 \n            FREEHOLD     350599.812     48506.485        0.136      7.228    0.000     255447.802     445751.821 \n-----------------------------------------------------------------------------------------------------------------\n\n\nNow, we have only statistically significant variables.\n\n\n4.4.5 Publication Quality Table: gtsummary method\nIn the code chunk below, tbl_regression() is used to create a well formatted regression report using gtsummary package that provides an elegant and flexible way to create publication-ready summary tables in R.\n\n\ngtsummary\ntbl_regression(condo.mlr1, intercept = TRUE)\n\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n527,633\n315,417, 739,849\n<0.001\n    AREA_SQM\n12,778\n12,057, 13,498\n<0.001\n    AGE\n-24,688\n-30,092, -19,284\n<0.001\n    PROX_CBD\n-77,131\n-88,436, -65,826\n<0.001\n    PROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n    PROX_ELDERLYCARE\n185,576\n107,303, 263,849\n<0.001\n    PROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n<0.001\n    PROX_MRT\n-294,745\n-406,394, -183,096\n<0.001\n    PROX_PARK\n570,505\n442,004, 699,006\n<0.001\n    PROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n    PROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n<0.001\n    PROX_BUS_STOP\n682,482\n418,616, 946,348\n<0.001\n    NO_Of_UNITS\n-245\n-418, -73\n0.005\n    FAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n    FREEHOLD\n350,600\n255,448, 445,752\n<0.001\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\nWith this, model statistics can also be included in the report by either appending them to the report table by using add_glance_table() or adding as a table source note by using add_glance_source_note() as shown in the code chunk below.\n\n\nModel statistics\ntbl_regression(condo.mlr1, \n               intercept = TRUE) %>% \n  add_glance_source_note(\n    label = list(sigma ~ \"\\U03C3\"),\n    include = c(r.squared, adj.r.squared, \n                AIC, statistic,\n                p.value, sigma))\n\n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Beta\n      95% CI1\n      p-value\n    \n  \n  \n    (Intercept)\n527,633\n315,417, 739,849\n<0.001\n    AREA_SQM\n12,778\n12,057, 13,498\n<0.001\n    AGE\n-24,688\n-30,092, -19,284\n<0.001\n    PROX_CBD\n-77,131\n-88,436, -65,826\n<0.001\n    PROX_CHILDCARE\n-318,473\n-530,250, -106,696\n0.003\n    PROX_ELDERLYCARE\n185,576\n107,303, 263,849\n<0.001\n    PROX_URA_GROWTH_AREA\n39,163\n16,105, 62,222\n<0.001\n    PROX_MRT\n-294,745\n-406,394, -183,096\n<0.001\n    PROX_PARK\n570,505\n442,004, 699,006\n<0.001\n    PROX_PRIMARY_SCH\n159,856\n41,698, 278,014\n0.008\n    PROX_SHOPPING_MALL\n-220,947\n-292,668, -149,226\n<0.001\n    PROX_BUS_STOP\n682,482\n418,616, 946,348\n<0.001\n    NO_Of_UNITS\n-245\n-418, -73\n0.005\n    FAMILY_FRIENDLY\n146,308\n54,321, 238,295\n0.002\n    FREEHOLD\n350,600\n255,448, 445,752\n<0.001\n  \n  \n    \n      R² = 0.651; Adjusted R² = 0.647; AIC = 42,967; Statistic = 189; p-value = <0.001; σ = 755,957\n    \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\n\n4.4.6 Checking for Multicollinearity\nLet us check if there is any sign of multicollinearity using ols_vif_tol() of olsrr package\n\n\nMulticollinearity check\nols_vif_tol(condo.mlr1)\n\n\n              Variables Tolerance      VIF\n1              AREA_SQM 0.8728554 1.145665\n2                   AGE 0.7071275 1.414172\n3              PROX_CBD 0.6356147 1.573280\n4        PROX_CHILDCARE 0.3066019 3.261559\n5      PROX_ELDERLYCARE 0.6598479 1.515501\n6  PROX_URA_GROWTH_AREA 0.7510311 1.331503\n7              PROX_MRT 0.5236090 1.909822\n8             PROX_PARK 0.8279261 1.207837\n9      PROX_PRIMARY_SCH 0.4524628 2.210126\n10   PROX_SHOPPING_MALL 0.6738795 1.483945\n11        PROX_BUS_STOP 0.3514118 2.845664\n12          NO_Of_UNITS 0.6901036 1.449058\n13      FAMILY_FRIENDLY 0.7244157 1.380423\n14             FREEHOLD 0.6931163 1.442759\n\n\nWe can conclude that there are no sign of multicollinearity among the independent variables as VIF of the independent variables are less than 10.\n\n\n4.4.7 Test for Non-Linearity\nIn the code chunk below, the ols_plot_resid_fit() of olsrr package is used to perform linearity assumption test.\n\n\nNon-Linearity test\nols_plot_resid_fit(condo.mlr1)\n\n\n\n\n\nWe can conclude that the relationships between the dependent variable and independent variables are linear as most of the data points are scattered around the 0 line.\n\n\n4.4.8 Test for Normality Assumption\nThe code chunk below uses ols_plot_resid_hist() of olsrr package to perform normality assumption test.\n\n\nNormality Assumption Test\nols_plot_resid_hist(condo.mlr1)\n\n\n\n\n\nIt is shown that the residual of the multiple linear regression model (i.e. condo.mlr1) resembles normal distribution.\n\n\n4.4.9 Testing for Spatial Autocorrelation\nIn order to perform spatial autocorrelation test, let us perform the following steps\n\nConvert condo_resale.sf from sf data frame into a SpatialPointsDataFrame.\nConvert condo_resale.res.sf from simple feature object into a SpatialPointsDataFrame because spdep package can only process sp conformed spatial data objects.\nDisplay the distribution of the residuals on an interactive map\n\n\n\nSpatial autocorrelation test\nmlr.output <- as.data.frame(condo.mlr1$residuals)\ncondo_resale.res.sf <- cbind(condo_resale.sf, \n                        condo.mlr1$residuals) %>%\nrename(`MLR_RES` = `condo.mlr1.residuals`)\ncondo_resale.sp <- as_Spatial(condo_resale.res.sf)\n\n\nLet us visualise the spatial distribution using below code chunk\n\n\nMapping the data\ntm_shape(mpsz_svy21)+\n  tmap_options(check.and.fix = TRUE) +\n  tm_polygons(alpha = 0.4) +\ntm_shape(condo_resale.res.sf) +  \n  tm_dots(col = \"MLR_RES\",\n          alpha = 0.6,\n          style=\"quantile\") +\n  tmap_mode(\"view\")+\n  tm_view(set.zoom.limits = c(11,14))\n\n\ntmap mode set to interactive viewing\n\n\nWarning: The shape mpsz_svy21 is invalid (after reprojection). See\nsf::st_is_valid\n\n\nVariable(s) \"MLR_RES\" contains positive and negative values, so midpoint is set to 0. Set midpoint = NA to show the full spectrum of the color palette.\n\n\n\n\n\n\n\nThe figure above reveal that there is sign of spatial autocorrelation.\nLet us double check by performing Moran’s I test\nFollowing steps will be performed\n\nCompute the distance-based weight matrix by using dnearneigh() function of spdep.\nConvert the output neighbours lists (i.e. nb) into a spatial weights using nb2listw() of spdep packge.\nConduct Moran’s I test for residual spatial autocorrelation by using lm.morantest() of spdep package.\n\n\n\nMoran’s I test\nnb <- dnearneigh(coordinates(condo_resale.sp), 0, 1500, longlat = FALSE)\nnb_lw <- nb2listw(nb, style = 'W')\nlm.morantest(condo.mlr1, nb_lw)\n\n\n\n    Global Moran I for regression residuals\n\ndata:  \nmodel: lm(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD +\nPROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + PROX_MRT +\nPROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP +\nNO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, data = condo_resale.sf)\nweights: nb_lw\n\nMoran I statistic standard deviate = 24.366, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nObserved Moran I      Expectation         Variance \n    1.438876e-01    -5.487594e-03     3.758259e-05 \n\n\nSince the Observed Global Moran I = 0.1424418 which is greater than 0, we can infer than the residuals resemble cluster distribution.\n\n\n4.5 Hedonic Pricing Models using GWmodel\nLet us model hedonic pricing using both the fixed and adaptive bandwidth schemes\n\n\n4.5.1 Computing fixed bandwidth\nIn the code chunk below bw.gwr() of GWModel package is used to determine the optimal fixed bandwidth to use in the model.\n\n\nFixed bandwidth\nbw.fixed <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                     PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                     PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                     PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                     FAMILY_FRIENDLY + FREEHOLD, \n                   data=condo_resale.sp, \n                   approach=\"CV\", \n                   kernel=\"gaussian\", \n                   adaptive=FALSE, \n                   longlat=FALSE)\n\n\nFixed bandwidth: 17660.96 CV score: 8.259118e+14 \nFixed bandwidth: 10917.26 CV score: 7.970454e+14 \nFixed bandwidth: 6749.419 CV score: 7.273273e+14 \nFixed bandwidth: 4173.553 CV score: 6.300006e+14 \nFixed bandwidth: 2581.58 CV score: 5.404958e+14 \nFixed bandwidth: 1597.687 CV score: 4.857515e+14 \nFixed bandwidth: 989.6077 CV score: 4.722431e+14 \nFixed bandwidth: 613.7939 CV score: 1.378294e+16 \nFixed bandwidth: 1221.873 CV score: 4.778717e+14 \nFixed bandwidth: 846.0596 CV score: 4.791629e+14 \nFixed bandwidth: 1078.325 CV score: 4.751406e+14 \nFixed bandwidth: 934.7772 CV score: 4.72518e+14 \nFixed bandwidth: 1023.495 CV score: 4.730305e+14 \nFixed bandwidth: 968.6643 CV score: 4.721317e+14 \nFixed bandwidth: 955.7206 CV score: 4.722072e+14 \nFixed bandwidth: 976.6639 CV score: 4.721387e+14 \nFixed bandwidth: 963.7202 CV score: 4.721484e+14 \nFixed bandwidth: 971.7199 CV score: 4.721293e+14 \nFixed bandwidth: 973.6083 CV score: 4.721309e+14 \nFixed bandwidth: 970.5527 CV score: 4.721295e+14 \nFixed bandwidth: 972.4412 CV score: 4.721296e+14 \nFixed bandwidth: 971.2741 CV score: 4.721292e+14 \nFixed bandwidth: 970.9985 CV score: 4.721293e+14 \nFixed bandwidth: 971.4443 CV score: 4.721292e+14 \nFixed bandwidth: 971.5496 CV score: 4.721293e+14 \nFixed bandwidth: 971.3793 CV score: 4.721292e+14 \nFixed bandwidth: 971.3391 CV score: 4.721292e+14 \nFixed bandwidth: 971.3143 CV score: 4.721292e+14 \nFixed bandwidth: 971.3545 CV score: 4.721292e+14 \nFixed bandwidth: 971.3296 CV score: 4.721292e+14 \nFixed bandwidth: 971.345 CV score: 4.721292e+14 \nFixed bandwidth: 971.3355 CV score: 4.721292e+14 \nFixed bandwidth: 971.3413 CV score: 4.721292e+14 \nFixed bandwidth: 971.3377 CV score: 4.721292e+14 \nFixed bandwidth: 971.34 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3408 CV score: 4.721292e+14 \nFixed bandwidth: 971.3403 CV score: 4.721292e+14 \nFixed bandwidth: 971.3406 CV score: 4.721292e+14 \nFixed bandwidth: 971.3404 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \nFixed bandwidth: 971.3405 CV score: 4.721292e+14 \n\n\nThe result shows that the recommended bandwidth is 971.3405 metres. Since the projection is in svy21 format, the values are in metres.\n\n\n4.5.2 GWRModel method - Fixed bandwidth\nLet us calibrate the gwr model using fixed bandwidth and gaussian kernel.\n\n\nGWR- Fixed bandwidth\ngwr.fixed <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n                         PROX_CHILDCARE + PROX_ELDERLYCARE  + PROX_URA_GROWTH_AREA + \n                         PROX_MRT   + PROX_PARK + PROX_PRIMARY_SCH + \n                         PROX_SHOPPING_MALL + PROX_BUS_STOP + NO_Of_UNITS + \n                         FAMILY_FRIENDLY + FREEHOLD, \n                       data=condo_resale.sp, \n                       bw=bw.fixed, \n                       kernel = 'gaussian', \n                       longlat = FALSE)\n\n\n\ngwr.fixed\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2022-12-11 17:00:48 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.fixed, kernel = \"gaussian\", \n    longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  < 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  < 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  < 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  < 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: < 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Fixed bandwidth: 971.3405 \n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -3.5988e+07 -5.1998e+05  7.6780e+05  1.7412e+06\n   AREA_SQM              1.0003e+03  5.2758e+03  7.4740e+03  1.2301e+04\n   AGE                  -1.3475e+05 -2.0813e+04 -8.6260e+03 -3.7784e+03\n   PROX_CBD             -7.7047e+07 -2.3608e+05 -8.3600e+04  3.4646e+04\n   PROX_CHILDCARE       -6.0097e+06 -3.3667e+05 -9.7425e+04  2.9007e+05\n   PROX_ELDERLYCARE     -3.5000e+06 -1.5970e+05  3.1971e+04  1.9577e+05\n   PROX_URA_GROWTH_AREA -3.0170e+06 -8.2013e+04  7.0749e+04  2.2612e+05\n   PROX_MRT             -3.5282e+06 -6.5836e+05 -1.8833e+05  3.6922e+04\n   PROX_PARK            -1.2062e+06 -2.1732e+05  3.5383e+04  4.1335e+05\n   PROX_PRIMARY_SCH     -2.2695e+07 -1.7066e+05  4.8472e+04  5.1555e+05\n   PROX_SHOPPING_MALL   -7.2585e+06 -1.6684e+05 -1.0517e+04  1.5923e+05\n   PROX_BUS_STOP        -1.4676e+06 -4.5207e+04  3.7601e+05  1.1664e+06\n   NO_Of_UNITS          -1.3170e+03 -2.4822e+02 -3.0846e+01  2.5496e+02\n   FAMILY_FRIENDLY      -2.2749e+06 -1.1140e+05  7.6214e+03  1.6107e+05\n   FREEHOLD             -9.2067e+06  3.8073e+04  1.5169e+05  3.7528e+05\n                             Max.\n   Intercept            112793548\n   AREA_SQM                 21575\n   AGE                     434201\n   PROX_CBD               2704596\n   PROX_CHILDCARE         1654087\n   PROX_ELDERLYCARE      38867814\n   PROX_URA_GROWTH_AREA  78515730\n   PROX_MRT               3124316\n   PROX_PARK             18122425\n   PROX_PRIMARY_SCH       4637503\n   PROX_SHOPPING_MALL     1529952\n   PROX_BUS_STOP         11342182\n   NO_Of_UNITS              12907\n   FAMILY_FRIENDLY        1720744\n   FREEHOLD               6073636\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 438.3804 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 997.6196 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 42263.61 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41632.36 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 42515.71 \n   Residual sum of squares: 2.53407e+14 \n   R-square value:  0.8909912 \n   Adjusted R-square value:  0.8430417 \n\n   ***********************************************************************\n   Program stops at: 2022-12-11 17:00:49 \n\n\nThe report shows that the adjusted r-square of the gwr is 0.8430 which is significantly better than the global multiple linear regression model of 0.6472.\n\n\n4.5.3 GWRModel method - Adaptive bandwidth\nLet us calibrate the gwr-absed hedonic pricing model by using adaptive bandwidth approach.\nComputing the adaptive bandwidth\nIn the code chunk below bw.gwr() of GWModel package is used to determine the optimal fixed bandwidth with adaptive = TRUE to use in the model.\n\n\nAdaptive Bandwidth\nbw.adaptive <- bw.gwr(formula = SELLING_PRICE ~ AREA_SQM + AGE  + \n                        PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE    + \n                        PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                        PROX_PRIMARY_SCH + PROX_SHOPPING_MALL   + PROX_BUS_STOP + \n                        NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                      data=condo_resale.sp, \n                      approach=\"CV\", \n                      kernel=\"gaussian\", \n                      adaptive=TRUE, \n                      longlat=FALSE)\n\n\nAdaptive bandwidth: 895 CV score: 7.952401e+14 \nAdaptive bandwidth: 561 CV score: 7.667364e+14 \nAdaptive bandwidth: 354 CV score: 6.953454e+14 \nAdaptive bandwidth: 226 CV score: 6.15223e+14 \nAdaptive bandwidth: 147 CV score: 5.674373e+14 \nAdaptive bandwidth: 98 CV score: 5.426745e+14 \nAdaptive bandwidth: 68 CV score: 5.168117e+14 \nAdaptive bandwidth: 49 CV score: 4.859631e+14 \nAdaptive bandwidth: 37 CV score: 4.646518e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \nAdaptive bandwidth: 25 CV score: 4.430816e+14 \nAdaptive bandwidth: 32 CV score: 4.505602e+14 \nAdaptive bandwidth: 27 CV score: 4.462172e+14 \nAdaptive bandwidth: 30 CV score: 4.422088e+14 \n\n\nThe result reveals that 30 is the recommended data points to be used.\n\n\nConstructing the adaptive bandwidth gwr model\nLet us calibrate the gwr-based hedonic pricing model by using adaptive bandwidth and gaussian kernel as shown in the code chunk below.\n\n\nGWR - Adaptive Bandwidth\ngwr.adaptive <- gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + \n                            PROX_CBD + PROX_CHILDCARE + PROX_ELDERLYCARE + \n                            PROX_URA_GROWTH_AREA + PROX_MRT + PROX_PARK + \n                            PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + PROX_BUS_STOP + \n                            NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n                          data=condo_resale.sp, bw=bw.adaptive, \n                          kernel = 'gaussian', \n                          adaptive=TRUE, \n                          longlat = FALSE)\ngwr.adaptive\n\n\n   ***********************************************************************\n   *                       Package   GWmodel                             *\n   ***********************************************************************\n   Program starts at: 2022-12-11 17:01:00 \n   Call:\n   gwr.basic(formula = SELLING_PRICE ~ AREA_SQM + AGE + PROX_CBD + \n    PROX_CHILDCARE + PROX_ELDERLYCARE + PROX_URA_GROWTH_AREA + \n    PROX_MRT + PROX_PARK + PROX_PRIMARY_SCH + PROX_SHOPPING_MALL + \n    PROX_BUS_STOP + NO_Of_UNITS + FAMILY_FRIENDLY + FREEHOLD, \n    data = condo_resale.sp, bw = bw.adaptive, kernel = \"gaussian\", \n    adaptive = TRUE, longlat = FALSE)\n\n   Dependent (y) variable:  SELLING_PRICE\n   Independent variables:  AREA_SQM AGE PROX_CBD PROX_CHILDCARE PROX_ELDERLYCARE PROX_URA_GROWTH_AREA PROX_MRT PROX_PARK PROX_PRIMARY_SCH PROX_SHOPPING_MALL PROX_BUS_STOP NO_Of_UNITS FAMILY_FRIENDLY FREEHOLD\n   Number of data points: 1436\n   ***********************************************************************\n   *                    Results of Global Regression                     *\n   ***********************************************************************\n\n   Call:\n    lm(formula = formula, data = data)\n\n   Residuals:\n     Min       1Q   Median       3Q      Max \n-3470778  -298119   -23481   248917 12234210 \n\n   Coefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n   (Intercept)           527633.22  108183.22   4.877 1.20e-06 ***\n   AREA_SQM               12777.52     367.48  34.771  < 2e-16 ***\n   AGE                   -24687.74    2754.84  -8.962  < 2e-16 ***\n   PROX_CBD              -77131.32    5763.12 -13.384  < 2e-16 ***\n   PROX_CHILDCARE       -318472.75  107959.51  -2.950 0.003231 ** \n   PROX_ELDERLYCARE      185575.62   39901.86   4.651 3.61e-06 ***\n   PROX_URA_GROWTH_AREA   39163.25   11754.83   3.332 0.000885 ***\n   PROX_MRT             -294745.11   56916.37  -5.179 2.56e-07 ***\n   PROX_PARK             570504.81   65507.03   8.709  < 2e-16 ***\n   PROX_PRIMARY_SCH      159856.14   60234.60   2.654 0.008046 ** \n   PROX_SHOPPING_MALL   -220947.25   36561.83  -6.043 1.93e-09 ***\n   PROX_BUS_STOP         682482.22  134513.24   5.074 4.42e-07 ***\n   NO_Of_UNITS             -245.48      87.95  -2.791 0.005321 ** \n   FAMILY_FRIENDLY       146307.58   46893.02   3.120 0.001845 ** \n   FREEHOLD              350599.81   48506.48   7.228 7.98e-13 ***\n\n   ---Significance stars\n   Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 \n   Residual standard error: 756000 on 1421 degrees of freedom\n   Multiple R-squared: 0.6507\n   Adjusted R-squared: 0.6472 \n   F-statistic: 189.1 on 14 and 1421 DF,  p-value: < 2.2e-16 \n   ***Extra Diagnostic information\n   Residual sum of squares: 8.120609e+14\n   Sigma(hat): 752522.9\n   AIC:  42966.76\n   AICc:  42967.14\n   BIC:  41731.39\n   ***********************************************************************\n   *          Results of Geographically Weighted Regression              *\n   ***********************************************************************\n\n   *********************Model calibration information*********************\n   Kernel function: gaussian \n   Adaptive bandwidth: 30 (number of nearest neighbours)\n   Regression points: the same locations as observations are used.\n   Distance metric: Euclidean distance metric is used.\n\n   ****************Summary of GWR coefficient estimates:******************\n                               Min.     1st Qu.      Median     3rd Qu.\n   Intercept            -1.3487e+08 -2.4669e+05  7.7928e+05  1.6194e+06\n   AREA_SQM              3.3188e+03  5.6285e+03  7.7825e+03  1.2738e+04\n   AGE                  -9.6746e+04 -2.9288e+04 -1.4043e+04 -5.6119e+03\n   PROX_CBD             -2.5330e+06 -1.6256e+05 -7.7242e+04  2.6624e+03\n   PROX_CHILDCARE       -1.2790e+06 -2.0175e+05  8.7158e+03  3.7778e+05\n   PROX_ELDERLYCARE     -1.6212e+06 -9.2050e+04  6.1029e+04  2.8184e+05\n   PROX_URA_GROWTH_AREA -7.2686e+06 -3.0350e+04  4.5869e+04  2.4613e+05\n   PROX_MRT             -4.3781e+07 -6.7282e+05 -2.2115e+05 -7.4593e+04\n   PROX_PARK            -2.9020e+06 -1.6782e+05  1.1601e+05  4.6572e+05\n   PROX_PRIMARY_SCH     -8.6418e+05 -1.6627e+05 -7.7853e+03  4.3222e+05\n   PROX_SHOPPING_MALL   -1.8272e+06 -1.3175e+05 -1.4049e+04  1.3799e+05\n   PROX_BUS_STOP        -2.0579e+06 -7.1461e+04  4.1104e+05  1.2071e+06\n   NO_Of_UNITS          -2.1993e+03 -2.3685e+02 -3.4699e+01  1.1657e+02\n   FAMILY_FRIENDLY      -5.9879e+05 -5.0927e+04  2.6173e+04  2.2481e+05\n   FREEHOLD             -1.6340e+05  4.0765e+04  1.9023e+05  3.7960e+05\n                            Max.\n   Intercept            18758355\n   AREA_SQM                23064\n   AGE                     13303\n   PROX_CBD             11346650\n   PROX_CHILDCARE        2892127\n   PROX_ELDERLYCARE      2465671\n   PROX_URA_GROWTH_AREA  7384059\n   PROX_MRT              1186242\n   PROX_PARK             2588497\n   PROX_PRIMARY_SCH      3381462\n   PROX_SHOPPING_MALL   38038564\n   PROX_BUS_STOP        12081592\n   NO_Of_UNITS              1010\n   FAMILY_FRIENDLY       2072414\n   FREEHOLD              1813995\n   ************************Diagnostic information*************************\n   Number of data points: 1436 \n   Effective number of parameters (2trace(S) - trace(S'S)): 350.3088 \n   Effective degrees of freedom (n-2trace(S) + trace(S'S)): 1085.691 \n   AICc (GWR book, Fotheringham, et al. 2002, p. 61, eq 2.33): 41982.22 \n   AIC (GWR book, Fotheringham, et al. 2002,GWR p. 96, eq. 4.22): 41546.74 \n   BIC (GWR book, Fotheringham, et al. 2002,GWR p. 61, eq. 2.34): 41914.08 \n   Residual sum of squares: 2.528227e+14 \n   R-square value:  0.8912425 \n   Adjusted R-square value:  0.8561185 \n\n   ***********************************************************************\n   Program stops at: 2022-12-11 17:01:02 \n\n\nIt reveals that the adjusted r-square of the GWR is 0.8561 which is significantly better than the global multiple linear regression model of 0.6472. Also, AICc value of the adaptive distance gwr is 41982.22 which is even smaller than the AICc of the fixed distance gwr of 42263.61. Lesser the AICC, better the model and hence adaptive model is chosen in the subsequent analysis,\n\n\n4.5.4 Visualising GWR Output\nThe output feature class table includes fields for observed and predicted y values, condition number (cond), Local R2, residuals, and explanatory variable coefficients and standard errors. They are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called SDF of the output list.\n\n\nGWR Output\ncondo_resale.sf.adaptive <- st_as_sf(gwr.adaptive$SDF) %>%\n  st_transform(crs=3414)\ncondo_resale.sf.adaptive.svy21 <- st_transform(condo_resale.sf.adaptive, 3414)\n\ngwr.adaptive.output <- as.data.frame(gwr.adaptive$SDF)\ncondo_resale.sf.adaptive <- cbind(condo_resale.res.sf, as.matrix(gwr.adaptive.output))\n\nsummary(gwr.adaptive$SDF$yhat)\n\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n  171347  1102001  1385528  1751842  1982307 13887901 \n\n\n\n\n4.5.5 Visualising local R2\nThe code chunks below is used to create an interactive point symbol map to visualise local R2 values.\n\n\nMapping Local R2\ntmap_mode(\"view\")\n\n\ntmap mode set to interactive viewing\n\n\nMapping Local R2\ntm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"Local_R2\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\n\nWarning: The shape mpsz_svy21 is invalid (after reprojection). See\nsf::st_is_valid\n\n\n\n\n\n\n\nWe can see that the places which have darker points indicate that the local regression model fits well whereas very low values indicate the local model is performing poorly. Mapping these Local R2 values helps us to understand where GWR predicts well and where it predicts poorly which may provide clues about important variables that may be missing from the regression model.\n\n\n4.5.6 Visualising coefficient estimates\nCoefficient Standard Error: these values measure the reliability of each coefficient estimate. Confidence in those estimates are higher when standard errors are small in relation to the actual coefficient values. Large standard errors may indicate problems with local collinearity.\nThey are all stored in a SpatialPointsDataFrame or SpatialPolygonsDataFrame object integrated with fit.points, GWR coefficient estimates, y value, predicted values, coefficient standard errors and t-values in its “data” slot in an object called SDF of the output list.\nNow let’s create an interactive static point symbol map to view standard error and t-values of area_sqm adjacent to each other using the code chunk below\n\ntmap_mode(\"view\")\n\ntmap mode set to interactive viewing\n\nAREA_SQM_SE <- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_SE\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\nAREA_SQM_TV <- tm_shape(mpsz_svy21)+\n  tm_polygons(alpha = 0.1) +\ntm_shape(condo_resale.sf.adaptive) +  \n  tm_dots(col = \"AREA_SQM_TV\",\n          border.col = \"gray60\",\n          border.lwd = 1) +\n  tm_view(set.zoom.limits = c(11,14))\n\ntmap_arrange(AREA_SQM_SE, AREA_SQM_TV, \n             asp=1, ncol=2,\n             sync = TRUE)\n\nWarning: The shape mpsz_svy21 is invalid (after reprojection). See\nsf::st_is_valid\n\nWarning: The shape mpsz_svy21 is invalid (after reprojection). See\nsf::st_is_valid\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nargument sync in tmap_arrange() helps us to view the 2 maps next to each other and when one map is zoomed in or zoomed out, the other one is also affected.\n\n\n4.5.7 Visualizing URA Planning Region\nThe code chunks below is used to create an static point symbol map to visualise local R2 by planning region (Here, Central region)\n\n\nMapping URA Planning region\ntm_shape(mpsz_svy21[mpsz_svy21$REGION_N==\"CENTRAL REGION\", ])+\n  tm_polygons()+\ntm_shape(condo_resale.sf.adaptive) + \n  tm_bubbles(col = \"Local_R2\",\n           size = 0.15,\n           border.col = \"gray60\",\n           border.lwd = 1)+\n  tmap_mode(\"plot\")\n\n\ntmap mode set to plotting\n\n\nWarning: The shape mpsz_svy21[mpsz_svy21$REGION_N == \"CENTRAL REGION\", ] is\ninvalid. See sf::st_is_valid\n\n\n\n\n\nIn the above map, we can see that the places which have darker static point symbols indicate that the local regression model fits well whereas very low values indicate the local model is performing poorly.\n\n\n\n5. Conclusion\nSo, in this study, we have seen in detail how to build model with geographically weighted regression and the various steps in preparing the final data such as test for normality assumption, tests for non-linearity, checking multi collinearity. Some of the new concepts mentioned in this study are building publication table using Ordinary Least Square Regression OLSR) method and gtsummary method and the feature of syncing 2 maps together."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Geospatial Analytics",
    "section": "",
    "text": "Hi, My name is Raveena :)\nWelcome to my Geo Spatial Analysis Blog! Let’s travel and explore maps :)"
  },
  {
    "objectID": "TakeHomeEx/ex1.html",
    "href": "TakeHomeEx/ex1.html",
    "title": "Where is the pump? - Exploring spatial patterns of non-functional waterpoints in Nigeria",
    "section": "",
    "text": "2. Objective\nThe main aim of the study is to understand the dynamics of spatial patterns of non-functional water points in Nigeria and its diffusion over spatial boundaries by applying appropriate global and local measures of spatial association techniques.\n\n\n\nPicture Credit - Google\n\n\n\n\n3. Glimpse of Steps\nSome of the important steps performed in this study are as follows\n\nImporting shapefile into R using sf package.\nDeriving the proportion of functional and non-functional water point at LGA level using appropriate tidyr and dplyr methods.\nCombining the geospatial and aspatial data frame into simple feature data frame.\nPerforming outliers/clusters analysis by using appropriate local measures of spatial association methods.\nPerforming hotspot areas analysis by using appropriate local measures of spatial association methods.\nThematic Mapping - Plotting maps to show the spatial distribution of functional and non-functional water point rate at LGA level by using appropriate thematic mapping technique provided by tmap package\nAnalytical Mapping - Plotting hotspot areas and outliers/clusters maps of functional and non-functional water point rate at LGA level by using appropriate thematic mapping technique provided by tmap package\n\n\n\n4. Data\nFor this study, data from WPdx Global Data Repositories and geoBoundaries are used. Both are in geospatial format. These dataset provide information about waterpoints and Nigeria’s Administrative boundary shape file.\n\nTable1: Datasets Used\n\n\nData Type\nDescription\nSource\n\n\n\n\nGeospatial\nNigeria Level-2 Administrative Boundary\nGeoboundaries\n\n\nGeospatial\nWater point related data on WPdx standard\nWaterpoint access data\n\n\n\n\n\n5. Deep Dive into Geospatial Analysis\nLet us try to understand the dynamics of spatial patterns of non-functional water points in Nigeria and its diffusion over spatial boundaries using appropriate global and local measures of spatial association techniques.\n\n5.1 Loading packages\nLet us first load required packages into R environment. p_load function pf pacman package is used to install and load sf and tidyverse pacagkes into R environment.\n\n\nPackages\npacman::p_load(sf, tidyverse, tmap, spdep, patchwork, ggthemes)\n\n\n\n\n5.2 Importing Geospatial Data\nNow let us import both the geospatial data. The code chunk below uses st_read() function of sf package to import geoBoundaries-NGA-ADM2_simplified shapefile and geo_export into R environment.\nTwo arguments are used :\n\ndsn - destination : to define the data path\nlayer - to provide the shapefile name\n\n\n\nImporting Data\nwaterpts <- st_read(dsn = \"data/aspatial\",\n              layer = \"geo_export\",\n              crs = 4326) %>%\n  filter(clean_coun == \"Nigeria\")\nnigeria <- st_read(dsn = \"data/geospatial\",\n               layer = \"geoBoundaries-NGA-ADM2\",\n               crs = 4326)\n\n\n\nst_read() of sf package is used to import geo_export and geoBoundaries-NGA-ADM2_simplified shapefile into R environment and save the imported geospatial data into simple feature data table.\nfilter() of dplyr package is used to extract water point records of Nigeria.\n\nIn order to reduce the file size let us save the data in .rds format.\n\n\nSaving the file\nwrite_rds(waterpts, \"data/wp_nga.rds\")\n\n\nwrite_rds() of readr package is used to save the extracted sf data table into an output file in rds data format.\n\n\n5.3 Data Wrangling\nLet us now preprocess the data before performing any analysis\n\n\nReplacing NA values into Unknown\nHere, we are recoding the NA values into Unknown. In the code chunk below, replace_na() is used to recode all the NA values in status_cle field into Unknown.\n\n\nReplacing NA\nwp_nga <- read_rds(\"data/wp_nga.rds\") %>%\n  mutate(status_cle = replace_na(status_cle, \"Unknown\"))\n\n\n\n\nExtracting Funtional, Non-Functional and Unknown water points\nAs our objective is to focus on waterpoints, let us extract the three types and save it as a dataframe for further analysis\n\n\nExtracting waterpoints\nwpt_functional <- wp_nga %>%\n  filter(status_cle %in%\n           c(\"Functional\", \n             \"Functional but not in use\",\n             \"Functional but needs repair\"))\n\nwpt_nonfunctional <- wp_nga %>%\n  filter(status_cle %in%\n           c(\"Abandoned/Decommissioned\", \n             \"Abandoned\",\n             \"Non-Functional\",\n             \"Non functional due to dry season\",\n             \"Non-Functional due to dry season\"))\n\nwpt_unknown <- wp_nga %>%\n  filter(status_cle == \"Unknown\")\n\n\nIn the code chunk above, filter() of dplyr is used to select the specific water points.\n\n\nComputing Number of Waterpoints in each Second-level Administrative Division\nWe have to perform 2 steps to calculate the total number of functional, non-functional and Unknown waterpoints in each division.\n\nLet us identify no. of waterpoints located inside each division by using st_intersects().\nNext, let us calculate number of waterpoints that fall inside each division by using length() function.\n\n\n\nComputing numbers\nnga_wp <- nigeria %>% \n  mutate(`total wpt` = lengths(\n    st_intersects(nigeria, wp_nga))) %>%\n  mutate(`wpt functional` = lengths(\n    st_intersects(nigeria, wpt_functional))) %>%\n  mutate(`wpt non-functional` = lengths(\n    st_intersects(nigeria, wpt_nonfunctional))) %>%\n  mutate(`wpt unknown` = lengths(\n    st_intersects(nigeria, wpt_unknown)))\n\n\n\n\nComputing Proportion of Functional and Non-Functional water points\nNow, let us calculate what is the overall proportion of functional and non-functional waterpoints by dividing the no. of functional waterpoints by the total no. of waterpoints. Similarly, for non-functional waterpoint proportion, numerator is replaced by non-functional waterpoint.\n\n\nComputing proportion\nnga_wp <- nga_wp %>%\n  mutate(pct_functional = `wpt functional`/`total wpt`) %>%\n  mutate(`pct_non-functional` = `wpt non-functional`/`total wpt`)\n\n\n\n\nSaving the final rds file\nIn order to manage the storage data efficiently, we are saving the final data frame in rds format.\n\n\nSaving rds file\nwrite_rds(nga_wp, \"data/nga_wp.rds\")\nwrite_rds(wpt_functional, \"data/wpt_functional.rds\")\nwrite_rds(wpt_nonfunctional, \"data/wpt_nonfunctional.rds\")\n\n\n\n\n5.4 Exploratory Data Analysis\nBefore performing spatial analysis, let us first do some preliminary data analysis to understand the data better in terms of water points.\n\n\n5.4.1 What is the proportion of functional and non-functional water points?\nBefore visualising, its important for us to prepare the data. Based on WPDx Data Standard, the variable ‘#status_id’ refers to Presence of Water when Assessed. Binary response, i.e. Yes/No are recoded into Functional / Not-Functional.\n\n\nPreparing the data\nnga_sf <- read_rds(\"data/nga_sf.rds\")\nngawater_sf <- read_rds(\"data/ngawater_sf.rds\")\n\nngawater_sf<-ngawater_sf %>%\n  mutate(`#status_id`=\n                case_when(`#status_id`==\"Yes\"~\"Functional\",\n                          `#status_id`==\"No\"~\"Non-Functional\",\n                          `#status_id`== \"Unknown\"~\"unknown\"))\n\n\n\n\nProportion graph\nggplot(data= ngawater_sf, \n       aes(x= `#status_id`)) +\n       geom_bar(fill= '#CD5C5C') +\n       #ylim(0, 150) +\n       geom_text(stat = 'count',\n           aes(label= paste0(stat(count), ', ', \n                             round(stat(count)/sum(stat(count))*100, \n                             1), '%')), vjust= -0.5, size= 2.5) +\n       labs(y= 'No. of \\nwater points',\n            x= 'Water Points',\n            title = \"Distribution of water points\") +\n       theme(axis.title.y= element_text(angle=0), \n             axis.ticks.x= element_blank(),\n             panel.background= element_blank(),\n             axis.line= element_line(color= 'grey'),\n             axis.title.y.left = element_text(vjust = 0.5),\n             plot.title = element_text(hjust=0.5))\n\n\n\n\n\nInsights:\nNigeria consists of almost 55% of functional, 34% of non-functional and 11% of unknown waterpoints.\n\n\n5.4.2 What is the district wise proportion of water points?\nFirst let us prepare the data\n\nFilter all the NA values in waterpoint_status\nGroup by shape name and waterpoint status\nCompute the count and proposition by dividing the count by total number of waterpoints in that division\nSelecting top 10 rows with more no. of waterpoints\n\n\n\nPreparing data\nnga_sf <- nga_sf %>% filter(!is.na(waterpoint_status)) \ndf <- nga_sf %>% \n  group_by(shapeName,waterpoint_status) %>% \n  tally() %>%\n  group_by(shapeName) %>%\n  mutate(total=sum(n),\n         prop=round(n*100/total)) %>%\n  arrange(desc(total))\ntop_10 <- head(df,10)\n\n\n\n\nDistrictwise Proportion\np3 <- ggplot(data=top_10,\n             aes(x=shapeName,\n                 y=prop,\n                 fill=waterpoint_status))+\n  geom_col()+\n  geom_text(aes(label=paste0(prop,\"%\")),\n            position = position_stack(vjust=0.5),size=3)+\n  theme(axis.text.x=element_text(angle=0))+\n    xlab(\"Division\")+\n    ylab(\"% of \\n Waterpoints\")+\n    ggtitle(\"Proportion of waterpoints by District\")+\n    theme_bw()+\n    guides(fill=guide_legend(title=\"Waterpoint\"),\n           shape=guide_legend(override.aes = list(size=0.5)))+\n    theme(plot.title = element_text(hjust=0.5, size=13),\n          legend.title = element_text(size=9),\n          legend.text = element_text(size=7),\n          axis.text = element_text(face=\"bold\"),\n          axis.ticks.x=element_blank(),\n          axis.title.y=element_text(angle=0),\n          axis.title.y.left = element_text(vjust = 0.5))\np3\n\n\n\n\n\nInsights:\nAmong the divisions which have most no. of waterpoints, in Ifelodun division, almost 50% of waterpoints are non-functional. It is a matter of concern.\n\n\n5.4.3 Which district has most no. of non-functional waterpoints?\nTo find out the solution for this question, first let’s prepare the data accordingly:\n\nFilter non-functional waterpoints\nArrange it in descending order by the count values\nSelect top 10 divisions\n\n\n\nPreparing data\nnonfunc_top10 <- df %>%\n  filter(waterpoint_status == \"Non-Functional\") %>%\n  arrange(desc(n)) \nnonfunc_top10 <- head(nonfunc_top10, 10)\n\n\n\n\nTop 10 divisions\nggplot(data = nonfunc_top10,\n       aes(y = reorder(shapeName, n),\n           x=n)) + \n  geom_bar(stat = \"identity\",\n           fill = \"coral\")+\n  labs(y= 'Division',\n       x='No. of Non-Functional water points',\n       title=\"Top 10 divisions by Non-Functional waterpoints\",) +\n  geom_text(stat='identity', aes(label=paste0(n)),hjust=-0.5)+\n  theme(axis.title.y=element_text(angle=0), \n        axis.ticks.x=element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_line(color='grey'),\n        plot.title = element_text(hjust = 0.5),\n        axis.title.y.left = element_text(vjust = 0.5), \n        axis.text = element_text(face=\"bold\") )\n\n\n\n\n\nInsights:\nAmong all 774 administrative level 2 divisions, Ifelodun has most no. of non-functional water points followed by Igabi, Irepodun, Oyun and Sabon-Gari.\n\n\n5.4.4 What are the causes of waterpoints to be non-functional?\nAs our objective is to focus more on non-functional waterpoints, let us try to understand what are the major causes for a water point to become non-functional and which contributes the most?\nTo find out the solution for this question, first let’s prepare the data accordingly:\n\nImport the data which is already filtered by non-functional waterpoints.\nSimilar causes are recoded to avoid redundancy\n\n\n\nPreparing data\nwpt_nonfunctional <- read_rds(\"data/wpt_nonfunctional.rds\")\nwpt_nonfunctional <- wpt_nonfunctional %>%\n  mutate(status_cle=recode(status_cle, \n                     'Non-Functional due to dry season'='Dry Season',\n                     'Non functional due to dry season'='Dry Season',\n                     'Abandoned/Decommissioned' = 'Abandoned / Decommissioned',\n                     'Abandoned' = 'Abandoned / Decommissioned'))\nnonfun_order <- factor(wpt_nonfunctional$status_cle, level = c('Non-Functional', 'Dry Season','Abandoned / Decommissioned'))\n\n\n\n\nNon-functional types\nggplot(data= wpt_nonfunctional, \n       aes(x= nonfun_order)) +\n       geom_bar(fill= 'plum') +\n       #ylim(0, 150) +\n       geom_text(stat = 'count',\n           aes(label= paste0(stat(count), ', ', \n                             round(stat(count)/sum(stat(count))*100, \n                             1), '%')), vjust= -0.5, size= 2.5) +\n       labs(x= 'Reasons',\n            y= 'No. of \\nwater points',\n            title = \"What are the causes of non-functional water points?\") +\n       theme(axis.title.y= element_text(angle=0), \n             axis.ticks.x= element_blank(),\n             panel.background= element_blank(),\n             axis.line= element_line(color= 'grey'),\n             axis.title.y.left = element_text(vjust = 0.5),\n             plot.title = element_text(hjust=0.5))\n\n\n\n\n\nInsights:\nAlmost 90% of non-functional waterpoints are not reason specific. Some other reasons include dry season , waterpoints are decommissioned or some are left without being taken care of.\n\n\n5.5 Exploratory Spatial Data Analysis\nNow that, we have understood the non-functional waterpoints better in terms of numbers, let us plot maps to show the spatial distribution of functional and non-functional water point rate at LGA level by using appropriate thematic mapping technique provided by tmap package.\n\n\n5.5.1 How are the waterpoints spread across the country?\nWe want to know how the waterpoints are spatially distributed across the country. For that,\n\nCompute the maps indivually for total, functional , non-functional and unknown no. of waterpoints.\nMerge into 1 figure using tmap_arrange().\n\nThe code chunk below will draw a cartographic standard choropleth map of total, functional, non-functional and unkown waterpoints.\n\n\nTotal waterpoint spread\nnga_wp <- read_rds(\"data/nga_wp.rds\")\nm1 <-  tm_shape(nga_wp)+\n  tm_fill(\"total wpt\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Total \\nWaterpoints ratio\") +\n  tm_layout(main.title = \"Distribution of total water points ratio by division\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 1, \n            legend.width = 1,\n            legend.text.size = 1,\n            legend.title.size = 1,\n            main.title.fontface = \"bold\",\n            frame = TRUE) +\n  tmap_mode(\"plot\")+\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\",\n             position=c(\"right\", \"top\"))\n\n\n\n\nFunctional waterpoint spread\nm2 <-  tm_shape(nga_wp)+\n  tm_fill(\"wpt functional\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Functional \\nWaterpoints ratio\") +\n  tm_layout(main.title = \"Distribution of functional water points ratio by division\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 1, \n            legend.width = 1,\n            legend.text.size = 1,\n            legend.title.size = 1,\n            main.title.fontface = \"bold\",\n            frame = TRUE) +\n  tmap_mode(\"plot\")+\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\",\n             position=c(\"right\", \"top\"))\n\n\n\n\nNonfunctional waterpoint spread\nm3 <-  tm_shape(nga_wp)+\n  tm_fill(\"wpt non-functional\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Non-functional \\nWaterpoints ratio\") +\n  tm_layout(main.title = \"Distribution of non-functional water points ratio by division\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 1, \n            legend.width = 1,\n            legend.text.size = 1,\n            legend.title.size = 1,\n            main.title.fontface = \"bold\",\n            frame = TRUE) +\n  tmap_mode(\"plot\")+\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\",\n             position=c(\"right\", \"top\"))\n\n\n\n\nUnknown waterpoint spread\nm4 <-  tm_shape(nga_wp)+\n  tm_fill(\"wpt unknown\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Unknown \\nWaterpoints ratio\") +\n  tm_layout(main.title = \"Distribution of unknown water points ratio by division\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 1, \n            legend.width = 1,\n            legend.text.size = 1,\n            legend.title.size = 1,\n            main.title.fontface = \"bold\",\n            frame = TRUE) +\n  tmap_mode(\"plot\")+\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\",\n             position=c(\"right\", \"top\"))\n\n\nThe code chunk below uses tmap_arrange() to combine all 4 plots into one.\n\ntmap_arrange(m1, m2, m3, m4, ncol=2)\n\n\n\n\nInsights:\n\nOverall, most no. of waterpoints are located in south east and northern parts of the nation. Surprisingly, eastern part of the nation doesn’t have any waterpoints at all.\nOf all, most parts of the northern region, some parts of west and central region have functional water points.\nThe non-functional waterpoints are all over in place. They are scattered mostly around southeast, and central region.\nFinally, most of the waterpoints are classified into functional or non-functional. Only few waterpoints in the southeast and western region are unknown.\n\n\n\n5.5.2 How are the non-functional waterpoints spread across the country?\nAlthough we have seen how the overall pattern is, our focus is more on non-functional waterpoints. Also, we were not able to see the exact division in the former plot. So let us be more specific and try viewing in interactive mode.\n\n\nNonfunctional waterpoint spread\n  tm_shape(nga_wp)+\n  tm_fill(\"wpt non-functional\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Non-functional \\nWaterpoints ratio\") +\n  tm_layout(main.title = \"Distribution of non-functional water points ratio by   division\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 1, \n            legend.width = 1,\n            legend.text.size = 1,\n            legend.title.size = 1,\n            frame = TRUE) +\n  tmap_mode(\"view\")+\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\",\n             position=c(\"right\", \"top\"))\n\n\n\n\n\n\n\nInsights:\nSouth eastern divisions such as Bali, Gassol, Ukum, Wukari, Donga constitutes of more no. of non-functional waterpoints whereas central region divisions such as Ganjuwa, Ningi, Toro have medium no. of non-functional waterpoints.\nLimitation:\nAlthough exploring using the choropleth maps provide us details, it is limited in their ability to represent geospatial data in a useful and methodologically rigors way.\nHence, let us proceed to using Exploratory Spatial Data Analysis methods where we attempt to establish whether the data is in fact spatially autocorrelated.\n\n\n5.5.3 Visualising Non-Functional Waterpoints\nIts very important to know not only how many different classes that the data should be categorized into, but aslo what the value ranges of those classes should be. There are most common “default” methods of classifying data. Of all, equal and quantile are common. Each has its own advantages and disadvantages.\nQuantiles : This method classifies data into a certain number of categories with an equal number of units in each category.\nEqual Intervals : This method sets the value ranges in each category equal in size. The entire range of data values (max - min) is divided equally into however many categories have been chosen.\nIn the former plots, we have used quantile classiifica, let us now compare with equal interval classification. The below code chunk creates 2 maps classified as equal and quantile interval. Then merged to one using tmap_arrange()\n\n\nEqual interval classification map\nequal <- tm_shape(nga_wp) +\n  tm_fill(\"wpt non-functional\",\n          n = 5,\n          style = \"equal\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Equal interval classification\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            legend.height = 1, \n            legend.width = 1,\n            legend.text.size = 1,\n            legend.title.size = 1)+\n  tmap_mode(\"plot\")+\n  tm_compass(type=\"8star\",\n             position=c(\"right\", \"top\"))\n\n\n\n\nQuantile interval classification map\nquantile <- tm_shape(nga_wp) +\n  tm_fill(\"wpt non-functional\",\n          n = 5,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Quantile interval classification map\",\n            main.title.position = \"center\",\n            main.title.size = 2,\n            legend.height = 1, \n            legend.width = 1,\n            legend.text.size = 1,\n            legend.title.size = 1)+\n  tmap_mode(\"plot\")+\n  tm_compass(type=\"8star\",\n             position=c(\"right\", \"top\"))\n\n\n\ntmap_arrange(equal, \n             quantile, \n             asp=1, \n             ncol=2)\n\n\n\n\nInsights:\nIt is seen that map classification with quantile interval is better than map classification with equal interval as the former clearly spreads out rather clustering into one segment. Hence, quantile style will be used in further analysis.\n\n\n5.6 Computing Spatial Weight Matrix\nBy measuring global spatial autocorrelation we can find out whether the distribution of non-functional waterpoints are spatially dependent or the distribution is just a random phenomenon.\nBefore we can compute the global spatial autocorrelation statistics, we need to construct a spatial weights which are used to define the neighbourhood relationships between the divisions in Nigeria.\nIn general, there are contiguity based spatial weights and distance based spatial weights\nContiguity based spatial weights: Contiguity means that two spatial units share a common border of non-zero length. Operationally, we can further distinguish between a rook and a queen criterion of contiguity.\nThere are various methods of determining adjacency. The Rook method (named for the chess piece) considers areas adjacent if they are directly located horizontally or vertically on a 2- dimensional plane. The Queen method considers diagonally adjacent locations in addition to the Rook. The spatial weight matrices derived from these methods are Wr and Wq respectively.\n\n\nQueen’s case\nwm_q <- poly2nb(nga_wp, queen=TRUE)\nsummary(wm_q)\n\n\nNeighbour list object:\nNumber of regions: 774 \nNumber of nonzero links: 4440 \nPercentage nonzero weights: 0.7411414 \nAverage number of links: 5.736434 \n1 region with no links:\n86\nLink number distribution:\n\n  0   1   2   3   4   5   6   7   8   9  10  11  12  14 \n  1   2  14  57 125 182 140 122  72  41  12   4   1   1 \n2 least connected regions:\n138 560 with 1 link\n1 most connected region:\n508 with 14 links\n\n\n\n\nRook’s Case\nwm_r <- poly2nb(nga_wp, queen=FALSE)\nsummary(wm_r)\n\n\nNeighbour list object:\nNumber of regions: 774 \nNumber of nonzero links: 4420 \nPercentage nonzero weights: 0.7378029 \nAverage number of links: 5.710594 \n1 region with no links:\n86\nLink number distribution:\n\n  0   1   2   3   4   5   6   7   8   9  10  11  12  14 \n  1   2  14  59 127 181 141 124  66  42  11   4   1   1 \n2 least connected regions:\n138 560 with 1 link\n1 most connected region:\n508 with 14 links\n\n\nBased on the above two summaries, following analysis is done:\n\nTable 2: Comparison of Rook and Queen methods\n\n\nMetrics\nWr\nWq\n\n\n\n\nNo. of regions without links\n1\n1\n\n\nAverage no. of links per region\n5.710\n5.736\n\n\n\nAs all the regions are mentioned by its number, let’s try to find the actual division name.\nThe below code chunk is to find the actual division name of the numbers mentioned\n\n\nFinding neighbours\nnga_wp$shapeName[c(86,138,560,508)]\n\nnb1 <- wm_q[[138]]\nnb1 <- nga_wp$shapeName[nb1]\nnb1\n\nnb2 <- wm_q[[508]]\nnb2 <- nga_wp$shapeName[nb1]\nnb2\n\n\n\nTable 3: Observations\n\n\n\n\n\n\nMetrics\nResults\n\n\n\n\nRegion with no link\nBakassi\n\n\nLeast connected regions\nChanchaga , Offa These two divisions are connected with only 1 division - Bosso\n\n\nMost connected region\nMokwa - is connected with 14 other divisions\n\n\n\nDistance Based Spatial Weights\n\n\nComputing distance based weights\nlongitude <- map_dbl(nga_wp$geometry, ~st_centroid(.x)[[1]])\nlatitude <- map_dbl(nga_wp$geometry, ~st_centroid(.x)[[2]])\ncoords <- cbind(longitude, latitude)\n\nknn4 <- knn2nb(knearneigh(coords, k=4))\nknn6 <- knn2nb(knearneigh(coords, k=6))\nknn7 <- knn2nb(knearneigh(coords, k=7))\nknn8 <- knn2nb(knearneigh(coords, k=8))\n\n\n\nTable 4: Comparison of Knn with different weights\n\n\nMetric\nW(k=4)\nW(k=6)\nW(k=7)\nW(k=8)\n\n\n\n\nAvg. no. of links per region\n6\n6\n7\n8\n\n\n\n\n\nComputing knn8\nlongitude <- map_dbl(nga_wp$geometry, ~st_centroid(.x)[[1]])\nlatitude <- map_dbl(nga_wp$geometry, ~st_centroid(.x)[[2]])\ncoords <- cbind(longitude, latitude)\n\nknn8 <- knn2nb(knearneigh(coords, k=8))\n\n\nBased on the results, k-nearest neighbours method is effective as the average no. of links for W(k=8) is greater than the avg no. of links determined using Queen or Rook’s contiguity based weight matrices.\nJustification\nHowever, k-nearest neighbours method is effective when the distribution of the data varies across the region so that some features are far away from all other features. But it’s not the same in this case. The divisions of Nigeria are so connected that all divisions have shared boundaries except one. So, it is very ideal to use contiguity spatial weights specifcally Queen based as the average no. of links per region is higher than Rook’s to compute weight matrices thereby measuring global spatial autocorrelation.\nComputing Row-standardised weights matrix\nNext, we need to assign weights to each neighboring polygon. The code chunk below uses nb2listw() which supplements a neighbours list with spatial weights for the chosen coding scheme. The coding scheme can be the following:\nB - basic binary coding\nW - row standardised (sums over all links to n)\nC - globally standardised (sums over all links to n)\nU - equal to C divided by the number of neighbours (sums over all links to unity)\nS - variance-stabilizing coding scheme (sums over all links to n).\nThe code chunk below uses “W” as weights coding scheme.\n\n\nComputing distance based weights\nrswm_q <- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nset.ZeroPolicyOption(TRUE)\n\n\n[1] FALSE\n\n\nComputing distance based weights\nrswm_q\n\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 774 \nNumber of nonzero links: 4440 \nPercentage nonzero weights: 0.7411414 \nAverage number of links: 5.736434 \n1 region with no links:\n86\n\nWeights style: W \nWeights constants summary:\n    n     nn  S0       S1       S2\nW 773 597529 773 285.0658 3198.414\n\n\nSimilarly, all other schemes are tested. All their results say average no. of links = 5.736. And so let us go ahead with “W”.\n\n\n5.7 Studying Spatial Autocorrelation at the Global level\nIn this section, we test the hypothesis\nH1 (Alternative Hypothesis) - The distribution of non functional waterpoints is spatially dependent through calculating the Moran’s I statistic and Geary’s C ratio for the data set.\nH0 (Null Hypothesis) - The distribution of non functional waterpoints is is a random phenomenon.\nTo establish the p-values of these statistics, we ran the Monte Carlo simulations for Moran’s I and Geary’s C for each weight matrix for each year with 1000 permutations.\nAs mentioned above, the two measures of spatial autocorrelation that are used in this study\n1) Moran’s I statistic - describe how features differ from the values in the study area as a whole\n2) Geary’s C Ratio - describes how features differ from their immediate neighbors\nMoran’s I Statistic\nGenerally, Moran’s I is calculated as follows:\n\nwhere wij is a spatial weight matrix which compares the closeness between location i and location j\nxi is the no. of non-functional waterpoints in division i\nxj is the no. of non-functional waterpoints in division j\n𝜇 is the average no. of non-functional waterpoints\nN is the total number of division.\nIn the code chunk below, moran.test() of spdep package is used to calculate the Moran’s I scores.\n\n\nComputing Moran’s I\nset.ZeroPolicyOption(TRUE)\n\n\n[1] TRUE\n\n\nComputing Moran’s I\nmoran.test(nga_wp$`wpt non-functional`, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n\n    Moran I test under randomisation\n\ndata:  nga_wp$`wpt non-functional`  \nweights: rswm_q  n reduced by no-neighbour observations\n  \n\nMoran I statistic standard deviate = 20.043, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.433932927      -0.001295337       0.000471516 \n\n\nInsights:\nIt is seen that p value is significant (less than 0.05) and z-score is positive, hence we can reject the null hypothesis and confirm that non-functional water points are not randomly distributed and the spatial distribution of high values and/or low values are more spatially clustered and observations tend to be similar.\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\nThe set. seed() function is used here to create reproducible results when writing code that involves creating variables that take on random values. By using this set. seed() function, we can guarantee that the same random values are produced each time we run the code.\n\n\nMonte-Carlo Moran’s I\nset.seed(3456)\nbpermI= moran.mc(nga_wp$`wpt non-functional`, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbpermI\n\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  nga_wp$`wpt non-functional` \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.43393, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nInsights:\nThis confirms our earlier result even after performing 1000 computations, p value is significant and z score is positive and hence we can reject the null hypothesis and confirm that non-functional water points are not randomly distributed and the spatial distribution of high values and/or low values are more spatially clustered and observations tend to be similar.\nGeary’s C Ratio\nAnother indicator of spatial autocorrelation is the Geary’s contiguity ratio (or Geary’s C). The Geary’s C ratio is based upon a paired comparison of juxtaposed map values and is calculated as follows:\n\nGeary’s C is inversely related to Moran’s I and all the terms calculating C are the same as defined for the Moran’s I. The code chunk below performs Geary’s C test for spatial autocorrelation by using geary.test() of spdep.\n\n\nComputing Geary’s C\ngeary.test(nga_wp$`wpt non-functional`, listw=rswm_q)\n\n\n\n    Geary C test under randomisation\n\ndata:  nga_wp$`wpt non-functional` \nweights: rswm_q \n\nGeary C statistic standard deviate = 14.457, p-value < 2.2e-16\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n     0.6170907765      1.0000000000      0.0007014859 \n\n\nInsights:\nBased on the above result, since the Geary’s C value is less than 1, it is evident that the non-functional waterpoints are clustered and the values tend to be similar.\nThe code chunk below performs permutation test for Geary’s C statistic by using geary.mc() of spdep. As mentioned above for Moran’s I statistic value computation, here also we used set.seed() function to make sure that the same random values are produced each time we run the code.\n\n\nMonte-Carlo Geary’s C\nset.seed(3456)\nbpermC=geary.mc(nga_wp$`wpt non-functional`, \n               listw=rswm_q, \n               nsim=999)\nbpermC\n\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  nga_wp$`wpt non-functional` \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.61709, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\nInsights:\nAfter performing Monte-Carlo simulation of 1000 permutations, it is statistically confirmed that the distribution of all non-functional water points are spatially dependent, clustered and the observations tend to be similar.\nVisualising the results:\nIt is always a good practice for us to the examine the simulated Moran’s I test statistics and Geary’s C statistical values in greater detail. First let us compute the distributions individually and then merge using patchwork\nIn the below code, res values are passed as data in the form of dataframe, geom_histogram() function is used to create the histogram and geom_vline() function is used to draw a vertical line at Moran’s I test statistic value.\n\n\nMoran’s I- Histogram\np1 <- ggplot(data=as.data.frame(bpermI[7]), \n       aes(x= res)) +\n      geom_histogram(bins=20, \n                     color=\"black\", \n                     fill=\"light blue\") +\n      geom_vline(aes(xintercept=0.43215),\n                     color=\"red\", \n                     linetype=\"dashed\", \n                     size=1)+\n      labs(title = \"Moran's I test statistic distribution\",\n       x = \"Moran's I\",\n       y = \"Frequency\")\n\n\nSimilarly, the distribution is observed as density plot using geom_density() function.\n\n\nMoran’s I- DensityPlot\np2 <- ggplot(as.data.frame(bpermI[7]), \n             aes(x=res)) + \n      geom_density() +\n      geom_vline(aes(xintercept=0.43215),\n                     color=\"red\", \n                     linetype=\"dashed\",\n                     size=1) +\n      labs(title = \"Moran's I test statistic distribution\",\n           x = \"Moran's I\",\n           y = \"Density\")\n\n\nThe code chunk below plots the distribution of Geary’s C simulated values and its frequency using geom_histogram() functon.\n\n\nGeary’s C-Histogram\np3 <- ggplot(data=as.data.frame(bpermC[7]), \n             aes(x= res)) +\n      geom_histogram(bins=20, \n                     color=\"black\", \n                     fill=\"light blue\") +\n      geom_vline(aes(xintercept=0.62927),\n                 color=\"red\", \n                 linetype=\"dashed\", \n                 size=1)+\n      labs(title = \"Geary's C test statistic distribution\",\n           x = \"Geary's C\",\n           y = \"Frequency\")\n\n\nFinally, the distribution of Geary’s C simulated values and its frequencies are viewed as density plot using geom_density() function.\n\n\nGeary’s C- Density Plot\np4 <- ggplot(as.data.frame(bpermC[7]), \n             aes(x=res)) + \n      geom_density() +\n      geom_vline(aes(xintercept=0.62927),\n                 color=\"red\", \n                 linetype=\"dashed\",\n                 size=1) +\n      labs(title = \"Geary's C test statistic distribution\",\n           x = \"Geary's C\",\n           y = \"Frequency\")\n\n\nNow, all the distribution plots are merged together as one using\n\npatchwork package - It has a very simple syntax where we can create layouts super easily. The general syntax that combines:\nTwo-Column Layout using the Plus Sign +\nParenthesis () to create a subplot group\nTwo-Row Layout using the Division Sign \\\nplot_annotation() - auto-tagging capabilities, in order to identify subplots in text\ntheme_economist() of ggthemes package - A theme that approximates the style of The Economist\n\n\npatchwork <- ((p1 / p3) | (p2 / p4)) + plot_annotation(tag_levels = 'I')\npatchwork & theme_economist()\n\n\n\n\nInterpretation:\nFrom the subplots I and III, we can understand that actual Moran’s I statistic (indicated as red dashed line) is quite far away from the simulated values towards the right. Hence we can confirm that there is positive autocorrelation among the features.\nSimilarly, from the subplots II and IV, we can understand that that actual Geary’s C statistic (indicated as red dashed line) is quite far away from the simulated values towards the left unlike Moran’s I test statistic value. Both these test values are inversely related. Hence we can confirm that there is positive autocorrelation among the features.\nAnalysis:\nAs we have understood that features are positively auto correlated, we can confirm the following:\nClustering - The spatial distribution of non-functional waterpoints tend to be in similar locations.\nNeighbours are similar - Their neighbours are more alike than they would be under spatial randomness.\nComparison of Moran’s I and Geary’s C spatial correlogram\nLet us examine the patterns of spatial autocorrelation in the data using spatial correlograms. It helps us to understand how correlated are pairs of spatial observations when the distance (lag) between them is increased. Precisely, theses are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.\nIn the code chunk below, sp.correlogram() of spdep package is used to compute a 10-lag spatial correlogram of non-functional waterpoints. The global spatial autocorrelation used in Moran’s I and Geary’s C in subsequent code lines.\nThe plot() of base graph is then used to plot the output and the plots are arranged using par(mfrow()) parameter, here it is 1 row and 2 columns.\n\n\nPlotting spatial correlogram\npar(mfrow=c(1,2))\nMI_corr <- sp.correlogram(wm_q, \n                          nga_wp$`wpt non-functional`,\n                          order=10, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr,\n     main =\"Moran's I error plot\")\n\nGC_corr <- sp.correlogram(wm_q, \n                          nga_wp$`wpt non-functional`, \n                          order=10, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr,\n     main =\"Geary's C error plot\")\n\n\n\n\n\nInterpretation: The plots reveal that the relationship between Moran’s I and Geary’C test statistic values are inverse. Moran’s I values approach -1 whereas Geary’s C test values approach 2 for subsequent lags which indicates that quite dissimilar values tend to form a cluster.\nBy plotting the output might not allow us to provide complete interpretation. This is because not all autocorrelation values are statistically significant. Hence, it is important for us to examine the full analysis report by printing out the analysis results as in the code chunk below.\n\nprint(MI_corr)\n\nSpatial correlogram for nga_wp$`wpt non-functional` \nmethod: Moran's I\n            estimate expectation    variance standard deviate Pr(I) two sided\n1 (773)   4.3393e-01 -1.2953e-03  4.7152e-04          20.0433       < 2.2e-16\n2 (773)   2.6647e-01 -1.2953e-03  2.0206e-04          18.8374       < 2.2e-16\n3 (773)   1.9507e-01 -1.2953e-03  1.2189e-04          17.7863       < 2.2e-16\n4 (773)   1.4019e-01 -1.2953e-03  8.7589e-05          15.1181       < 2.2e-16\n5 (773)   6.3735e-02 -1.2953e-03  6.8779e-05           7.8413       4.459e-15\n6 (773)   2.1698e-02 -1.2953e-03  5.7380e-05           3.0354        0.002402\n7 (773)  -8.8208e-03 -1.2953e-03  5.0377e-05          -1.0603        0.289021\n8 (773)  -4.5305e-02 -1.2953e-03  4.6074e-05          -6.4837       8.952e-11\n9 (773)  -7.6086e-02 -1.2953e-03  4.3585e-05         -11.3287       < 2.2e-16\n10 (773) -7.6594e-02 -1.2953e-03  4.2342e-05         -11.5718       < 2.2e-16\n            \n1 (773)  ***\n2 (773)  ***\n3 (773)  ***\n4 (773)  ***\n5 (773)  ***\n6 (773)  ** \n7 (773)     \n8 (773)  ***\n9 (773)  ***\n10 (773) ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nprint(GC_corr)\n\nSpatial correlogram for nga_wp$`wpt non-functional` \nmethod: Geary's C\n           estimate expectation   variance standard deviate Pr(I) two sided    \n1 (773)  0.61709078  1.00000000 0.00070149         -14.4573       < 2.2e-16 ***\n2 (773)  0.74726091  1.00000000 0.00038571         -12.8689       < 2.2e-16 ***\n3 (773)  0.80953972  1.00000000 0.00027325         -11.5219       < 2.2e-16 ***\n4 (773)  0.86705201  1.00000000 0.00024463          -8.5002       < 2.2e-16 ***\n5 (773)  0.94961811  1.00000000 0.00024087          -3.2462        0.001169 ** \n6 (773)  0.99919645  1.00000000 0.00026273          -0.0496        0.960462    \n7 (773)  1.02833918  1.00000000 0.00032695           1.5673        0.117048    \n8 (773)  1.05591178  1.00000000 0.00040558           2.7763        0.005499 ** \n9 (773)  1.09586502  1.00000000 0.00045556           4.4915       7.074e-06 ***\n10 (773) 1.08793934  1.00000000 0.00039883           4.4034       1.066e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInterpretation:\nBoth Moran’s I and Geary’s C test results show that at 90% confidence, (alpha value = 0.01), most of the autocorrelated values are significant. And hence we can reject the null hypothesis stating the feature are random and with statistical evidences, we can confirm the following\n\nObserved spatial pattern of non-functional waterpoint values is spatially dependent and not equally likely as any other spatial pattern.\nNon-functional waterpoint values at one location depends on values at other (neighbouring) locations.\nAny non-functional waterpoint values altered at one location will affect the other location values.\n\n\n\n5.8 Studying Spatial Autocorrelation at the Local Level\nWhile the global Moran’s I score and the Geary’s C ratio can tell us whether non-functional waterpoints tend to cluster or not on the map, it does not provide any information on the distribution of spatial dependence of non-functional waterpoints and it is unable to identify the location of hotspots and clusters. Let us apply appropriate Local Indicators for Spatial Association (LISA), especially local Moran’I to detect cluster and/or outlier from non-functional waterpoints of Nigeria.\nComputing & Mapping Local Moran’s test statistic I and p-value\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\nThe code chunks below are used to compute local Moran’s I at shape level (here, divisions)\n\n\nComputing Local Moran\nfips <- order(nga_wp$shapeName)\nlocalMI <- localmoran(nga_wp$`wpt non-functional`,rswm_q)\nhead(localMI)\n\n\n           Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.32365786 -9.995243e-04 1.924638e-01 -0.73547576     0.46204980\n2  0.07000542 -4.092463e-05 1.053077e-02  0.68258288     0.49487045\n3  1.25819985 -1.627684e-03 4.181728e-01  1.94819847     0.05139122\n4 -0.03537489 -5.427505e-05 5.954304e-03 -0.45773361     0.64714384\n5  0.01201533 -2.590965e-04 3.988998e-02  0.06145673     0.95099547\n6  0.00768085 -1.538445e-07 1.687859e-05  1.86960486     0.06153871\n\n\nlocalmoran() function returned a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(): the p-value of local moran statistic\n\nLet us rename the p values column from Pr(z != E(Ii)) to Pr.Ii for ease of use.\n\nnga_wp.localMI <- cbind(nga_wp,localMI) %>%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\nLet us now map both local Moran’s I and p values Using choropleth mapping functions of tmap package for effective interpretation.\n\n\nMapping Local Moran statistocs & p values\nlocalMI.map <- tm_shape(nga_wp.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"Local Moran's I values\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Local Moran statistics \",\n            main.title.size = 1.5,\n            main.title.position = \"center\",\n            main.title.fontface = \"bold\")+\n  tm_compass(type=\"8star\",\n             position=c(\"right\", \"top\"))\n\npvalue.map <- tm_shape(nga_wp.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"Local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)+\n  tm_layout(main.title = \"Local Moran's p values\",\n            main.title.size = 1.5,\n            main.title.position = \"center\",\n            main.title.fontface = \"bold\")+\n  tm_compass(type=\"8star\",\n             position=c(\"right\", \"top\"))\n\ntmap_arrange(localMI.map, pvalue.map, asp=1, ncol=2)\n\n\n\n\n\nInterpretation:\nFrom the left map, we can understand the following\n\nCluster - All positive valued region have neighboring features with similarly high or low attribute values\nOutlier - All negative valued regions have neighboring features with dissimilar values\n\nFrom the right map, we can understand the following\n\nFor every corresponding I value, we can locate the regions which are significant or not.\nDarker the region, higher the significance.\n\n\nsignificant_regions <- nga_wp.localMI %>%\n             filter(Pr.Ii <= 0.05)\ndim(significant_regions)\n\n[1] 189  17\n\n\nIt is seen that there are 189 significant regions. For better understanding, let us plot to study only the divisions which are significant.\n\n\nMapping significant regions\nlocalMI.signfmap <- \n  tm_shape(nga_wp.localMI) +\n  tm_fill(\"white\") +\n  tm_borders(alpha = 0.5) +\n  tm_shape(nga_wp.localMI %>%\n             filter(Pr.Ii <= 0.05)) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"Local Moran's I values\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Local Moran's I-significant values \",\n            main.title.size = 1,\n            main.title.position = \"center\",\n            main.title.fontface = \"bold\")+\n  tmap_mode(\"view\")\n\nlocalMI.signfmap\n\n\n\n\n\n\n\nInsights:\nFrom our analysis, we see that 189 regions are significant.\n\nDivisions like Zaria, Sabon-Gari in the central region, Oyun in the west have most similar features as they are highly positive.\nDivisions like Alkaleri, Ibi, Awe in the eastern region, Zango-Kataf, Chikun, Kajuru in the central region, Ijebu East, Orhionmwon in the southwest region having negative Moran’s I values indicate that they have dissimilar features.\n\n\n\n5.9 Studying LISA Cluster Map\nLet us now use LISA Cluster Map to visualise the significant locations color coded by type of spatial autocorrelation. Let us first plot Moran scatterplot to generate the LISA cluster map.\nMoran Scatterplots\nThe Moran Scatterplot allows us to study the local spatial instability of the distribution of functional and non-functional waterpoints in Nigeria. The spdep package provides the moran.plot() function to help us to plot the Moran Scatterplot. The various regions are distributed across the scatterplot, with spatially lagged values of non-functional waterpoints in these regions plotted on the y-axis against the original values of non-functional waterpoints on the x-axis.\nBefore plotting, let’s scale the variable for standardization for both functional and non-functional.\n\n\nStandardization\nnga_wp$z.f <- scale(nga_wp$`wpt functional`) %>% \n  as.vector\n\nnga_wp$z.nf <- scale(nga_wp$`wpt non-functional`) %>% \n  as.vector\n\n\nThe Moran scatterplot is divided into four areas, with each quadrant corresponding with one of four categories:\n\nHigh-High (HH) in the top-right quadrant;\nHigh-Low (HL) in the bottom right quadrant;\nLow-High (LH) in the top-left quadrant;\nLow-Low (LL) in the bottom left quadrant.\n\n\n\nMoran Scatterplot\npar(mfrow=c(2,1))\n\nncif <- moran.plot(nga_wp$z.f, rswm_q,\n                   labels=as.character(nga_wp$shapeName),\n                   xlab=\"Z-functional waterpoints\", \n                   ylab=\"Spatially Lag Z-functional waterpoints\")\n\nnci_nf <- moran.plot(nga_wp$z.nf, rswm_q,\n                   labels=as.character(nga_wp$shapeName),\n                   xlab=\"Z-Non-functional waterpoints\", \n                   ylab=\"Spatially Lag Z-Non-functional waterpoints\")\n\n\n\n\n\nInterpretation:\nFrom subplot 1, we understand functional water points are located in the Low-Low quadrant which indicates that they are associated with relatively low values in surrounding locations.Similarly, from subplot 2, we understand that non-functional waterpoints are all over place in all the four quadrants although there is an edge in the Low-Low quadrant like the functional waterpoints.\nThe direction and magnitude of global autocorrelation is observed in a Moran Scatterplot, as the slope of the linear regression of the lagged values of waterpoints vs the original frequencies of waterpoints is equivalent to the Moran’s I score. However, we are not able to see which of these areas are significant. For that, we perform LISA analysis.\nThe code chunk below prepares a LISA cluster map.\n\nDerive the spatially lagged variable of interest (i.e. GDPPC) and centers the spatially lagged variable around its mean.\nCentering the local Moran’s around the mean.\nSet a statistical significance level for the local Moran.\nDefine the four categories low-low (1), low-high (2), high-low (3) and high-high categories.\nLastly, places non-significant Moran in the category 0.\n\n\n\nPreparing Data - LISA (Functional)\nlocalMIf <- localmoran(nga_wp$`wpt functional`,rswm_q)\nnga_wp.localMIf <- cbind(nga_wp,localMIf) %>%\n  rename(Pr.Ii = Pr.z....E.Ii..)\nquadrantf <- vector(mode=\"numeric\",length=nrow(localMIf))\nnga_wp$lag_functional <- lag.listw(rswm_q, nga_wp$`wpt functional`)\nDVf <- nga_wp$lag_functional - mean(nga_wp$lag_functional)     \nLM_If <- localMIf[,1]   \nsigniffunc <- 0.05       \nquadrantf[DVf <0 & LM_If>0] <- 1\nquadrantf[DVf >0 & LM_If<0] <- 2\nquadrantf[DVf <0 & LM_If<0] <- 3  \nquadrantf[DVf >0 & LM_If>0] <- 4    \nquadrantf[localMIf[,5]>signiffunc] <- 0\n\n\n\n\nPreparing Data - LISA (Non-Functional)\nquadrant <- vector(mode=\"numeric\",length=nrow(localMI))\nnga_wp$lag_nonfunctional <- lag.listw(rswm_q, nga_wp$`wpt non-functional`)\nDV <- nga_wp$lag_nonfunctional - mean(nga_wp$lag_nonfunctional)     \nLM_I <- localMI[,1]   \nsignif <- 0.05       \nquadrant[DV <0 & LM_I>0] <- 1\nquadrant[DV >0 & LM_I<0] <- 2\nquadrant[DV <0 & LM_I<0] <- 3  \nquadrant[DV >0 & LM_I>0] <- 4    \nquadrant[localMI[,5]>signif] <- 0\n\n\nNow let us build and plot the LISA map by using the code chunks below for both functional and non-functional water points.\n\n\nLISA Map(Functional)\nnga_wp.localMIf$quadrant <- quadrantf\ncolors <- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmapf <- tm_shape(nga_wp.localMIf) +\n            tm_fill(col = \"quadrant\", \n                    style = \"cat\", \n                    palette = colors[c(sort(unique(quadrantf)))+1], \n                    labels = clusters[c(sort(unique(quadrantf)))+1])+\n                    #popup.vars = c(\"\")) +\n            tmap_mode(\"plot\")+\n            tm_view(set.zoom.limits = c(11,17)) +\n            tm_borders(alpha=0.5)+\n            tm_compass(type=\"8star\",\n                       position=c(\"right\", \"top\"))+\n            tm_layout(main.title = \"LISA Map (Functional)\",\n                      main.title.fontface = \"bold\",\n                      main.title.position = \"center\",\n                      legend.height = 1, \n                      legend.width = 1,\n                      legend.text.size = 1,\n                      legend.title.size = 1,)\n\n\n\n\nLISA Map(Non-Functional)\nnga_wp.localMI$quadrant <- quadrant\ncolors <- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nLISAmap <- tm_shape(nga_wp.localMI) +\n           tm_fill(col = \"quadrant\", \n                   style = \"cat\", \n                   palette = colors[c(sort(unique(quadrant)))+1], \n                   labels = clusters[c(sort(unique(quadrant)))+1])+\n                   #popup.vars = c(\"\")) +\n           tmap_mode(\"plot\")+\n           tm_view(set.zoom.limits = c(11,17)) +\n           tm_borders(alpha=0.5)+\n           tm_compass(type=\"8star\",\n                      position=c(\"right\", \"top\"))+\n           tm_layout(main.title = \"LISA Map (Non-functional)\",\n                     main.title.fontface = \"bold\",\n                     main.title.position = \"center\",\n                     legend.height = 1, \n                     legend.width = 1,\n                     legend.text.size = 1,\n                     legend.title.size = 1)\n\n\nLet us plot both LISA maps of functional and non-functional waterpoints next to each other for better interpretation.\n\ntmap_arrange(LISAmapf, LISAmap, \n             asp=1, ncol=2)\n\n\n\n\nInterpretation:\nWe can see that spatial distribution of functional waterpoints are positively autocorrelated in the northern region i.e. those are associated with relatively high values of the surrounding locations.\nOn the other hand, spatial distribution of non-functional waterpoints are positively autocorrelated in parts of south east, west and central part of the nation.\n\n\n5.10 Hot Spot and Cold Spot Area Analysis\nIn the previous sections, we have detected cluster and outliers for the spatial distribution of non-functional water points. In this section let us detect hot spot and/or cold spot areas using localised spatial statistics.\nGetis and Ord’s G-Statistics\nIt is an alternative spatial statistics to detect spatial anomalies. Here, statistically significant hot-spots are recognised as areas of high values where other areas within a neighbourhood range also share high values too.\nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\n\nDeriving spatial weight matrix\nIn section 5.6, after trying multiple spatial weights, we have concluded that (adaptive weight) K-nearest neighbour is effective and so we are computing spatial weight matrix using nb2listw() which is used to convert the nb object into spatial weights object.\n\n\nSpatial Weight\nknn8\n\n\nNeighbour list object:\nNumber of regions: 774 \nNumber of nonzero links: 6192 \nPercentage nonzero weights: 1.033592 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nSpatial Weight\nknn_lw <- nb2listw(knn8, style = 'W')\n\n\nComputing Gi statistics\nThe code chunk below is used to compute the Gi values for both functional and non-functional waterpoints by using an adaptive distance weight matrix.\n\n\nGi Statistics\nfips <- order(nga_wp$shapeName)\ngi.nfadaptive <- localG(nga_wp$`wpt non-functional`, knn_lw)\nnga_nfwp.gi <- cbind(nga_wp, as.matrix(gi.nfadaptive)) %>%\n  rename(gstatnf_adaptive = as.matrix.gi.nfadaptive.)\n\n\ngi.fadaptive <- localG(nga_wp$`wpt functional`, knn_lw)\nnga_fwp.gi <- cbind(nga_wp, as.matrix(gi.fadaptive)) %>%\n  rename(gstatf_adaptive = as.matrix.gi.fadaptive.)\n\n\nMapping Gi statistics\nLet us visualise the locations of hot spot and cold spot areas of functional and non-functional waterpoints acroos the country. The choropleth mapping functions of tmap package will be used to map the Gi values.\nThe code chunk below shows the functions used to map the Gi values derived using adaptive distance weight matrix.\n\n\nGi Map-Functional\nGimap_f <- tm_shape(nga_fwp.gi) + \n           tm_fill(col = \"gstatf_adaptive\", \n           style = \"pretty\", \n           palette=\"-RdBu\", \n           title = \"local Gi\") + \n           tm_borders(alpha = 0.5)+\n           tm_layout(main.title = \"Gi Map (Functional)\",\n                     main.title.fontface = \"bold\",\n                     main.title.position = \"center\",\n                     legend.height = 1, \n                     legend.width = 1,\n                     legend.text.size = 1,\n                     legend.title.size = 1,)+\n           tm_compass(type=\"8star\",\n                      position=c(\"right\", \"top\"))\n\n\n\n\nGi Map-NonFunctional\nGimap_nf <- tm_shape(nga_nfwp.gi) + \n            tm_fill(col = \"gstatnf_adaptive\", \n                    style = \"pretty\", \n                    palette=\"-RdBu\", \n                    title = \"local Gi\") + \n            tm_borders(alpha = 0.5)+\n            tm_layout(main.title = \"Gi Map (Non-Functional)\",\n                      main.title.fontface = \"bold\",\n                      main.title.position = \"center\",\n                      legend.height = 1, \n                      legend.width = 1,\n                      legend.text.size = 1,\n                      legend.title.size = 1,)+\n            tm_compass(type=\"8star\",\n                       position=c(\"right\", \"top\"))\n\n\nLet us plot both the maps next to each other for better understanding using tmap_arrange() function.\n\ntmap_arrange(Gimap_f, Gimap_nf,\n             asp=1,ncol=2)\n\n\n\n\nInterpretation:\nPlotting Gi map for functional and non-functional waterpoints help us to understand that functional points are associated with relatively high values of the surrounding locations (clustered- hotspot areas) and non-functional water points are associated with relatively low values in surrounding locations (scattered - coldspot) in northern region of Nigeria. Similarly, functional points are more scattered and non-functional points are more clustered in south eastern region of Nigeria.\n\n\n\n6. Key Takeaway\n\ntmap_arrange(LISAmap, Gimap_nf,\n             asp=1,ncol=2)\n\n\n\n\nThroughout the study, we have come across lot of observations and interpretations from graphs, charts and maps. To conclude, based on the objective of this study, focusing on spatial pattern of non-functional waterpoints, both LISA map and GI Map reveals that parts of southeastern regions, parts of west and central region which are darker in both the maps indicate that they are associated with relatively high values of the surrounding locations i.e. clustered and majority of the eastern region doesn’t have any waterpoints (unknown). These are the observations which are matter of concern and has to be taken care of.\n\n\n7. Conclusion\nThe study of the spatial distribution of waterpoints in Nigeria using the approach of Global and Local measures of Autocorrelation had generated much insights regarding the waterpoints distribution. The LISA analysis in particular helped to locate statistically significant hotspots of waterpoints more rigorously than choropleths maps.Such geospatial analytics methods are quite important supplement to traditional analysis as the latter only provides the locational context not the analytical context.\n\n\n8. References\n\n8.1 Geovisualisation and Geocommunication\nUganda Water Point Analysis\nTiefelsdorf, M., Griffith, D. A., Boots, B. 1999 A variance-stabilizing coding scheme for spatial link matrices, Environment and Planning A, 31, pp. 165-180\nComparison of Equal and Quantile style classification\nExploring and visualizing household electricity consumption patterns in Singapore: A geospatial analytics approach\nIs there space for violence?: A data-driven approach to the exploration of spatial-temporal dimensions of conflict\n\n\n8.2 Editing Quarto Document & Data Wrangling\nFormatting Tables\nHTML Formatting code blocks\nExploratory Spatial Data Analysis\nFormer Student Reference"
  },
  {
    "objectID": "TakeHomeEx/ex2.html",
    "href": "TakeHomeEx/ex2.html",
    "title": "Why regionalisation is vital? Clustering of multivariate Water point attributes in Nigeria",
    "section": "",
    "text": "2. Objective\nThe main aim of the study is to delineate homogeneous region of Nigeria by using geographically referenced multivariate waterpoint attributes data. Some of the major analysis include\n\nhierarchical cluster analysis\nk means cluster analysis\nspatially constrained cluster analysis using SKATER approach\nspatially constrained cluster analysis using ClustGeo method\n\n\n\n3. Glimpse of Steps\nSome of the important steps performed in this study are as follows\n\nImporting shapefile into R using sf package.\nDeriving the proportion of functional and non-functional water point at LGA level using appropriate tidyr and dplyr methods.\nCombining the geospatial and aspatial data frame into simple feature data frame.\nDelineating water point measures functional regions by using conventional hierarchical clustering.\nDelineating water point measures functional regions by using spatially constrained clustering algorithms.\nThematic Mapping - Plotting maps to show the water points measures derived by using appropriate statistical graphics and choropleth mapping technique.\nAnalytical Mapping - Plotting functional regions delineated by using both non-spatially constrained and spatially constrained clustering algorithms.\n\n\n\n4. Data\nFor this study, data from WPdx Global Data Repositories and geoBoundaries are used which are in csv and shape file formats respectively. These datasets provide information about waterpoints and Nigeria’s Administrative boundary shape file.\n\nTable1: Datasets Used\n\n\nData Type\nDescription\nFormat\nSource\n\n\n\n\nGeospatial\nNigeria Level-2 Administrative Boundary\nShape file\nGeoboundaries\n\n\nAspatial\nWater point related data on WPdx standard\ncsv\nWaterpoint access data\n\n\n\n\n\n5. Deep Dive into Geospatial Analysis\nLet us try to delineate homogeneous region of Nigeria by using geographically referenced multivariate waterpoint attributes data with sptially constrained and non-spatially constrained clustering methods\n\n5.1 Loading packages\nLet us first load required packages into R environment. p_load function pf pacman package is used to install the following packages into R environment.\n\nsf, rgdal and spdep - Spatial data handling\ntidyverse, especially readr, ggplot2 and dplyr - Attribute data handling\ntmap - Choropleth mapping\ncoorplot, ggpubr, and heatmaply - Multivariate data visualisation and analysis\ncluster, ClustGeo - Cluster analysis\npatchwork, ggthemes - Effective visualisation and layouts\n\n\n\nLoading Packages\npacman::p_load(sf, tidyverse, tmap, corrplot, \n               psych, ggpubr, cluster, factoextra,\n               heatmaply, spdep, ClustGeo,\n               rgdal, NbClust, GGally, patchwork, ggthemes, knitr)\n\n\n\n\n5.2 Importing Geospatial Data\nNow let us import geospatial data. The code chunk below uses st_read() function of sf package to import geoBoundaries-NGA-ADM2 shapefile into R environment.\nTwo arguments are used :\n\ndsn - destination : to define the data path\nlayer - to provide the shapefile name\ncrs - coordinate reference system in epsg format. EPSG: 4326 is wgs84 Geographic Coordinate System\n\n\n\nImporting geospatial data\nnga <- st_read(dsn = \"data2/geospatial\",\n               layer = \"geoBoundaries-NGA-ADM2\",\n               crs = 4326)\n\n\n\n\n5.3 Importing Spatial Data\nNow let us import spatial data. Since water point data set is in csv file format, we will use read_csv() of readr package to import Water_Point_Data_Exchange.csv as shown the code chunk below. The output R object is called listings and it is a tibble data frame.\n\nfilter() of dplyr package is used to extract water point records of Nigeria.\n\n\n\nImporting aspatial data\nwp_nga <- read_csv(\"data2/aspatial/Water_Point_Data_Exchange.csv\") %>%\n  filter(`#clean_country_name` == \"Nigeria\")\n\n\n\n\n5.4 Creating Simple feature data frame from an aspatial and geospatial data frame\n\n\nConverting Projection\nwp_nga_sf <- st_as_sf(wp_nga, \n                       coords = c(\"#lat_deg\", \"#lon_deg\"),\n                       crs=4326) %>%\n  st_transform(crs = 26391)\n\n\ncrs argument is to provide the coordinates system in epsg format. EPSG: 4326 is wgs84 Geographic Coordinate System EPSG: 26391 is Nigeria’s SVY21 Projected Coordinate System\n\n\nConverting Projection\nnga_sf <- st_transform(nga,crs = 26391)\n\n\nst_transform() of sf package is used to reproject nga from one coordinate system(WGS 84) to another coordinate system(ESPG) mathemetically.\n\n\n5.5 Data Wrangling\nBefore finalising the data, we have to perform some data cleaning\n5.5.1 Checking for duplicate values\nThanks to our classmate, who have identified the right location for duplicate values with the help of google map data. Reference link\nduplicated() function returns a logical vector where TRUE specifies which elements of a vector or data frame are duplicates.\nfilter() function filters those duplicates and lets check what are the duplicated shape Names\n\n\nChecking duplicates\nduplicated_area <- nga_sf %>% \n  mutate(dup_shapeName = duplicated(shapeName)) %>% \n  filter(dup_shapeName)\nduplicated_area$shapeName\n\n\nThe below chunk assigns the right shape names corresponding to index values\n\n\nAssigning right shape names\nnga_sf$shapeName[c(94,95,304,305,355,356,519,546,547,693,694)] <- \n  c(\"Bassa (Kogi)\",\"Bassa (Plateau)\",                                                  \"Ifelodun (Kwara)\",\"Ifelodun (Osun)\",                                              \"Irepodun (Kwara)\",\"Irepodun (Osun)\",                                              \"Nassarawa\",\"Obi (Benue)\",\"Obi(Nasarawa)\",                                         \"Surulere (Lagos)\",\"Surulere (Oyo)\")\n\n\nNow let’s run the same code again and check if there are any duplicates.\n\n\nRechecking duplicate values\nduplicated_area <- nga_sf %>% \n  mutate(dup_shapeName = duplicated(shapeName)) %>% \n  filter(dup_shapeName)\nduplicated_area$shapeName\n\n\nThe output is zero which means there are no duplicate values now.\n\n\n5.5.2 Combining geospatial and aspatial data\nLet us now use a geoprocessing function (or commonly know as GIS analysis) called point-in-polygon overlay to transfer the attribute information in nga sf data frame into wp_sf data frame using st_join()function.\n\n\ncombining data\nwp_sf <- st_join(wp_nga_sf, nga_sf)\n\n\n\n\n5.5.3 Measures used for regionalisation\nBased on the WPdx standard, following measures are chosen,\n\n\n\n\n\n\n\n\nMeasure\ncol_id\nDescription\n\n\n\n\nTotal number of functional water points\n#status_clean\nNo. of functional waterpoints are derived from this categorised column\n\n\nTotal number of nonfunctional water points\n#status_clean\nNo. of non- functional waterpoints are derived from this categorised column\n\n\nPercentage of functional water points\n#status_clean\nPercentage of functional waterpoints are derived from the already derived columns\n\n\nPercentage of non-functional water points\n#status_clean\nPercentage of non-functional waterpoints are derived from the already derived columns\n\n\nPercentage of main water point technology\n#water_tech_clean\nDescribe the system being used to transport the water from the source to the point of collection (e.g. Handpump)\n\n\nPercentage of usage capacity\nusage_cap\nRecommended maximum users per water point. For eg. 250 people per tap [tapstand, kiosk, rainwater catchment] 500 people per hand pump.\n\n\nPercentage of rural water points\nis_urban\nIs in an urban area as defined by EU Global Human Settlement Database TRUE/FALSE - urban/rural\n\n\nPercentage of crucial waterpoints\ncrucialness_score\ncrucialness score shows how important the water point would be if it were to be rehabilitated.\n\n\nPercentage of stale waterpoints\nstaleness_score\nThe staleness score represents the depreciation of the point’s relevance. Varies between 0 and 100. Higher the value more the staleness.\n\n\n\n\n\n5.5.4 Renaming the column names\nLet us rename some of the column names which begins with # for ease of use by using rename() function\n\n\nRenaming columns\nwp_sf <-wp_sf %>% \n  rename (\"water_tech\" = \"#water_tech_clean\") %>%\n  rename (\"status_clean\" = \"#status_clean\") %>%\n  rename (\"status_id\" = \"#status_id\")\n\n\n\n\n5.5.5 Replacing NA values\nNow we are replacing the NA values in the status_clean column by Unknown\n\n\nReplacing NA\nwp_nga <- wp_sf  %>%\n  mutate(status_cle = replace_na(status_clean, \"Unknown\"))\n\n\n\n\n5.5.5 Recoding the values\nThere are water technology values mentioned along with the manufacturer like India Mark III, Afridev. So lets recode all those manufacturers into one class as Hand Pump using recode() function\n\n\nRecoding handpumps\nwp_nga <-wp_nga %>% \nmutate(wpt_tech=recode(water_tech, \n                     'Hand Pump - Rope Pump'='Hand Pump',\n                     'Hand Pump - India Mark III'='Hand Pump',\n                     'Hand Pump - Afridev'='Hand Pump',\n                     'Hand Pump'='Hand Pump',\n                     'Hand Pump - India Mark II'='Hand Pump',\n                     'Hand Pump - Mono'= 'Hand Pump'))\n\n\n\n\n5.5.6 Extracting Funtional, Non-Functional and Unknown water points\nAs our objective is to focus on waterpoints, let us extract the three types and save it as a dataframe for further analysis\n\n\nExtracting waterpoints\nwpt_functional <- wp_nga %>%\n  filter(status_cle %in%\n           c(\"Functional\", \n             \"Functional but not in use\",\n             \"Functional but needs repair\"))\n\nwpt_nonfunctional <- wp_nga %>%\n  filter(status_cle %in%\n           c(\"Abandoned/Decommissioned\", \n             \"Abandoned\",\n             \"Non-Functional\",\n             \"Non functional due to dry season\",\n             \"Non-Functional due to dry season\"))\n\nwpt_unknown <- wp_nga %>%\n  filter(status_cle == \"Unknown\")\n\nhandpump <- wp_nga %>%\n  filter(wpt_tech == \"Hand Pump\")\n\nusecap_lessthan1000 <- wp_nga %>%\n  filter(usage_capacity < 1000)\n\nusecap_greatthan1000 <- wp_nga %>%\n  filter(usage_capacity >= 1000)\n\nwpt_rural <- wp_nga %>%\n  filter(is_urban == FALSE)\n\ncrucial_score <- wp_nga %>%\n  filter(crucialness_score >= 0.5)\n\nstale_score <- wp_nga %>%\n  filter(staleness_score >= 50)\n\n\n\n\n5.5.7 Computing absolute numbers\nWe have to perform 2 steps to calculate the absolue numbers of waterpoint attributes in each division.\n\nLet us identify no. of waterpoints located inside each division by using st_intersects().\nNext, let us calculate numbers of pre-schools that fall inside each division by using length() function.\n\n\n\nComputing numbers\nnga_wp <- nga_sf %>% \n  mutate(total_wpt = lengths(\n    st_intersects(nga_sf, wp_nga))) %>%\n  mutate(wpt_functional = lengths(\n    st_intersects(nga_sf, wpt_functional))) %>%\n  mutate(wpt_nonfunctional = lengths(\n    st_intersects(nga_sf, wpt_nonfunctional))) %>%\n  mutate(wpt_unknown = lengths(\n    st_intersects(nga_sf, wpt_unknown)))%>%\n  mutate(handpump = lengths(\n    st_intersects(nga_sf, handpump)))%>%\n  mutate(cap_lessthan1000 = lengths(\n    st_intersects(nga_sf, usecap_lessthan1000)))%>%\n  mutate(cap_greatthan1000 = lengths(\n    st_intersects(nga_sf, usecap_greatthan1000)))%>%\n  mutate(wpt_rural = lengths(\n    st_intersects(nga_sf, wpt_rural)))%>%\n  mutate(wpt_crucial = lengths(\n    st_intersects(nga_sf, crucial_score)))%>%\n  mutate(wpt_stale = lengths(\n    st_intersects(nga_sf, stale_score)))\n\n\n\n\n5.5.8 Computing Proportion\nNow, let us calculate what is the overall proportion with respect to the total no. of waterpoints\n\n\nComputing proportion\nnga_wp_clus <- nga_wp %>%\n  mutate(pct_functional = wpt_functional/total_wpt) %>%\n  mutate(pct_nonfunctional = wpt_nonfunctional/total_wpt) %>%\n  mutate(pct_handpump = handpump/total_wpt) %>%\n  mutate(pct_capless1000 = cap_lessthan1000/total_wpt) %>%\n  mutate(pct_capmore1000 = cap_greatthan1000/total_wpt) %>%\n  mutate(pct_rural = wpt_rural/total_wpt) %>%\n  mutate(pct_crucial = wpt_crucial/total_wpt) %>%\n  mutate(pct_stale = wpt_stale/total_wpt) \n\n\n\n\nSaving the final rds file\nIn order to manage the storage data efficiently, we are saving the final data frame in rds format.\n\n\nSaving files\nwrite_rds(nga_wp_clus, \"data2/nga_wp_clus.rds\")\nwrite_rds(wpt_functional, \"data2/wpt_functional.rds\")\nwrite_rds(wpt_nonfunctional, \"data2/wpt_nonfunctional.rds\")\nwrite_rds(wp_nga, \"data2/wp_nga.rds\")\n\n\n\n\n5.6 Exploratory Data Analysis\nBefore performing spatial analysis, let us first do some preliminary data analysis to understand the data better in terms of water points.\n\n\n5.6.1 What is the proportion of functional and non-functional water points?\nBefore visualising, its important for us to prepare the data. Based on WPDx Data Standard, the variable ‘#status_id’ refers to Presence of Water when Assessed. Binary response, i.e. Yes/No are recoded into Functional / Not-Functional.\n\n\nPreparing the data\nwp_nga <- read_rds(\"data2/wp_nga.rds\")\n\nngawater_sf<-wp_nga %>%\n  mutate(status_id=\n                case_when(status_id==\"Yes\"~\"Functional\",\n                          status_id==\"No\"~\"Non-Functional\",\n                          status_id== \"Unknown\"~\"unknown\"))\n\n\n\n\nProportion graph\nggplot(data= ngawater_sf, \n       aes(x= status_id)) +\n       geom_bar(fill= '#CD5C5C') +\n       #ylim(0, 150) +\n       geom_text(stat = 'count',\n           aes(label= paste0(stat(count), ', ', \n                             round(stat(count)/sum(stat(count))*100, \n                             1), '%')), vjust= -0.5, size= 2.5) +\n       labs(y= 'No. of \\nwater points',\n            x= 'Water Points',\n            title = \"Distribution of water points\") +\n       theme(axis.title.y= element_text(angle=0), \n             axis.ticks.x= element_blank(),\n             panel.background= element_blank(),\n             axis.line= element_line(color= 'grey'),\n             axis.title.y.left = element_text(vjust = 0.5),\n             plot.title = element_text(hjust=0.5))\n\n\n\n\n\nInsights:\nNigeria consists of almost 55% of functional, 34% of non-functional and 11% of unknown waterpoints.\n\n\n5.6.2 What is the district wise proportion of water points?\nTo find out the solution for this question, first let’s prepare the data accordingly:\n\nCreate separate dataframe for each plot\nArrange it in descending order by the count values using arrange() function\nSelect top 10 divisions using slice_head() function\n\n\n\nPreparing data\nnga_wp_clus <- read_rds(\"data2/nga_wp_clus.rds\")\n\ntop10_crucial <- nga_wp_clus %>%\n  st_set_geometry(NULL) %>%\n  arrange(desc(wpt_crucial)) %>%\n  slice_head(n=10)\n\ntop10_rural <- nga_wp_clus %>%\n  st_set_geometry(NULL) %>%\n  arrange(desc(wpt_rural)) %>%\n  slice_head(n=10)\n\ntop10_stale <- nga_wp_clus %>%\n  st_set_geometry(NULL) %>%\n  arrange(desc(wpt_stale)) %>%\n  slice_head(n=10)\n\ntop10_handpump <- nga_wp_clus %>%\n  st_set_geometry(NULL) %>%\n  arrange(desc(handpump)) %>%\n  slice_head(n=10)\n\n\n\n\nTop 10 crucial\ntp1 <- ggplot(data = top10_crucial,\n       aes(y = reorder(shapeName,wpt_crucial),\n           x= wpt_crucial)) + \n  geom_bar(stat = \"identity\",\n           fill = \"coral\")+\n  labs(x='No. of crucial water points',\n       y=\" \",\n       title=\"Top 10 divisions with crucial waterpoints\") +\n  geom_text(stat='identity', aes(label=paste0(wpt_crucial)),hjust=-0.5)+\n  theme(axis.title.y=element_text(angle=0),\n                                  \n        axis.ticks.x=element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_line(color='grey'),\n        plot.title = element_text(hjust = 0.5,\n                                  size = 20),\n        axis.title.y.left = element_text(vjust = 0.5), \n        axis.text = element_text(face=\"bold\",\n                                  size=15))\n\n\n\n\nTop 10 rural\ntp2 <- ggplot(data = top10_rural,\n       aes(y = reorder(shapeName,wpt_rural),\n           x= wpt_rural)) + \n  geom_bar(stat = \"identity\",\n           fill = \"hotpink\")+\n  labs(x='No. of rural water points',\n       y=\" \",\n       title=\"Top 10 divisions with rural waterpoints\",) +\n  geom_text(stat='identity', aes(label=paste0(wpt_rural)),hjust=-0.5)+\n  theme(axis.title.y=element_text(angle=0),\n        axis.ticks.x=element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_line(color='grey'),\n        plot.title = element_text(hjust = 0.5,\n                                  size = 20),\n        axis.title.y.left = element_text(vjust = 0.5), \n        axis.text = element_text(face=\"bold\",\n                                 size=15) )\n\n\n\n\nTop 10 stale\ntp3 <- ggplot(data = top10_stale,\n       aes(y = reorder(shapeName,wpt_stale),\n           x= wpt_stale)) + \n  geom_bar(stat = \"identity\",\n           fill = \"red\")+\n  labs(x='No. of stale water points',\n       y=\" \",\n       title=\"Top 10 divisions with stale waterpoints\",) +\n  geom_text(stat='identity', aes(label=paste0(wpt_stale)),hjust=-0.5)+\n  theme(axis.title.y=element_text(angle=0),\n        #axis.title.y=element_blank(),\n        axis.ticks.x=element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_line(color='grey'),\n        plot.title = element_text(hjust = 0.5,\n                                  size = 20),\n        axis.title.y.left = element_text(vjust = 0.5), \n        axis.text = element_text(face=\"bold\",\n                                 size=15))\n\n\n\n\nTop 10 - handpumps\ntp4 <- ggplot(data = top10_crucial,\n       aes(y = reorder(shapeName,handpump),\n           x= handpump)) + \n  geom_bar(stat = \"identity\",\n           fill = \"yellow\")+\n  labs(x='No. of handpump water points',\n       y=\" \",\n       title=\"Top 10 divisions with handpump waterpoints\",) +\n  geom_text(stat='identity', aes(label=paste0(handpump)),hjust=-0.5)+\n  theme(axis.title.y=element_text(angle=0), \n        axis.ticks.x=element_blank(),\n        panel.background = element_blank(),\n        axis.line = element_line(color='grey'),\n        plot.title = element_text(hjust = 0.5,\n                                  size = 20),\n        axis.title.y.left = element_text(vjust = 0.5), \n        axis.text = element_text(face=\"bold\",\n                                 size=15))\n\n\nggarrange() function of ggpubr package is used to group these bargraphs together.\n\nggarrange(tp1, tp2, tp3, tp4,\n          ncol=2,\n          nrow=2)\n\n\n\n\nInsights:\n\n\n5.6.3 What is the distribution of waterpoint proportion attributes?\nThe code chunks below are used to create multiple histograms to reveal the overall distribution of the selected variables in nga_wp_clus. They consist of two main parts.\n\nCreate the individual histograms using geom_histogram() and geom_density() functions\nGroup these histograms together by using the ggarrange() function of ggpubr package\n\n\n\nDistribution plots\nh1 <- ggplot(data=nga_wp_clus, \n             aes(x= pct_functional,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nh2 <- ggplot(data=nga_wp_clus, \n             aes(x= pct_nonfunctional,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nh3 <- ggplot(data=nga_wp_clus, \n             aes(x= pct_handpump,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n    geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nh4 <- ggplot(data=nga_wp_clus, \n             aes(x= pct_rural,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nh5 <- ggplot(data=nga_wp_clus, \n             aes(x= pct_capmore1000,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nh6 <- ggplot(data=nga_wp_clus, \n             aes(x= pct_capless1000,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nh7 <- ggplot(data=nga_wp_clus, \n             aes(x= pct_crucial,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nh8 <- ggplot(data=nga_wp_clus, \n             aes(x= pct_stale,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nggarrange(h1, h2, h3, h4, h5, h6, h7, h8,\n          ncol = 4, \n          nrow = 2)\n\n\n\n\n\nWe can observe that distribution of rural waterpoints proportion is left skewed and distribution of proportion of staleness waterpoints is right skewed. The distribution is not normal for all except pct_functional and pct_nonfunctional\n\n\n5.6.4 How is the handpump proportion spread across the country?\nPreviously, we have performed preliminary data analysis using statistical charts. Now let us visualise it using maps.\n\n\nTotal waterpoints spread\ncm1 <-  tm_shape(nga_wp_clus)+\n  tm_fill(\"total_wpt\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Total \\nWaterpoints ratio\") +\n  tm_layout(main.title = \"Distribution of total no. of waterpoints\",\n            main.title.position = \"center\",\n            main.title.size = 1.5,\n            legend.height = 1, \n            legend.width = 1,\n            legend.text.size = 1,\n            legend.title.size = 1,\n            main.title.fontface = \"bold\",\n            frame = TRUE) +\n  tmap_mode(\"plot\")+\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\",\n             position=c(\"right\", \"top\"))\n\n\n\n\nHandpump spread\ncm2 <-  tm_shape(nga_wp_clus)+\n  tm_fill(\"handpump\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Handpump \\nWaterpoints ratio\") +\n  tm_layout(main.title = \"Distribution of handpumps\",\n            main.title.position = \"center\",\n            main.title.size = 1.5,\n            legend.height = 1, \n            legend.width = 1,\n            legend.text.size = 1,\n            legend.title.size = 1,\n            main.title.fontface = \"bold\",\n            frame = TRUE) +\n  tmap_mode(\"plot\")+\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\",\n             position=c(\"right\", \"top\"))\n\n\n\n\nHandpump proportion spread\ncm3 <-  tm_shape(nga_wp_clus)+\n  tm_fill(\"pct_handpump\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Handpump \\nproportion ratio\") +\n  tm_layout(main.title = \"Distribution of handpump proportion\",\n            main.title.position = \"center\",\n            main.title.size = 1.5,\n            legend.height = 1, \n            legend.width = 1,\n            legend.text.size = 1,\n            legend.title.size = 1,\n            main.title.fontface = \"bold\",\n            frame = TRUE) +\n  tmap_mode(\"plot\")+\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\",\n             position=c(\"right\", \"top\"))\n\n\nIndividual maps are combined using tmap_arrange() function\n\ntmap_arrange(cm1, cm2, cm3, ncol=3)\n\n\n\n\nInsights:\nIt is evident that the second map is almost similar to first map because if we take absolute numbers into consideration, no. of handpumps will be obviously higher in the places where there are more no. of waterpoints. For example, central and eastern Nigeria has most no. of handpumps .Whereas, the third map shows handpumps proportion in central region is comparatively lower than western region.\n\n\n\n5.7 Clustering Analysis - Preparation\nThe below figure shows a basic framework for cluster analysis and the steps involved in the process.\n\n\n\nFig 1- Steps involved in cluster analysis\n\n\n\nAre there any highly correlated cluster variables?\nLet us ensure that the cluster variables are not highly correlated.\nThe code chunk below helps us to visualise and analyse the correlation of the input variables by using corrplot.mixed() function of corrplot package.\n\n\nclustering variables\nderived <- nga_wp_clus %>%\n  select(8,9,17:24) %>%\n  st_drop_geometry() %>%\n  replace(is.na(.), 0)\n\n\n\n\nComputing correlation\ncluster_vars.cor = cor(derived[,1:10])\ncorrplot.mixed(cluster_vars.cor,\n         lower = \"ellipse\", \n               upper = \"number\",\n               tl.pos = \"lt\",\n               diag = \"l\",\n               tl.col = \"black\")\n\n\n\n\n\nInterpretation:\nIn general, if correlation coefficient value is greater than 0.85 then the variables are said to be highly correlated. In that case, here, pct_handpump and pct_capless1000 is highly correlated as the magnitude of corelation coefficient is 0.94. Hence, let us exclude the cluster variable pct_capless1000.\nExtracting cluster variables\nAs per the flowchart, we have so far completed first step. Let us decide on the clustering variables.\nThe below code chunk is to extract and save the clustering variables in a separate dataframe.\n\nDrop geometry from the dataframe using st_set_geometry() function\nChoose the desired columns by using select() clause\nReplace the NA values by 0 using replace(is.na()) function\nPrepare a tabular format using kable() function\n\n\n\nExtracting clustering variables\ncluster_vars <- nga_wp_clus %>%\n  st_set_geometry(NULL) %>%\n  select(1,7,8,16:18,20:23) %>%\n  replace(is.na(.), 0)\nkable(head(cluster_vars,5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshapeName\nwpt_functional\nwpt_nonfunctional\npct_functional\npct_nonfunctional\npct_handpump\npct_capmore1000\npct_rural\npct_crucial\npct_stale\n\n\n\n\nAba North\n13\n11\n0.5416667\n0.4583333\n0.7916667\n0.2083333\n0.2500000\n0.0000000\n0.0000000\n\n\nAba South\n5\n8\n0.3846154\n0.6153846\n0.8461538\n0.1538462\n0.6923077\n0.1538462\n0.0000000\n\n\nAbadam\n0\n0\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n\n\nAbaji\n90\n45\n0.5625000\n0.2812500\n0.7312500\n0.1187500\n0.9687500\n0.4937500\n0.1500000\n\n\nAbak\n74\n46\n0.5323741\n0.3309353\n0.4964029\n0.3669065\n0.4820144\n0.0575540\n0.1366906\n\n\n\n\n\nNext, let us change the rows by division name instead of row number by using the code chunk below\n\n\nDivision as rows\nrow.names(cluster_vars) <- cluster_vars$shapeName \nnga_derived <- select(cluster_vars, c(2:10)) \nwrite_rds(nga_derived, \"data2/nga_derived.rds\")\n               \nkable(head(nga_derived,5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwpt_functional\nwpt_nonfunctional\npct_functional\npct_nonfunctional\npct_handpump\npct_capmore1000\npct_rural\npct_crucial\npct_stale\n\n\n\n\nAba North\n13\n11\n0.5416667\n0.4583333\n0.7916667\n0.2083333\n0.2500000\n0.0000000\n0.0000000\n\n\nAba South\n5\n8\n0.3846154\n0.6153846\n0.8461538\n0.1538462\n0.6923077\n0.1538462\n0.0000000\n\n\nAbadam\n0\n0\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n0.0000000\n\n\nAbaji\n90\n45\n0.5625000\n0.2812500\n0.7312500\n0.1187500\n0.9687500\n0.4937500\n0.1500000\n\n\nAbak\n74\n46\n0.5323741\n0.3309353\n0.4964029\n0.3669065\n0.4820144\n0.0575540\n0.1366906\n\n\n\n\n\nExamining the distribution\nCluster variables with large variances tend to have a greater effect on the resulting clusters than those with small variances. Hence, before performing cluster analysis we should examine the distribution of the cluster variables using appropriate exploratory data analysis techniques, such as histogram. \n\n\nUnivariate distribution\nrh1 <- ggplot(data=nga_derived, \n             aes(x= wpt_functional,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nh2 <- ggplot(data=nga_derived, \n             aes(x= wpt_nonfunctional,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nh3 <- ggplot(data=nga_derived, \n             aes(x= pct_functional,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n    geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nh4 <- ggplot(data=nga_derived, \n             aes(x= pct_nonfunctional,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nh5 <- ggplot(data=nga_derived, \n             aes(x= pct_handpump,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nh6 <- ggplot(data=nga_derived, \n             aes(x= pct_capmore1000,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nh7 <- ggplot(data=nga_derived, \n             aes(x= pct_rural,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nh8 <- ggplot(data=nga_derived, \n             aes(x= pct_crucial,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nh9 <- ggplot(data=nga_derived, \n             aes(x= pct_stale,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nggarrange(rh1, h2, h3, h4, h5, h6, h7, h8, h9,\n          ncol = 3, \n          nrow = 3)\n\n\n\n\n\nThe univariate distribution plot shows that there are majority of zero. Let us exclude zero and view how the plot turns out to be\n\n\nUnivariate distribution\nnga_derived_excludeZero <- filter_if(nga_derived, is.numeric, all_vars((.) != 0))\n\nh1 <- ggplot(data=nga_derived_excludeZero,\n             aes(x= wpt_functional,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nh2 <- ggplot(data=nga_derived_excludeZero, \n             aes(x= wpt_nonfunctional,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nh3 <- ggplot(data=nga_derived_excludeZero, \n             aes(x= pct_functional,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n    geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nh4 <- ggplot(data=nga_derived_excludeZero, \n             aes(x= pct_nonfunctional,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nh5 <- ggplot(data=nga_derived_excludeZero, \n             aes(x= pct_handpump,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nh6 <- ggplot(data=nga_derived_excludeZero, \n             aes(x= pct_capmore1000,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nh7 <- ggplot(data=nga_derived_excludeZero, \n             aes(x= pct_rural,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nh8 <- ggplot(data=nga_derived_excludeZero, \n             aes(x= pct_crucial,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nh9 <- ggplot(data=nga_derived_excludeZero, \n             aes(x= pct_stale,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nggarrange(h1, h2, h3, h4, h5, h6, h7, h8, h9,\n          ncol = 3, \n          nrow = 3)\n\n\n\n\n\nDistribution of attributes like pct_functional, pct_nonfunctional are normal whereas wpt_functional, wpt_non_functional are right skewed, pct_rural distribution is left skewed. Let us perform transformation to convert it into normal distribution\nLog Transformation\n\n\nUnivariate distribution\nlognga_derived <- nga_derived_excludeZero\nlognga_derived[,1:9] <- log(lognga_derived[,1:9]+1)\n\nlh1 <- ggplot(data=lognga_derived,\n             aes(x= wpt_functional,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nlh2 <- ggplot(data=lognga_derived, \n             aes(x= wpt_nonfunctional,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nlh3 <- ggplot(data=lognga_derived, \n             aes(x= pct_functional,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n    geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nlh4 <- ggplot(data=lognga_derived, \n             aes(x= pct_nonfunctional,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nlh5 <- ggplot(data=lognga_derived, \n             aes(x= pct_handpump,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nlh6 <- ggplot(data=lognga_derived, \n             aes(x= pct_capmore1000,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nlh7 <- ggplot(data=lognga_derived, \n             aes(x= pct_rural,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nlh8 <- ggplot(data=lognga_derived, \n             aes(x= pct_crucial,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nlh9 <- ggplot(data=lognga_derived, \n             aes(x= pct_stale,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\nggarrange(lh1, lh2, lh3, lh4,lh5, lh6, lh7,lh8, lh9,\n          ncol = 3, \n          nrow = 3)\n\n\n\n\n\nAfter performing log transformation, now the distribution is normal. For eg, let us compare the distribution of wpt_functional before and after transformation\n\n\nComaprison\nh1 <- h1 + labs(title= \"Raw values\")\nlh1 <- lh1 + labs(title = \"Log transformation\")\n\nggarrange(h1, lh1, ncol=2)\n\n\n\n\n\nThe skewness is removed and the distribution has become normal\nData Standardisation\nAs our clustering variables include absolute numbers and percentage values, it is obvious that their values range are different. In order to avoid the cluster analysis result is baised to clustering variables with large values, it is useful to standardise the input variables before performing cluster analysis.\nThere are 3 major standardisation techniques\n Here, we are performing Z-score standardisation as it can handle outliers well and it is applicable to variables which are from normal distribution\n\n\nZ-score\nnga_derived.z <- scale(lognga_derived)\n\nzh1 <- ggplot(data=as.data.frame(nga_derived.z),\n             aes(x= wpt_functional,\n                 y= ..density..)) +\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"coral\")+\n  geom_density(color=\"black\",\n               alpha = 0.5)+\n  theme(panel.background= element_blank())\n\n\n\n\nComparison\nrh1 <- rh1 + labs(title= \"Raw Data\")\nh1 <-  h1 + labs(title= \"Excluding Zeroes\")\nlh1 <- lh1 + labs(title = \"Log transformation\")\nzh1 <- zh1 + labs(title = \"Z-score standardisation\")\n\nggarrange(rh1,h1, lh1,zh1, ncol=4)\n\n\n\n\n\nHence, we have transformed the variables so as to have normal distribution and performed z-score standardisation to maintain same ranges among all variables.\nComputing Proximity Matrix\nA proximity matrix is a square matrix (two-dimensional array) containing the distances, taken pairwise between the elements of a matrix. Broadly defined; a proximity matrix measures the similarity or dissimilarity between the pairs of matrix.\nMajor types are euclidean, maximum, manhattan, canberra, binary and minkowski.\nThe code chunk below is used to compute the proximity matrix using dist() function and euclidean method.\n\n\nProximity matrix\nproxmat <- dist(nga_derived, method = 'euclidean')\n\n\n\n\n\n5.8 Conventional Clustering\nIn this section, let us perform Hierarchical and K-means clustering and analyse the results\n\n5.8.1 Hierarchical Clustering\nDetermining optimal algorithm While performing hierarchical clustering we have to identify the strongest clustering structures.\nThe code chunk below will be used to compute the agglomerative coefficients of all hierarchical clustering algorithms.\n\n\nOptimal algorithm\nm <- c( \"average\", \"single\", \"complete\", \"ward\")\nnames(m) <- c( \"average\", \"single\", \"complete\", \"ward\")\n\nac <- function(x) {\n  agnes(nga_derived, method = x)$ac\n}\n\nmap_dbl(m, ac)\n\n\n  average    single  complete      ward \n0.9934065 0.9892576 0.9959162 0.9986306 \n\n\nWe can see that Ward’s method provides the strongest clustering structure among the four methods assessed. Hence, let us use the same in the subsequent analysis.\nDetermining optimal clusters\nThere are three commonly used methods to determine the optimal clusters, they are:\n\nElbow Method\nAverage Silhouette Method\nGap Statistic Method\n\nLet us use Elbow Method to determine optimal no. of clusters.\nfviz_nbclust() of factoextra package - to specify the no. of clusters to be generated\nmethod - wss - within sum of squares\n\n\nElbow method\nfviz_nbclust(nga_derived.z, \n             hcut, \n             method = \"wss\")\n\n\n\n\n\nMin no. of clusters required to perform clustering analysis is 3. The plot reveals that optimal no. of clusters is 4 as the slope reduces gradually.\nCreating dendrogram\nIn the dendrogram displayed below, each leaf corresponds to one observation. As we move up the tree, observations that are similar to each other are combined into branches, which are themselves fused at a higher height. The code chunk below performs hierarchical cluster analysis using ward.D method. The hierarchical clustering output is stored in an object of class hclust which describes the tree produced by the clustering process. rect.hclust() - to draw the dendrogram with a border around the selected clusters\nborder - to specify the border colors for the rectangles\n\n\nDendrogram\nset.seed(3456)\nhclust_ward <- hclust(proxmat, method = 'ward.D')\nplot(hclust_ward, cex = 0.9)\nrect.hclust(hclust_ward, \n            k = 4, \n            border = 2:5)\n\n\n\n\n\nVisual interpretation of clusters - Spatial Map\nWe are retaining 4 clusters for the analysis.The code chunk performs the following steps\n\nDerive a 6-cluster model using cutree() function\nConvert the groups list object into a matrix using as.matrix() function\nAppend groups matrix onto nga_wp_clus to produce an output simple feature object called hierarCluster using cbind() function\nRename as.matrix.groups as Hierar cluster using rename() function\n\n\n\nPreparing Data\ngroups <- as.factor(cutree(hclust_ward, k=4))\nhierarCluster <- cbind(nga_wp_clus, as.matrix(groups)) %>%\n  rename(`Hierar cluster` = `as.matrix.groups.`)\n\n\n\nlength(as.matrix(groups))\n\n[1] 774\n\n\n\n\nMapping Hierarchical Cluster\ncmap1 <- tm_shape (hierarCluster) +\n          tm_polygons(\"Hierar cluster\",\n          title = \"Hierarchical Cluster\") +\n          tm_layout(main.title = \"Conventional Hierarchical clusters\",\n                    main.title.position = \"center\",\n                    main.title.size = 1.5,\n                    legend.height = 1.5, \n                    legend.width = 1.5,\n                    legend.text.size = 1.5,\n                    legend.title.size = 1.5,\n                    main.title.fontface = \"bold\",\n                    frame = TRUE) +\n        tmap_mode(\"plot\")+\n        tm_borders(alpha = 0.5) +\n        tm_compass(type=\"8star\",\n                  position=c(\"right\", \"top\"))\n\n\ntmap mode set to plotting\n\n\nMapping Hierarchical Cluster\ncmap1\n\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\nVisual interpretation of clusters - Parallel Plot\n\n\nHierarchical Cluster - Parallel plot\nggparcoord(data = hierarCluster,\n           columns = c(7,8,16,17,18,20:23),\n           scale = \"std\",\n           alphaLines = 0.2,\n           boxplot = TRUE,\n           title = \"Multiple Parallel Coordinates Plots of Hierarchical variables by Cluster\") +\n  facet_grid(~ `Hierar cluster`) +\n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\n5.9 Spatially Constrained Clustering\nIn this section, let us perform the following spatially constrained clustering using\n\nSpatial ’K’luster Analysis by Tree Edge Removal (SKATER) method\nSpatially Constrained Hierarchical Clustering using ClustGeo method\n\n\n5.9.1 SKATER method\nFirst, let us convert nga_wp_clus into SpatialPolygonsDataFrame. This is because SKATER function only support sp objects such as SpatialPolygonDataFrame.\nThe code chunk below uses as_Spatial() of sf package to convert nga_wp_clus into a SpatialPolygonDataFrame called nga_sp.\npoly2nd() of spdep package will be used to compute the neighbours list from polygon list.\nnbcosts() of spdep package is used to compute the cost of each edge. It is the distance between it nodes.\nWe have to remove the region which has no neighbours\n\n\nPreparing Data\nnga_sp <- as_Spatial(nga_wp_clus %>%\n                       filter(shapeName != \"Bakassi\"))\nnga.nb <- poly2nb(nga_sp)\nlcosts <- nbcosts(nga.nb, nga_derived)\nnga.w <- nb2listw(nga.nb, \n                   lcosts, \n                   style=\"B\")\n\n\nWarning in nb2listw(nga.nb, lcosts, style = \"B\"): zero sum general weights\n\n\nPreparing Data\nnga.mst <- mstree(nga.w)\n\n\n\n\nPlotting Min spanning tree\nplot(nga_sp, border=gray(.5))\nplot.mst(nga.mst, \n         coordinates(nga_sp), \n         col=\"red\", \n         cex.lab=0.7, \n         cex.circles=0.005, \n         add=TRUE)\n\n\n\n\n\n\n\nComputing cluster - SKATER\nset.seed(3456)\nclust4 <- spdep::skater(edges = nga.mst[,1:2], \n                 data = nga_derived, \n                 method = \"euclidean\", \n                 ncuts = 3)\n\ngroups_mat <- as.matrix(clust4$groups)\nskaterCluster <- cbind(nga_wp_clus %>%\n                       filter(shapeName != \"Bakassi\"), as.factor(groups_mat)) %>%\n  rename(`SKATER cluster`=`as.factor.groups_mat.`)\n\n\n\n\nMapping SKATER Cluster\ncmap2 <- tm_shape (skaterCluster) +\n          tm_polygons(\"SKATER cluster\",\n          title = \"SKATER Cluster\") +\n          tm_layout(main.title = \"Spatially constraied- SKATER\",\n                    main.title.position = \"center\",\n                    main.title.size = 1.5,\n                    legend.height = 1.5, \n                    legend.width = 1.5,\n                    legend.text.size = 1.5,\n                    legend.title.size = 1.5,\n                    main.title.fontface = \"bold\",\n                    frame = TRUE) +\n        tmap_mode(\"plot\")+\n        tm_borders(alpha = 0.5) +\n        tm_compass(type=\"8star\",\n                  position=c(\"right\", \"top\"))\n\n\ntmap mode set to plotting\n\n\nMapping SKATER Cluster\ncmap2\n\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\nVisual interpretation of clusters - Parallel Plot\n\n\nSKATER - Parallel plot\nggparcoord(data = skaterCluster,\n           columns = c(7,8,16,17,18,20:23),\n           scale = \"std\",\n           alphaLines = 0.2,\n           boxplot = TRUE,\n           title = \"Multiple Parallel Coordinates Plots of SKATER cluster Variables by Cluster\") +\n  facet_grid(~ `SKATER cluster`) +\n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n5.9.2 Non spatially constrained clustering using ClustGeo method\n\n\nComputing cluster - ClustGeo\nset.seed(3456)\nnongeo_cluster <- hclustgeo(proxmat)\ngroups <- as.factor(cutree(nongeo_cluster, k=4))\ngeoCluster <- cbind(nga_wp_clus, as.matrix(groups)) %>%\n  rename(`Geo cluster` = `as.matrix.groups.`)\n\n\n\n\nMapping Geo Cluster\ncmap3 <- tm_shape (geoCluster) +\n          tm_polygons(\"Geo cluster\",\n          title = \"Geo Cluster\") +\n          tm_layout(main.title = \"Non-spatially constraied - GeoCluster\",\n                    main.title.position = \"center\",\n                    main.title.size = 1.5,\n                    legend.height = 1.5, \n                    legend.width = 1.5,\n                    legend.text.size = 1.5,\n                    legend.title.size = 1.5,\n                    main.title.fontface = \"bold\",\n                    frame = TRUE) +\n        tmap_mode(\"plot\")+\n        tm_borders(alpha = 0.5) +\n        tm_compass(type=\"8star\",\n                  position=c(\"right\", \"top\"))\n\n\ntmap mode set to plotting\n\n\nMapping Geo Cluster\ncmap3\n\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\nVisual interpretation of clusters - Parallel Plot\n\n\nNon spatially constrained - Parallel plot\nggparcoord(data = geoCluster,\n           columns = c(7,8,16,17,18,20:23),\n           scale = \"std\",\n           alphaLines = 0.2,\n           boxplot = TRUE,\n           title = \"Multiple Parallel Coordinates Plots of Non spatially constrained Variables by Cluster\") +\n  facet_grid(~ `Geo cluster`) +\n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n5.9.2 Spatially constrained hierarchical clustering using ClustGeo method\n\n\nComputing cluster - ClustGeo\ndist <- st_distance(nga_wp_clus, nga_wp_clus)\ndistmat <- as.dist(dist)\ncr <- choicealpha(D0 = proxmat, \n                  D1 = distmat, \n                  range.alpha = seq(0, 1, 0.1),\n                  K=4, \n                  graph = TRUE)\n\n\n\n\n\n\n\n\n\n\nMapping Geo Cluster\nset.seed(3456)\nclustG <- hclustgeo(proxmat, distmat, alpha = 0.3)\ngroups <- as.factor(cutree(clustG, k=4))\nspConstrainedCluster <- cbind(nga_wp_clus, as.matrix(groups)) %>%\n  rename(`spatial constrained cluster` = `as.matrix.groups.`)\n\n\n\n\nMapping Geo Cluster\ncmap4 <- tm_shape (spConstrainedCluster) +\n          tm_polygons(\"spatial constrained cluster\",\n          title = \"spatial constrained cluster\") +\n          tm_layout(main.title = \"Spatially constraied - GeoCluster\",\n                    main.title.position = \"center\",\n                    main.title.size = 1.5,\n                    legend.height = 1.5, \n                    legend.width = 1.5,\n                    legend.text.size = 1.5,\n                    legend.title.size = 1.5,\n                    main.title.fontface = \"bold\",\n                    frame = TRUE) +\n        tmap_mode(\"plot\")+\n        tm_borders(alpha = 0.5) +\n        tm_compass(type=\"8star\",\n                  position=c(\"right\", \"top\"))\n\n\ntmap mode set to plotting\n\n\nMapping Geo Cluster\ncmap4\n\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\nVisual interpretation of clusters - Parallel Plot\n\n\nNon spatially constrained - Parallel plot\nggparcoord(data = spConstrainedCluster,\n           columns = c(7,8,16,17,18,20:23),\n           scale = \"std\",\n           alphaLines = 0.2,\n           boxplot = TRUE,\n           title = \"Multiple Parallel Coordinates Plots of spatially constrained Variables by Cluster\") +\n  facet_grid(~ `spatial constrained cluster`) +\n  theme(axis.text.x = element_text(angle = 30))\n\n\n\n\n\n\n\n\n6.Key Insights\n\ntmap_arrange(cmap1, cmap2, cmap3, cmap4,\n             nrow = 2,\n             ncol = 2)\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\nWarning: One tm layer group has duplicated layer types, which are omitted. To\ndraw multiple layers of the same type, use multiple layer groups (i.e. specify\ntm_shape prior to each of them).\n\n\n\n\n\n\n\n7.Conclusion\n\n\n8. References"
  }
]